{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santander Product Recommendation\n",
    "\n",
    "* The aim is to\n",
    "* Modeling: Ensemble LightGBM, XGboost, NN(5 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras import models, layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/meta_data.pkl', 'rb') as fin:\n",
    "    meta = pickle.load(fin)\n",
    "\n",
    "features = meta['features']\n",
    "target = meta['target']\n",
    "prods = meta['prods']\n",
    "\n",
    "with open('../input/processed_data.pkl', 'rb') as finn:\n",
    "    data = pickle.load(finn)\n",
    "\n",
    "#validation data\n",
    "tst_vld = data['tst_vld']\n",
    "tst_all = data['tst_all']\n",
    "\n",
    "#training data\n",
    "trn = data['trn_all']\n",
    "\n",
    "del meta, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Method MAP@7.\n",
    "<br>\n",
    "<br>\n",
    "actually purchased products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncodpers_tst_vld = tst_vld['ncodpers'].values\n",
    "\n",
    "def get_purchased_products():    \n",
    "\n",
    "    # 검증 데이터에서 신규 구매를 구한다.\n",
    "    for prod in prods:\n",
    "        prev = prod + '_prev'\n",
    "        padd = prod + '_add'\n",
    "        tst_vld[padd] = tst_vld[prod] - tst_vld[prev]\n",
    "\n",
    "\n",
    "    # 고객별 신규 구매 정답 값을 add_vld_list에 저장하고, 총 count를 count_vld에 저장한다.\n",
    "    add_vld = tst_vld[[prod + '_add' for prod in prods]].values\n",
    "    add_vld_list = [list() for i in range(len(ncodpers_tst_vld))]\n",
    "\n",
    "    count_vld = 0\n",
    "    for ncodper in range(len(ncodpers_tst_vld)):\n",
    "        for prod in range(len(prods)):\n",
    "            if add_vld[ncodper, prod] > 0:\n",
    "                add_vld_list[ncodper].append(prod)\n",
    "                count_vld += 1\n",
    "                \n",
    "    return add_vld_list\n",
    "\n",
    "add_vld_list = get_purchased_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_7_products(preds_vld):\n",
    "    # 검증 데이터 예측 상위 7개를 추출한다.\n",
    "    result_vld = []\n",
    "    \n",
    "    for ncodper, pred in zip(ncodpers_tst_vld, preds_vld):\n",
    "        y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods[target], target)]\n",
    "        y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "        result_vld.append([ip for y,p,ip in y_prods])\n",
    "    \n",
    "    return result_vld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation method is MAP @ 7.\n",
    "It matches to 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=7, default=0.0):\n",
    "    # MAP@7 이므로, 최대 7개만 사용한다\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        # 점수를 부여하는 조건은 다음과 같다 :\n",
    "        # 예측값이 정답에 있고 (‘p in actual’)\n",
    "        # 예측값이 중복이 아니면 (‘p not in predicted[:i]’) \n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    # 정답값이 공백일 경우, 무조건 0.0점을 반환한다\n",
    "    if not actual:\n",
    "        return default\n",
    "\n",
    "    # 정답의 개수(len(actual))로 average precision을 구한다\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=7, default=0.0):\n",
    "    # list of list인 정답값(actual)과 예측값(predicted)에서 고객별 Average Precision을 구하고, np.mean()을 통해 평균을 계산한다\n",
    "    return np.mean([apk(a, p, k, default) for a, p in zip(actual, predicted)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04266379915553903\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터에서 얻을 수 있는 MAP@7 최고점을 미리 구한다. (0.042613)\n",
    "print(mapk(add_vld_list, add_vld_list, 7, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation, pull out actual products list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the models with validation data\n",
    "<br>\n",
    "trn_vld<br>\n",
    "eval_vld<br>\n",
    "tst_vld<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vld_date = '2016-05-28'\n",
    "\n",
    "trn_vld = trn[trn['fecha_dato'] < vld_date]\n",
    "eval_vld = trn[trn['fecha_dato']==vld_date]\n",
    "\n",
    "X_trn_vld = trn_vld[features].values\n",
    "y_trn_vld = trn_vld['target'].values\n",
    "\n",
    "X_eval_vld = eval_vld[features].values\n",
    "y_eval_vld = eval_vld['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10765757, 60), (10765757,), (689132, 60), (689132,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn_vld.shape, y_trn_vld.shape, X_eval_vld.shape, y_eval_vld.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* validation data\n",
    "X_trn_vld ,y_trn_vld , X_eval_vld , y_eval_vld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) XGBoost Model Training with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.11779\teval-mlogloss:2.09076\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:1.82085\teval-mlogloss:1.75636\n",
      "[2]\ttrain-mlogloss:1.63405\teval-mlogloss:1.52409\n",
      "[3]\ttrain-mlogloss:1.44439\teval-mlogloss:1.34769\n",
      "[4]\ttrain-mlogloss:1.30098\teval-mlogloss:1.2078\n",
      "[5]\ttrain-mlogloss:1.21081\teval-mlogloss:1.08793\n",
      "[6]\ttrain-mlogloss:1.11172\teval-mlogloss:0.989476\n",
      "[7]\ttrain-mlogloss:1.02199\teval-mlogloss:0.903919\n",
      "[8]\ttrain-mlogloss:0.948211\teval-mlogloss:0.828923\n",
      "[9]\ttrain-mlogloss:0.892847\teval-mlogloss:0.763686\n",
      "[10]\ttrain-mlogloss:0.834503\teval-mlogloss:0.705944\n",
      "[11]\ttrain-mlogloss:0.791048\teval-mlogloss:0.655371\n",
      "[12]\ttrain-mlogloss:0.748521\teval-mlogloss:0.610169\n",
      "[13]\ttrain-mlogloss:0.713106\teval-mlogloss:0.569988\n",
      "[14]\ttrain-mlogloss:0.6793\teval-mlogloss:0.534392\n",
      "[15]\ttrain-mlogloss:0.642269\teval-mlogloss:0.501605\n",
      "[16]\ttrain-mlogloss:0.617519\teval-mlogloss:0.472721\n",
      "[17]\ttrain-mlogloss:0.589075\teval-mlogloss:0.447039\n",
      "[18]\ttrain-mlogloss:0.571274\teval-mlogloss:0.423796\n",
      "[19]\ttrain-mlogloss:0.551139\teval-mlogloss:0.402494\n",
      "[20]\ttrain-mlogloss:0.535602\teval-mlogloss:0.383711\n",
      "[21]\ttrain-mlogloss:0.524963\teval-mlogloss:0.366933\n",
      "[22]\ttrain-mlogloss:0.509045\teval-mlogloss:0.351844\n",
      "[23]\ttrain-mlogloss:0.499826\teval-mlogloss:0.337971\n",
      "[24]\ttrain-mlogloss:0.486401\teval-mlogloss:0.325247\n",
      "[25]\ttrain-mlogloss:0.469134\teval-mlogloss:0.314352\n",
      "[26]\ttrain-mlogloss:0.457152\teval-mlogloss:0.303718\n",
      "[27]\ttrain-mlogloss:0.449309\teval-mlogloss:0.294732\n",
      "[28]\ttrain-mlogloss:0.441771\teval-mlogloss:0.286492\n",
      "[29]\ttrain-mlogloss:0.433104\teval-mlogloss:0.278901\n",
      "[30]\ttrain-mlogloss:0.425916\teval-mlogloss:0.27201\n",
      "[31]\ttrain-mlogloss:0.419311\teval-mlogloss:0.26562\n",
      "[32]\ttrain-mlogloss:0.414246\teval-mlogloss:0.259958\n",
      "[33]\ttrain-mlogloss:0.407912\teval-mlogloss:0.255132\n",
      "[34]\ttrain-mlogloss:0.403963\teval-mlogloss:0.250246\n",
      "[35]\ttrain-mlogloss:0.400184\teval-mlogloss:0.245742\n",
      "[36]\ttrain-mlogloss:0.395429\teval-mlogloss:0.242064\n",
      "[37]\ttrain-mlogloss:0.392208\teval-mlogloss:0.238402\n",
      "[38]\ttrain-mlogloss:0.389698\teval-mlogloss:0.23517\n",
      "[39]\ttrain-mlogloss:0.387573\teval-mlogloss:0.232401\n",
      "[40]\ttrain-mlogloss:0.385082\teval-mlogloss:0.229355\n",
      "[41]\ttrain-mlogloss:0.38319\teval-mlogloss:0.227127\n",
      "[42]\ttrain-mlogloss:0.379942\teval-mlogloss:0.225044\n",
      "[43]\ttrain-mlogloss:0.378271\teval-mlogloss:0.222965\n",
      "[44]\ttrain-mlogloss:0.376474\teval-mlogloss:0.220729\n",
      "[45]\ttrain-mlogloss:0.375036\teval-mlogloss:0.219129\n",
      "[46]\ttrain-mlogloss:0.373934\teval-mlogloss:0.217618\n",
      "[47]\ttrain-mlogloss:0.372656\teval-mlogloss:0.216234\n",
      "[48]\ttrain-mlogloss:0.371642\teval-mlogloss:0.214986\n",
      "[49]\ttrain-mlogloss:0.369915\teval-mlogloss:0.213292\n",
      "[50]\ttrain-mlogloss:0.368999\teval-mlogloss:0.212186\n",
      "[51]\ttrain-mlogloss:0.367961\teval-mlogloss:0.211164\n",
      "[52]\ttrain-mlogloss:0.367212\teval-mlogloss:0.210257\n",
      "[53]\ttrain-mlogloss:0.366176\teval-mlogloss:0.209348\n",
      "[54]\ttrain-mlogloss:0.365449\teval-mlogloss:0.208606\n",
      "[55]\ttrain-mlogloss:0.364867\teval-mlogloss:0.207874\n",
      "[56]\ttrain-mlogloss:0.364117\teval-mlogloss:0.207153\n",
      "[57]\ttrain-mlogloss:0.363535\teval-mlogloss:0.206526\n",
      "[58]\ttrain-mlogloss:0.363031\teval-mlogloss:0.205938\n",
      "[59]\ttrain-mlogloss:0.362598\teval-mlogloss:0.205415\n",
      "[60]\ttrain-mlogloss:0.361941\teval-mlogloss:0.204833\n",
      "[61]\ttrain-mlogloss:0.361224\teval-mlogloss:0.204203\n",
      "[62]\ttrain-mlogloss:0.360531\teval-mlogloss:0.203465\n",
      "[63]\ttrain-mlogloss:0.360065\teval-mlogloss:0.20298\n",
      "[64]\ttrain-mlogloss:0.359667\teval-mlogloss:0.202558\n",
      "[65]\ttrain-mlogloss:0.359301\teval-mlogloss:0.202172\n",
      "[66]\ttrain-mlogloss:0.358948\teval-mlogloss:0.201813\n",
      "[67]\ttrain-mlogloss:0.358632\teval-mlogloss:0.20146\n",
      "[68]\ttrain-mlogloss:0.358303\teval-mlogloss:0.201126\n",
      "[69]\ttrain-mlogloss:0.357929\teval-mlogloss:0.200842\n",
      "[70]\ttrain-mlogloss:0.357584\teval-mlogloss:0.200557\n",
      "[71]\ttrain-mlogloss:0.357331\teval-mlogloss:0.200318\n",
      "[72]\ttrain-mlogloss:0.357089\teval-mlogloss:0.200085\n",
      "[73]\ttrain-mlogloss:0.356838\teval-mlogloss:0.199858\n",
      "[74]\ttrain-mlogloss:0.356596\teval-mlogloss:0.199639\n",
      "[75]\ttrain-mlogloss:0.356367\teval-mlogloss:0.199417\n",
      "[76]\ttrain-mlogloss:0.356166\teval-mlogloss:0.199214\n",
      "[77]\ttrain-mlogloss:0.355963\teval-mlogloss:0.19903\n",
      "[78]\ttrain-mlogloss:0.355762\teval-mlogloss:0.198857\n",
      "[79]\ttrain-mlogloss:0.35558\teval-mlogloss:0.198689\n",
      "[80]\ttrain-mlogloss:0.355417\teval-mlogloss:0.198548\n",
      "[81]\ttrain-mlogloss:0.355251\teval-mlogloss:0.198393\n",
      "[82]\ttrain-mlogloss:0.355082\teval-mlogloss:0.198251\n",
      "[83]\ttrain-mlogloss:0.354924\teval-mlogloss:0.198132\n",
      "[84]\ttrain-mlogloss:0.354779\teval-mlogloss:0.198014\n",
      "[85]\ttrain-mlogloss:0.354648\teval-mlogloss:0.197902\n",
      "[86]\ttrain-mlogloss:0.354514\teval-mlogloss:0.197788\n",
      "[87]\ttrain-mlogloss:0.354382\teval-mlogloss:0.197674\n",
      "[88]\ttrain-mlogloss:0.354259\teval-mlogloss:0.197565\n",
      "[89]\ttrain-mlogloss:0.354148\teval-mlogloss:0.197465\n",
      "[90]\ttrain-mlogloss:0.354032\teval-mlogloss:0.197369\n",
      "[91]\ttrain-mlogloss:0.353927\teval-mlogloss:0.19729\n",
      "[92]\ttrain-mlogloss:0.353816\teval-mlogloss:0.197194\n",
      "[93]\ttrain-mlogloss:0.353716\teval-mlogloss:0.197117\n",
      "[94]\ttrain-mlogloss:0.353606\teval-mlogloss:0.197036\n",
      "[95]\ttrain-mlogloss:0.353499\teval-mlogloss:0.196937\n",
      "[96]\ttrain-mlogloss:0.353402\teval-mlogloss:0.196863\n",
      "[97]\ttrain-mlogloss:0.353299\teval-mlogloss:0.196781\n",
      "[98]\ttrain-mlogloss:0.353207\teval-mlogloss:0.196709\n",
      "[99]\ttrain-mlogloss:0.35312\teval-mlogloss:0.196641\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 모델 parameter를 설정한다.\n",
    "param_xgb = {\n",
    "    #'booster': 'gbtree',\n",
    "    'max_depth': 8,\n",
    "    'nthread': 4,\n",
    "    'num_class': 17,\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'eta': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'seed': 2018,\n",
    "    }\n",
    "\n",
    "# 훈련, 검증 데이터를 XGBoost 형태로 변환한다.\n",
    "dtrn = xgb.DMatrix(X_trn_vld, label=y_trn_vld, feature_names=features)\n",
    "dvld = xgb.DMatrix(X_eval_vld, label=y_eval_vld, feature_names=features)\n",
    "\n",
    "# XGBoost 모델을 훈련 데이터로 학습한다!\n",
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "model_xgb = xgb.train(param_xgb, dtrn, num_boost_round=100, evals=watch_list, early_stopping_rounds=10)\n",
    "best_ntree_limit = model_xgb.best_ntree_limit\n",
    "\n",
    "\n",
    "# 학습한 모델을 저장한다.\n",
    "pickle.dump(model_xgb, open(\"../model/xgb.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to DataFrame, shape must be (696539, 24): given (696539, 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-852dc0b74ef1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 저번 달에 보유한 제품은 신규 구매가 불가하기 때문에, 확률값에서 미리 1을 빼준다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpreds_vld_xgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds_vld_xgb\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtst_vld\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_prev'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mprod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprods\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[0;32m   2016\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2018\u001b[1;33m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2020\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36m_align_method_FRAME\u001b[1;34m(left, right, axis)\u001b[0m\n\u001b[0;32m   1974\u001b[0m                                  \u001b[1;34m\"must be {req_shape}: given {given_shape}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1975\u001b[0m                                  .format(req_shape=left.shape,\n\u001b[1;32m-> 1976\u001b[1;33m                                          given_shape=right.shape))\n\u001b[0m\u001b[0;32m   1977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1978\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to coerce to DataFrame, shape must be (696539, 24): given (696539, 17)"
     ]
    }
   ],
   "source": [
    "# 검증 데이터에 대한 예측 값을 구한다.\n",
    "X_tst_vld = tst_vld[features].values\n",
    "X_tst_vld = xgb.DMatrix(X_tst_vld, feature_names=features)\n",
    "preds_vld_xgb = model_xgb.predict(X_tst_vld, ntree_limit=best_ntree_limit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696539, 16)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_vld_xgb_16 = np.delete(preds_vld_xgb, 16, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저번 달에 보유한 제품은 신규 구매가 불가하기 때문에, 확률값에서 미리 1을 빼준다\n",
    "preds_vld_xgb_16 = preds_vld_xgb_16 - tst_vld[[prod+'_prev' for prod in prods[target]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03609679275470783\n"
     ]
    }
   ],
   "source": [
    "result_xgb = predict_7_products(preds_vld_xgb_16.values)\n",
    "\n",
    "# 검증 데이터에서의 MAP@7 점수를 구한다. (0.03609679275470783)\n",
    "print(mapk(add_vld_list, result_xgb, 7, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) lightGBM Model Training with validation data\n",
    "\n",
    "X_trn_vld ,y_trn_vld , X_eval_vld , y_eval_vld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 0.367338\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.345466\n",
      "[3]\tvalid_0's multi_logloss: 0.330157\n",
      "[4]\tvalid_0's multi_logloss: 0.316865\n",
      "[5]\tvalid_0's multi_logloss: 0.303929\n",
      "[6]\tvalid_0's multi_logloss: 0.293882\n",
      "[7]\tvalid_0's multi_logloss: 0.285786\n",
      "[8]\tvalid_0's multi_logloss: 0.277765\n",
      "[9]\tvalid_0's multi_logloss: 0.270836\n",
      "[10]\tvalid_0's multi_logloss: 0.264364\n",
      "[11]\tvalid_0's multi_logloss: 0.258532\n",
      "[12]\tvalid_0's multi_logloss: 0.253337\n",
      "[13]\tvalid_0's multi_logloss: 0.248596\n",
      "[14]\tvalid_0's multi_logloss: 0.244373\n",
      "[15]\tvalid_0's multi_logloss: 0.240536\n",
      "[16]\tvalid_0's multi_logloss: 0.237038\n",
      "[17]\tvalid_0's multi_logloss: 0.233878\n",
      "[18]\tvalid_0's multi_logloss: 0.230963\n",
      "[19]\tvalid_0's multi_logloss: 0.228342\n",
      "[20]\tvalid_0's multi_logloss: 0.22607\n",
      "[21]\tvalid_0's multi_logloss: 0.223855\n",
      "[22]\tvalid_0's multi_logloss: 0.221916\n",
      "[23]\tvalid_0's multi_logloss: 0.22005\n",
      "[24]\tvalid_0's multi_logloss: 0.218437\n",
      "[25]\tvalid_0's multi_logloss: 0.216943\n",
      "[26]\tvalid_0's multi_logloss: 0.215703\n",
      "[27]\tvalid_0's multi_logloss: 0.214354\n",
      "[28]\tvalid_0's multi_logloss: 0.213185\n",
      "[29]\tvalid_0's multi_logloss: 0.212087\n",
      "[30]\tvalid_0's multi_logloss: 0.211138\n",
      "[31]\tvalid_0's multi_logloss: 0.210171\n",
      "[32]\tvalid_0's multi_logloss: 0.209257\n",
      "[33]\tvalid_0's multi_logloss: 0.208468\n",
      "[34]\tvalid_0's multi_logloss: 0.207806\n",
      "[35]\tvalid_0's multi_logloss: 0.207085\n",
      "[36]\tvalid_0's multi_logloss: 0.206421\n",
      "[37]\tvalid_0's multi_logloss: 0.205809\n",
      "[38]\tvalid_0's multi_logloss: 0.205221\n",
      "[39]\tvalid_0's multi_logloss: 0.204715\n",
      "[40]\tvalid_0's multi_logloss: 0.204272\n",
      "[41]\tvalid_0's multi_logloss: 0.203806\n",
      "[42]\tvalid_0's multi_logloss: 0.203388\n",
      "[43]\tvalid_0's multi_logloss: 0.202963\n",
      "[44]\tvalid_0's multi_logloss: 0.202574\n",
      "[45]\tvalid_0's multi_logloss: 0.202267\n",
      "[46]\tvalid_0's multi_logloss: 0.20193\n",
      "[47]\tvalid_0's multi_logloss: 0.201608\n",
      "[48]\tvalid_0's multi_logloss: 0.201355\n",
      "[49]\tvalid_0's multi_logloss: 0.201125\n",
      "[50]\tvalid_0's multi_logloss: 0.200861\n",
      "[51]\tvalid_0's multi_logloss: 0.200598\n",
      "[52]\tvalid_0's multi_logloss: 0.200406\n",
      "[53]\tvalid_0's multi_logloss: 0.200176\n",
      "[54]\tvalid_0's multi_logloss: 0.199961\n",
      "[55]\tvalid_0's multi_logloss: 0.199746\n",
      "[56]\tvalid_0's multi_logloss: 0.199554\n",
      "[57]\tvalid_0's multi_logloss: 0.199379\n",
      "[58]\tvalid_0's multi_logloss: 0.199211\n",
      "[59]\tvalid_0's multi_logloss: 0.19904\n",
      "[60]\tvalid_0's multi_logloss: 0.198877\n",
      "[61]\tvalid_0's multi_logloss: 0.198743\n",
      "[62]\tvalid_0's multi_logloss: 0.198602\n",
      "[63]\tvalid_0's multi_logloss: 0.198465\n",
      "[64]\tvalid_0's multi_logloss: 0.198322\n",
      "[65]\tvalid_0's multi_logloss: 0.1982\n",
      "[66]\tvalid_0's multi_logloss: 0.198081\n",
      "[67]\tvalid_0's multi_logloss: 0.197967\n",
      "[68]\tvalid_0's multi_logloss: 0.19785\n",
      "[69]\tvalid_0's multi_logloss: 0.197747\n",
      "[70]\tvalid_0's multi_logloss: 0.197647\n",
      "[71]\tvalid_0's multi_logloss: 0.197561\n",
      "[72]\tvalid_0's multi_logloss: 0.197477\n",
      "[73]\tvalid_0's multi_logloss: 0.19741\n",
      "[74]\tvalid_0's multi_logloss: 0.197322\n",
      "[75]\tvalid_0's multi_logloss: 0.197264\n",
      "[76]\tvalid_0's multi_logloss: 0.197203\n",
      "[77]\tvalid_0's multi_logloss: 0.197127\n",
      "[78]\tvalid_0's multi_logloss: 0.197054\n",
      "[79]\tvalid_0's multi_logloss: 0.19699\n",
      "[80]\tvalid_0's multi_logloss: 0.196929\n",
      "[81]\tvalid_0's multi_logloss: 0.196875\n",
      "[82]\tvalid_0's multi_logloss: 0.19681\n",
      "[83]\tvalid_0's multi_logloss: 0.196753\n",
      "[84]\tvalid_0's multi_logloss: 0.19669\n",
      "[85]\tvalid_0's multi_logloss: 0.196631\n",
      "[86]\tvalid_0's multi_logloss: 0.196593\n",
      "[87]\tvalid_0's multi_logloss: 0.196535\n",
      "[88]\tvalid_0's multi_logloss: 0.196485\n",
      "[89]\tvalid_0's multi_logloss: 0.196442\n",
      "[90]\tvalid_0's multi_logloss: 0.196397\n",
      "[91]\tvalid_0's multi_logloss: 0.19635\n",
      "[92]\tvalid_0's multi_logloss: 0.196306\n",
      "[93]\tvalid_0's multi_logloss: 0.196263\n",
      "[94]\tvalid_0's multi_logloss: 0.196216\n",
      "[95]\tvalid_0's multi_logloss: 0.196168\n",
      "[96]\tvalid_0's multi_logloss: 0.196132\n",
      "[97]\tvalid_0's multi_logloss: 0.196103\n",
      "[98]\tvalid_0's multi_logloss: 0.196062\n",
      "[99]\tvalid_0's multi_logloss: 0.196024\n",
      "[100]\tvalid_0's multi_logloss: 0.19599\n",
      "[101]\tvalid_0's multi_logloss: 0.195957\n",
      "[102]\tvalid_0's multi_logloss: 0.195922\n",
      "[103]\tvalid_0's multi_logloss: 0.195891\n",
      "[104]\tvalid_0's multi_logloss: 0.195862\n",
      "[105]\tvalid_0's multi_logloss: 0.195831\n",
      "[106]\tvalid_0's multi_logloss: 0.195795\n",
      "[107]\tvalid_0's multi_logloss: 0.195758\n",
      "[108]\tvalid_0's multi_logloss: 0.195725\n",
      "[109]\tvalid_0's multi_logloss: 0.195694\n",
      "[110]\tvalid_0's multi_logloss: 0.195661\n",
      "[111]\tvalid_0's multi_logloss: 0.195622\n",
      "[112]\tvalid_0's multi_logloss: 0.195594\n",
      "[113]\tvalid_0's multi_logloss: 0.195562\n",
      "[114]\tvalid_0's multi_logloss: 0.195532\n",
      "[115]\tvalid_0's multi_logloss: 0.195506\n",
      "[116]\tvalid_0's multi_logloss: 0.19548\n",
      "[117]\tvalid_0's multi_logloss: 0.19546\n",
      "[118]\tvalid_0's multi_logloss: 0.195438\n",
      "[119]\tvalid_0's multi_logloss: 0.195417\n",
      "[120]\tvalid_0's multi_logloss: 0.195392\n",
      "[121]\tvalid_0's multi_logloss: 0.195368\n",
      "[122]\tvalid_0's multi_logloss: 0.195344\n",
      "[123]\tvalid_0's multi_logloss: 0.19532\n",
      "[124]\tvalid_0's multi_logloss: 0.1953\n",
      "[125]\tvalid_0's multi_logloss: 0.195278\n",
      "[126]\tvalid_0's multi_logloss: 0.195255\n",
      "[127]\tvalid_0's multi_logloss: 0.195237\n",
      "[128]\tvalid_0's multi_logloss: 0.195217\n",
      "[129]\tvalid_0's multi_logloss: 0.195198\n",
      "[130]\tvalid_0's multi_logloss: 0.195177\n",
      "[131]\tvalid_0's multi_logloss: 0.19516\n",
      "[132]\tvalid_0's multi_logloss: 0.19514\n",
      "[133]\tvalid_0's multi_logloss: 0.195119\n",
      "[134]\tvalid_0's multi_logloss: 0.195099\n",
      "[135]\tvalid_0's multi_logloss: 0.195087\n",
      "[136]\tvalid_0's multi_logloss: 0.195065\n",
      "[137]\tvalid_0's multi_logloss: 0.195042\n",
      "[138]\tvalid_0's multi_logloss: 0.195028\n",
      "[139]\tvalid_0's multi_logloss: 0.195008\n",
      "[140]\tvalid_0's multi_logloss: 0.194991\n",
      "[141]\tvalid_0's multi_logloss: 0.19497\n",
      "[142]\tvalid_0's multi_logloss: 0.194947\n",
      "[143]\tvalid_0's multi_logloss: 0.194933\n",
      "[144]\tvalid_0's multi_logloss: 0.194916\n",
      "[145]\tvalid_0's multi_logloss: 0.194898\n",
      "[146]\tvalid_0's multi_logloss: 0.194874\n",
      "[147]\tvalid_0's multi_logloss: 0.194858\n",
      "[148]\tvalid_0's multi_logloss: 0.194839\n",
      "[149]\tvalid_0's multi_logloss: 0.194816\n",
      "[150]\tvalid_0's multi_logloss: 0.194803\n",
      "[151]\tvalid_0's multi_logloss: 0.194789\n",
      "[152]\tvalid_0's multi_logloss: 0.194776\n",
      "[153]\tvalid_0's multi_logloss: 0.194759\n",
      "[154]\tvalid_0's multi_logloss: 0.194737\n",
      "[155]\tvalid_0's multi_logloss: 0.194714\n",
      "[156]\tvalid_0's multi_logloss: 0.1947\n",
      "[157]\tvalid_0's multi_logloss: 0.194685\n",
      "[158]\tvalid_0's multi_logloss: 0.194669\n",
      "[159]\tvalid_0's multi_logloss: 0.19466\n",
      "[160]\tvalid_0's multi_logloss: 0.194645\n",
      "[161]\tvalid_0's multi_logloss: 0.194631\n",
      "[162]\tvalid_0's multi_logloss: 0.194615\n",
      "[163]\tvalid_0's multi_logloss: 0.194601\n",
      "[164]\tvalid_0's multi_logloss: 0.194588\n",
      "[165]\tvalid_0's multi_logloss: 0.194575\n",
      "[166]\tvalid_0's multi_logloss: 0.194558\n",
      "[167]\tvalid_0's multi_logloss: 0.194536\n",
      "[168]\tvalid_0's multi_logloss: 0.194523\n",
      "[169]\tvalid_0's multi_logloss: 0.194508\n",
      "[170]\tvalid_0's multi_logloss: 0.194497\n",
      "[171]\tvalid_0's multi_logloss: 0.194486\n",
      "[172]\tvalid_0's multi_logloss: 0.194472\n",
      "[173]\tvalid_0's multi_logloss: 0.194459\n",
      "[174]\tvalid_0's multi_logloss: 0.194449\n",
      "[175]\tvalid_0's multi_logloss: 0.194435\n",
      "[176]\tvalid_0's multi_logloss: 0.194424\n",
      "[177]\tvalid_0's multi_logloss: 0.19441\n",
      "[178]\tvalid_0's multi_logloss: 0.194396\n",
      "[179]\tvalid_0's multi_logloss: 0.194383\n",
      "[180]\tvalid_0's multi_logloss: 0.194369\n",
      "[181]\tvalid_0's multi_logloss: 0.194358\n",
      "[182]\tvalid_0's multi_logloss: 0.19434\n",
      "[183]\tvalid_0's multi_logloss: 0.194328\n",
      "[184]\tvalid_0's multi_logloss: 0.194316\n",
      "[185]\tvalid_0's multi_logloss: 0.194309\n",
      "[186]\tvalid_0's multi_logloss: 0.194291\n",
      "[187]\tvalid_0's multi_logloss: 0.194281\n",
      "[188]\tvalid_0's multi_logloss: 0.194269\n",
      "[189]\tvalid_0's multi_logloss: 0.19426\n",
      "[190]\tvalid_0's multi_logloss: 0.194246\n",
      "[191]\tvalid_0's multi_logloss: 0.194237\n",
      "[192]\tvalid_0's multi_logloss: 0.194226\n",
      "[193]\tvalid_0's multi_logloss: 0.194214\n",
      "[194]\tvalid_0's multi_logloss: 0.194203\n",
      "[195]\tvalid_0's multi_logloss: 0.19419\n",
      "[196]\tvalid_0's multi_logloss: 0.194177\n",
      "[197]\tvalid_0's multi_logloss: 0.194165\n",
      "[198]\tvalid_0's multi_logloss: 0.194155\n",
      "[199]\tvalid_0's multi_logloss: 0.194145\n",
      "[200]\tvalid_0's multi_logloss: 0.194122\n",
      "[201]\tvalid_0's multi_logloss: 0.19411\n",
      "[202]\tvalid_0's multi_logloss: 0.194091\n",
      "[203]\tvalid_0's multi_logloss: 0.194079\n",
      "[204]\tvalid_0's multi_logloss: 0.194069\n",
      "[205]\tvalid_0's multi_logloss: 0.194059\n",
      "[206]\tvalid_0's multi_logloss: 0.194051\n",
      "[207]\tvalid_0's multi_logloss: 0.194039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208]\tvalid_0's multi_logloss: 0.194018\n",
      "[209]\tvalid_0's multi_logloss: 0.194009\n",
      "[210]\tvalid_0's multi_logloss: 0.193993\n",
      "[211]\tvalid_0's multi_logloss: 0.193983\n",
      "[212]\tvalid_0's multi_logloss: 0.193968\n",
      "[213]\tvalid_0's multi_logloss: 0.193955\n",
      "[214]\tvalid_0's multi_logloss: 0.193947\n",
      "[215]\tvalid_0's multi_logloss: 0.193938\n",
      "[216]\tvalid_0's multi_logloss: 0.193927\n",
      "[217]\tvalid_0's multi_logloss: 0.193915\n",
      "[218]\tvalid_0's multi_logloss: 0.193901\n",
      "[219]\tvalid_0's multi_logloss: 0.193892\n",
      "[220]\tvalid_0's multi_logloss: 0.193878\n",
      "[221]\tvalid_0's multi_logloss: 0.193866\n",
      "[222]\tvalid_0's multi_logloss: 0.193855\n",
      "[223]\tvalid_0's multi_logloss: 0.193841\n",
      "[224]\tvalid_0's multi_logloss: 0.193835\n",
      "[225]\tvalid_0's multi_logloss: 0.193826\n",
      "[226]\tvalid_0's multi_logloss: 0.193816\n",
      "[227]\tvalid_0's multi_logloss: 0.193806\n",
      "[228]\tvalid_0's multi_logloss: 0.193794\n",
      "[229]\tvalid_0's multi_logloss: 0.193785\n",
      "[230]\tvalid_0's multi_logloss: 0.193776\n",
      "[231]\tvalid_0's multi_logloss: 0.193765\n",
      "[232]\tvalid_0's multi_logloss: 0.193752\n",
      "[233]\tvalid_0's multi_logloss: 0.193745\n",
      "[234]\tvalid_0's multi_logloss: 0.193734\n",
      "[235]\tvalid_0's multi_logloss: 0.193727\n",
      "[236]\tvalid_0's multi_logloss: 0.193717\n",
      "[237]\tvalid_0's multi_logloss: 0.193709\n",
      "[238]\tvalid_0's multi_logloss: 0.193696\n",
      "[239]\tvalid_0's multi_logloss: 0.193686\n",
      "[240]\tvalid_0's multi_logloss: 0.193678\n",
      "[241]\tvalid_0's multi_logloss: 0.193668\n",
      "[242]\tvalid_0's multi_logloss: 0.193653\n",
      "[243]\tvalid_0's multi_logloss: 0.19364\n",
      "[244]\tvalid_0's multi_logloss: 0.193631\n",
      "[245]\tvalid_0's multi_logloss: 0.193623\n",
      "[246]\tvalid_0's multi_logloss: 0.193608\n",
      "[247]\tvalid_0's multi_logloss: 0.193603\n",
      "[248]\tvalid_0's multi_logloss: 0.193592\n",
      "[249]\tvalid_0's multi_logloss: 0.193583\n",
      "[250]\tvalid_0's multi_logloss: 0.193574\n",
      "[251]\tvalid_0's multi_logloss: 0.193563\n",
      "[252]\tvalid_0's multi_logloss: 0.193554\n",
      "[253]\tvalid_0's multi_logloss: 0.193541\n",
      "[254]\tvalid_0's multi_logloss: 0.193531\n",
      "[255]\tvalid_0's multi_logloss: 0.193522\n",
      "[256]\tvalid_0's multi_logloss: 0.193508\n",
      "[257]\tvalid_0's multi_logloss: 0.1935\n",
      "[258]\tvalid_0's multi_logloss: 0.193489\n",
      "[259]\tvalid_0's multi_logloss: 0.193483\n",
      "[260]\tvalid_0's multi_logloss: 0.193473\n",
      "[261]\tvalid_0's multi_logloss: 0.193464\n",
      "[262]\tvalid_0's multi_logloss: 0.193452\n",
      "[263]\tvalid_0's multi_logloss: 0.193443\n",
      "[264]\tvalid_0's multi_logloss: 0.193439\n",
      "[265]\tvalid_0's multi_logloss: 0.193424\n",
      "[266]\tvalid_0's multi_logloss: 0.193412\n",
      "[267]\tvalid_0's multi_logloss: 0.193403\n",
      "[268]\tvalid_0's multi_logloss: 0.193396\n",
      "[269]\tvalid_0's multi_logloss: 0.193386\n",
      "[270]\tvalid_0's multi_logloss: 0.193372\n",
      "[271]\tvalid_0's multi_logloss: 0.193363\n",
      "[272]\tvalid_0's multi_logloss: 0.193351\n",
      "[273]\tvalid_0's multi_logloss: 0.193342\n",
      "[274]\tvalid_0's multi_logloss: 0.193329\n",
      "[275]\tvalid_0's multi_logloss: 0.193316\n",
      "[276]\tvalid_0's multi_logloss: 0.19331\n",
      "[277]\tvalid_0's multi_logloss: 0.193305\n",
      "[278]\tvalid_0's multi_logloss: 0.193297\n",
      "[279]\tvalid_0's multi_logloss: 0.193287\n",
      "[280]\tvalid_0's multi_logloss: 0.193274\n",
      "[281]\tvalid_0's multi_logloss: 0.193265\n",
      "[282]\tvalid_0's multi_logloss: 0.193263\n",
      "[283]\tvalid_0's multi_logloss: 0.193248\n",
      "[284]\tvalid_0's multi_logloss: 0.193237\n",
      "[285]\tvalid_0's multi_logloss: 0.193231\n",
      "[286]\tvalid_0's multi_logloss: 0.19322\n",
      "[287]\tvalid_0's multi_logloss: 0.193213\n",
      "[288]\tvalid_0's multi_logloss: 0.193193\n",
      "[289]\tvalid_0's multi_logloss: 0.193186\n",
      "[290]\tvalid_0's multi_logloss: 0.193181\n",
      "[291]\tvalid_0's multi_logloss: 0.193171\n",
      "[292]\tvalid_0's multi_logloss: 0.193163\n",
      "[293]\tvalid_0's multi_logloss: 0.193157\n",
      "[294]\tvalid_0's multi_logloss: 0.193151\n",
      "[295]\tvalid_0's multi_logloss: 0.193142\n",
      "[296]\tvalid_0's multi_logloss: 0.19313\n",
      "[297]\tvalid_0's multi_logloss: 0.193119\n",
      "[298]\tvalid_0's multi_logloss: 0.193109\n",
      "[299]\tvalid_0's multi_logloss: 0.193098\n",
      "[300]\tvalid_0's multi_logloss: 0.193092\n",
      "[301]\tvalid_0's multi_logloss: 0.193082\n",
      "[302]\tvalid_0's multi_logloss: 0.193072\n",
      "[303]\tvalid_0's multi_logloss: 0.193065\n",
      "[304]\tvalid_0's multi_logloss: 0.193054\n",
      "[305]\tvalid_0's multi_logloss: 0.193044\n",
      "[306]\tvalid_0's multi_logloss: 0.193039\n",
      "[307]\tvalid_0's multi_logloss: 0.193033\n",
      "[308]\tvalid_0's multi_logloss: 0.193023\n",
      "[309]\tvalid_0's multi_logloss: 0.193014\n",
      "[310]\tvalid_0's multi_logloss: 0.193008\n",
      "[311]\tvalid_0's multi_logloss: 0.192997\n",
      "[312]\tvalid_0's multi_logloss: 0.192988\n",
      "[313]\tvalid_0's multi_logloss: 0.192977\n",
      "[314]\tvalid_0's multi_logloss: 0.192969\n",
      "[315]\tvalid_0's multi_logloss: 0.19296\n",
      "[316]\tvalid_0's multi_logloss: 0.192951\n",
      "[317]\tvalid_0's multi_logloss: 0.192939\n",
      "[318]\tvalid_0's multi_logloss: 0.192929\n",
      "[319]\tvalid_0's multi_logloss: 0.192924\n",
      "[320]\tvalid_0's multi_logloss: 0.192918\n",
      "[321]\tvalid_0's multi_logloss: 0.192908\n",
      "[322]\tvalid_0's multi_logloss: 0.1929\n",
      "[323]\tvalid_0's multi_logloss: 0.192894\n",
      "[324]\tvalid_0's multi_logloss: 0.192891\n",
      "[325]\tvalid_0's multi_logloss: 0.192885\n",
      "[326]\tvalid_0's multi_logloss: 0.192872\n",
      "[327]\tvalid_0's multi_logloss: 0.192864\n",
      "[328]\tvalid_0's multi_logloss: 0.192851\n",
      "[329]\tvalid_0's multi_logloss: 0.192843\n",
      "[330]\tvalid_0's multi_logloss: 0.192833\n",
      "[331]\tvalid_0's multi_logloss: 0.192822\n",
      "[332]\tvalid_0's multi_logloss: 0.192817\n",
      "[333]\tvalid_0's multi_logloss: 0.192807\n",
      "[334]\tvalid_0's multi_logloss: 0.192793\n",
      "[335]\tvalid_0's multi_logloss: 0.192783\n",
      "[336]\tvalid_0's multi_logloss: 0.192773\n",
      "[337]\tvalid_0's multi_logloss: 0.192761\n",
      "[338]\tvalid_0's multi_logloss: 0.192754\n",
      "[339]\tvalid_0's multi_logloss: 0.192753\n",
      "[340]\tvalid_0's multi_logloss: 0.192751\n",
      "[341]\tvalid_0's multi_logloss: 0.192746\n",
      "[342]\tvalid_0's multi_logloss: 0.192736\n",
      "[343]\tvalid_0's multi_logloss: 0.192737\n",
      "[344]\tvalid_0's multi_logloss: 0.19272\n",
      "[345]\tvalid_0's multi_logloss: 0.192708\n",
      "[346]\tvalid_0's multi_logloss: 0.1927\n",
      "[347]\tvalid_0's multi_logloss: 0.192694\n",
      "[348]\tvalid_0's multi_logloss: 0.192688\n",
      "[349]\tvalid_0's multi_logloss: 0.192686\n",
      "[350]\tvalid_0's multi_logloss: 0.192678\n",
      "[351]\tvalid_0's multi_logloss: 0.192666\n",
      "[352]\tvalid_0's multi_logloss: 0.192658\n",
      "[353]\tvalid_0's multi_logloss: 0.192654\n",
      "[354]\tvalid_0's multi_logloss: 0.192647\n",
      "[355]\tvalid_0's multi_logloss: 0.192639\n",
      "[356]\tvalid_0's multi_logloss: 0.192635\n",
      "[357]\tvalid_0's multi_logloss: 0.192625\n",
      "[358]\tvalid_0's multi_logloss: 0.192619\n",
      "[359]\tvalid_0's multi_logloss: 0.192612\n",
      "[360]\tvalid_0's multi_logloss: 0.1926\n",
      "[361]\tvalid_0's multi_logloss: 0.192596\n",
      "[362]\tvalid_0's multi_logloss: 0.192591\n",
      "[363]\tvalid_0's multi_logloss: 0.192581\n",
      "[364]\tvalid_0's multi_logloss: 0.192569\n",
      "[365]\tvalid_0's multi_logloss: 0.192561\n",
      "[366]\tvalid_0's multi_logloss: 0.192551\n",
      "[367]\tvalid_0's multi_logloss: 0.192546\n",
      "[368]\tvalid_0's multi_logloss: 0.192541\n",
      "[369]\tvalid_0's multi_logloss: 0.192534\n",
      "[370]\tvalid_0's multi_logloss: 0.192525\n",
      "[371]\tvalid_0's multi_logloss: 0.192519\n",
      "[372]\tvalid_0's multi_logloss: 0.19251\n",
      "[373]\tvalid_0's multi_logloss: 0.192501\n",
      "[374]\tvalid_0's multi_logloss: 0.192492\n",
      "[375]\tvalid_0's multi_logloss: 0.192481\n",
      "[376]\tvalid_0's multi_logloss: 0.19247\n",
      "[377]\tvalid_0's multi_logloss: 0.192465\n",
      "[378]\tvalid_0's multi_logloss: 0.192458\n",
      "[379]\tvalid_0's multi_logloss: 0.192448\n",
      "[380]\tvalid_0's multi_logloss: 0.192442\n",
      "[381]\tvalid_0's multi_logloss: 0.192435\n",
      "[382]\tvalid_0's multi_logloss: 0.192421\n",
      "[383]\tvalid_0's multi_logloss: 0.192413\n",
      "[384]\tvalid_0's multi_logloss: 0.192406\n",
      "[385]\tvalid_0's multi_logloss: 0.192397\n",
      "[386]\tvalid_0's multi_logloss: 0.192389\n",
      "[387]\tvalid_0's multi_logloss: 0.192383\n",
      "[388]\tvalid_0's multi_logloss: 0.192375\n",
      "[389]\tvalid_0's multi_logloss: 0.192366\n",
      "[390]\tvalid_0's multi_logloss: 0.192363\n",
      "[391]\tvalid_0's multi_logloss: 0.192355\n",
      "[392]\tvalid_0's multi_logloss: 0.192343\n",
      "[393]\tvalid_0's multi_logloss: 0.192334\n",
      "[394]\tvalid_0's multi_logloss: 0.192325\n",
      "[395]\tvalid_0's multi_logloss: 0.192315\n",
      "[396]\tvalid_0's multi_logloss: 0.192307\n",
      "[397]\tvalid_0's multi_logloss: 0.192303\n",
      "[398]\tvalid_0's multi_logloss: 0.192301\n",
      "[399]\tvalid_0's multi_logloss: 0.19229\n",
      "[400]\tvalid_0's multi_logloss: 0.192284\n",
      "[401]\tvalid_0's multi_logloss: 0.192283\n",
      "[402]\tvalid_0's multi_logloss: 0.192279\n",
      "[403]\tvalid_0's multi_logloss: 0.192286\n",
      "[404]\tvalid_0's multi_logloss: 0.192279\n",
      "[405]\tvalid_0's multi_logloss: 0.192272\n",
      "[406]\tvalid_0's multi_logloss: 0.192264\n",
      "[407]\tvalid_0's multi_logloss: 0.192257\n",
      "[408]\tvalid_0's multi_logloss: 0.192248\n",
      "[409]\tvalid_0's multi_logloss: 0.19224\n",
      "[410]\tvalid_0's multi_logloss: 0.192237\n",
      "[411]\tvalid_0's multi_logloss: 0.19223\n",
      "[412]\tvalid_0's multi_logloss: 0.192224\n",
      "[413]\tvalid_0's multi_logloss: 0.192216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[414]\tvalid_0's multi_logloss: 0.192209\n",
      "[415]\tvalid_0's multi_logloss: 0.192202\n",
      "[416]\tvalid_0's multi_logloss: 0.192192\n",
      "[417]\tvalid_0's multi_logloss: 0.192183\n",
      "[418]\tvalid_0's multi_logloss: 0.192177\n",
      "[419]\tvalid_0's multi_logloss: 0.192168\n",
      "[420]\tvalid_0's multi_logloss: 0.192161\n",
      "[421]\tvalid_0's multi_logloss: 0.19215\n",
      "[422]\tvalid_0's multi_logloss: 0.192144\n",
      "[423]\tvalid_0's multi_logloss: 0.192135\n",
      "[424]\tvalid_0's multi_logloss: 0.192127\n",
      "[425]\tvalid_0's multi_logloss: 0.192122\n",
      "[426]\tvalid_0's multi_logloss: 0.192109\n",
      "[427]\tvalid_0's multi_logloss: 0.192101\n",
      "[428]\tvalid_0's multi_logloss: 0.192094\n",
      "[429]\tvalid_0's multi_logloss: 0.19209\n",
      "[430]\tvalid_0's multi_logloss: 0.192082\n",
      "[431]\tvalid_0's multi_logloss: 0.192073\n",
      "[432]\tvalid_0's multi_logloss: 0.192067\n",
      "[433]\tvalid_0's multi_logloss: 0.192061\n",
      "[434]\tvalid_0's multi_logloss: 0.192053\n",
      "[435]\tvalid_0's multi_logloss: 0.192034\n",
      "[436]\tvalid_0's multi_logloss: 0.192026\n",
      "[437]\tvalid_0's multi_logloss: 0.192019\n",
      "[438]\tvalid_0's multi_logloss: 0.192012\n",
      "[439]\tvalid_0's multi_logloss: 0.192006\n",
      "[440]\tvalid_0's multi_logloss: 0.191996\n",
      "[441]\tvalid_0's multi_logloss: 0.191993\n",
      "[442]\tvalid_0's multi_logloss: 0.191986\n",
      "[443]\tvalid_0's multi_logloss: 0.191978\n",
      "[444]\tvalid_0's multi_logloss: 0.19197\n",
      "[445]\tvalid_0's multi_logloss: 0.191965\n",
      "[446]\tvalid_0's multi_logloss: 0.191957\n",
      "[447]\tvalid_0's multi_logloss: 0.191955\n",
      "[448]\tvalid_0's multi_logloss: 0.191951\n",
      "[449]\tvalid_0's multi_logloss: 0.191943\n",
      "[450]\tvalid_0's multi_logloss: 0.191938\n",
      "[451]\tvalid_0's multi_logloss: 0.191932\n",
      "[452]\tvalid_0's multi_logloss: 0.191926\n",
      "[453]\tvalid_0's multi_logloss: 0.191917\n",
      "[454]\tvalid_0's multi_logloss: 0.191907\n",
      "[455]\tvalid_0's multi_logloss: 0.191901\n",
      "[456]\tvalid_0's multi_logloss: 0.19189\n",
      "[457]\tvalid_0's multi_logloss: 0.191893\n",
      "[458]\tvalid_0's multi_logloss: 0.191888\n",
      "[459]\tvalid_0's multi_logloss: 0.191884\n",
      "[460]\tvalid_0's multi_logloss: 0.191877\n",
      "[461]\tvalid_0's multi_logloss: 0.191872\n",
      "[462]\tvalid_0's multi_logloss: 0.191862\n",
      "[463]\tvalid_0's multi_logloss: 0.191856\n",
      "[464]\tvalid_0's multi_logloss: 0.19185\n",
      "[465]\tvalid_0's multi_logloss: 0.191844\n",
      "[466]\tvalid_0's multi_logloss: 0.191838\n",
      "[467]\tvalid_0's multi_logloss: 0.191827\n",
      "[468]\tvalid_0's multi_logloss: 0.191818\n",
      "[469]\tvalid_0's multi_logloss: 0.191811\n",
      "[470]\tvalid_0's multi_logloss: 0.191806\n",
      "[471]\tvalid_0's multi_logloss: 0.191796\n",
      "[472]\tvalid_0's multi_logloss: 0.191788\n",
      "[473]\tvalid_0's multi_logloss: 0.191784\n",
      "[474]\tvalid_0's multi_logloss: 0.19178\n",
      "[475]\tvalid_0's multi_logloss: 0.191774\n",
      "[476]\tvalid_0's multi_logloss: 0.191767\n",
      "[477]\tvalid_0's multi_logloss: 0.191757\n",
      "[478]\tvalid_0's multi_logloss: 0.191762\n",
      "[479]\tvalid_0's multi_logloss: 0.191757\n",
      "[480]\tvalid_0's multi_logloss: 0.191748\n",
      "[481]\tvalid_0's multi_logloss: 0.191743\n",
      "[482]\tvalid_0's multi_logloss: 0.19174\n",
      "[483]\tvalid_0's multi_logloss: 0.191732\n",
      "[484]\tvalid_0's multi_logloss: 0.191727\n",
      "[485]\tvalid_0's multi_logloss: 0.191721\n",
      "[486]\tvalid_0's multi_logloss: 0.191714\n",
      "[487]\tvalid_0's multi_logloss: 0.191709\n",
      "[488]\tvalid_0's multi_logloss: 0.191707\n",
      "[489]\tvalid_0's multi_logloss: 0.191708\n",
      "[490]\tvalid_0's multi_logloss: 0.191702\n",
      "[491]\tvalid_0's multi_logloss: 0.191689\n",
      "[492]\tvalid_0's multi_logloss: 0.19168\n",
      "[493]\tvalid_0's multi_logloss: 0.191672\n",
      "[494]\tvalid_0's multi_logloss: 0.191663\n",
      "[495]\tvalid_0's multi_logloss: 0.191659\n",
      "[496]\tvalid_0's multi_logloss: 0.191655\n",
      "[497]\tvalid_0's multi_logloss: 0.191648\n",
      "[498]\tvalid_0's multi_logloss: 0.191641\n",
      "[499]\tvalid_0's multi_logloss: 0.191634\n",
      "[500]\tvalid_0's multi_logloss: 0.191628\n",
      "[501]\tvalid_0's multi_logloss: 0.191619\n",
      "[502]\tvalid_0's multi_logloss: 0.191614\n",
      "[503]\tvalid_0's multi_logloss: 0.19161\n",
      "[504]\tvalid_0's multi_logloss: 0.191602\n",
      "[505]\tvalid_0's multi_logloss: 0.191592\n",
      "[506]\tvalid_0's multi_logloss: 0.191578\n",
      "[507]\tvalid_0's multi_logloss: 0.191573\n",
      "[508]\tvalid_0's multi_logloss: 0.191564\n",
      "[509]\tvalid_0's multi_logloss: 0.191545\n",
      "[510]\tvalid_0's multi_logloss: 0.191539\n",
      "[511]\tvalid_0's multi_logloss: 0.191532\n",
      "[512]\tvalid_0's multi_logloss: 0.191525\n",
      "[513]\tvalid_0's multi_logloss: 0.191519\n",
      "[514]\tvalid_0's multi_logloss: 0.191513\n",
      "[515]\tvalid_0's multi_logloss: 0.191509\n",
      "[516]\tvalid_0's multi_logloss: 0.191502\n",
      "[517]\tvalid_0's multi_logloss: 0.191494\n",
      "[518]\tvalid_0's multi_logloss: 0.191486\n",
      "[519]\tvalid_0's multi_logloss: 0.191484\n",
      "[520]\tvalid_0's multi_logloss: 0.19148\n",
      "[521]\tvalid_0's multi_logloss: 0.191471\n",
      "[522]\tvalid_0's multi_logloss: 0.191466\n",
      "[523]\tvalid_0's multi_logloss: 0.191464\n",
      "[524]\tvalid_0's multi_logloss: 0.191461\n",
      "[525]\tvalid_0's multi_logloss: 0.191454\n",
      "[526]\tvalid_0's multi_logloss: 0.191445\n",
      "[527]\tvalid_0's multi_logloss: 0.191449\n",
      "[528]\tvalid_0's multi_logloss: 0.191443\n",
      "[529]\tvalid_0's multi_logloss: 0.191437\n",
      "[530]\tvalid_0's multi_logloss: 0.191432\n",
      "[531]\tvalid_0's multi_logloss: 0.191427\n",
      "[532]\tvalid_0's multi_logloss: 0.191423\n",
      "[533]\tvalid_0's multi_logloss: 0.191415\n",
      "[534]\tvalid_0's multi_logloss: 0.191407\n",
      "[535]\tvalid_0's multi_logloss: 0.191403\n",
      "[536]\tvalid_0's multi_logloss: 0.191397\n",
      "[537]\tvalid_0's multi_logloss: 0.191388\n",
      "[538]\tvalid_0's multi_logloss: 0.191386\n",
      "[539]\tvalid_0's multi_logloss: 0.191379\n",
      "[540]\tvalid_0's multi_logloss: 0.191372\n",
      "[541]\tvalid_0's multi_logloss: 0.191367\n",
      "[542]\tvalid_0's multi_logloss: 0.191361\n",
      "[543]\tvalid_0's multi_logloss: 0.191357\n",
      "[544]\tvalid_0's multi_logloss: 0.19135\n",
      "[545]\tvalid_0's multi_logloss: 0.191348\n",
      "[546]\tvalid_0's multi_logloss: 0.191342\n",
      "[547]\tvalid_0's multi_logloss: 0.191338\n",
      "[548]\tvalid_0's multi_logloss: 0.191334\n",
      "[549]\tvalid_0's multi_logloss: 0.191324\n",
      "[550]\tvalid_0's multi_logloss: 0.191317\n",
      "[551]\tvalid_0's multi_logloss: 0.191311\n",
      "[552]\tvalid_0's multi_logloss: 0.191302\n",
      "[553]\tvalid_0's multi_logloss: 0.191299\n",
      "[554]\tvalid_0's multi_logloss: 0.191288\n",
      "[555]\tvalid_0's multi_logloss: 0.191285\n",
      "[556]\tvalid_0's multi_logloss: 0.191271\n",
      "[557]\tvalid_0's multi_logloss: 0.191264\n",
      "[558]\tvalid_0's multi_logloss: 0.191262\n",
      "[559]\tvalid_0's multi_logloss: 0.191256\n",
      "[560]\tvalid_0's multi_logloss: 0.19125\n",
      "[561]\tvalid_0's multi_logloss: 0.191246\n",
      "[562]\tvalid_0's multi_logloss: 0.191239\n",
      "[563]\tvalid_0's multi_logloss: 0.191235\n",
      "[564]\tvalid_0's multi_logloss: 0.191229\n",
      "[565]\tvalid_0's multi_logloss: 0.191221\n",
      "[566]\tvalid_0's multi_logloss: 0.191214\n",
      "[567]\tvalid_0's multi_logloss: 0.19121\n",
      "[568]\tvalid_0's multi_logloss: 0.1912\n",
      "[569]\tvalid_0's multi_logloss: 0.191194\n",
      "[570]\tvalid_0's multi_logloss: 0.191187\n",
      "[571]\tvalid_0's multi_logloss: 0.191183\n",
      "[572]\tvalid_0's multi_logloss: 0.191175\n",
      "[573]\tvalid_0's multi_logloss: 0.19117\n",
      "[574]\tvalid_0's multi_logloss: 0.191164\n",
      "[575]\tvalid_0's multi_logloss: 0.191157\n",
      "[576]\tvalid_0's multi_logloss: 0.19115\n",
      "[577]\tvalid_0's multi_logloss: 0.191142\n",
      "[578]\tvalid_0's multi_logloss: 0.191139\n",
      "[579]\tvalid_0's multi_logloss: 0.191135\n",
      "[580]\tvalid_0's multi_logloss: 0.19113\n",
      "[581]\tvalid_0's multi_logloss: 0.191121\n",
      "[582]\tvalid_0's multi_logloss: 0.191115\n",
      "[583]\tvalid_0's multi_logloss: 0.19111\n",
      "[584]\tvalid_0's multi_logloss: 0.191108\n",
      "[585]\tvalid_0's multi_logloss: 0.191102\n",
      "[586]\tvalid_0's multi_logloss: 0.191099\n",
      "[587]\tvalid_0's multi_logloss: 0.191094\n",
      "[588]\tvalid_0's multi_logloss: 0.191088\n",
      "[589]\tvalid_0's multi_logloss: 0.191084\n",
      "[590]\tvalid_0's multi_logloss: 0.191078\n",
      "[591]\tvalid_0's multi_logloss: 0.191067\n",
      "[592]\tvalid_0's multi_logloss: 0.191065\n",
      "[593]\tvalid_0's multi_logloss: 0.191062\n",
      "[594]\tvalid_0's multi_logloss: 0.191055\n",
      "[595]\tvalid_0's multi_logloss: 0.191048\n",
      "[596]\tvalid_0's multi_logloss: 0.191042\n",
      "[597]\tvalid_0's multi_logloss: 0.191035\n",
      "[598]\tvalid_0's multi_logloss: 0.19103\n",
      "[599]\tvalid_0's multi_logloss: 0.191027\n",
      "[600]\tvalid_0's multi_logloss: 0.191017\n",
      "[601]\tvalid_0's multi_logloss: 0.191008\n",
      "[602]\tvalid_0's multi_logloss: 0.191003\n",
      "[603]\tvalid_0's multi_logloss: 0.190998\n",
      "[604]\tvalid_0's multi_logloss: 0.190991\n",
      "[605]\tvalid_0's multi_logloss: 0.190985\n",
      "[606]\tvalid_0's multi_logloss: 0.190979\n",
      "[607]\tvalid_0's multi_logloss: 0.190972\n",
      "[608]\tvalid_0's multi_logloss: 0.190966\n",
      "[609]\tvalid_0's multi_logloss: 0.190958\n",
      "[610]\tvalid_0's multi_logloss: 0.190954\n",
      "[611]\tvalid_0's multi_logloss: 0.190949\n",
      "[612]\tvalid_0's multi_logloss: 0.190945\n",
      "[613]\tvalid_0's multi_logloss: 0.19094\n",
      "[614]\tvalid_0's multi_logloss: 0.190935\n",
      "[615]\tvalid_0's multi_logloss: 0.190931\n",
      "[616]\tvalid_0's multi_logloss: 0.190925\n",
      "[617]\tvalid_0's multi_logloss: 0.190922\n",
      "[618]\tvalid_0's multi_logloss: 0.190918\n",
      "[619]\tvalid_0's multi_logloss: 0.190915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[620]\tvalid_0's multi_logloss: 0.190909\n",
      "[621]\tvalid_0's multi_logloss: 0.190904\n",
      "[622]\tvalid_0's multi_logloss: 0.190897\n",
      "[623]\tvalid_0's multi_logloss: 0.19089\n",
      "[624]\tvalid_0's multi_logloss: 0.190884\n",
      "[625]\tvalid_0's multi_logloss: 0.190879\n",
      "[626]\tvalid_0's multi_logloss: 0.190874\n",
      "[627]\tvalid_0's multi_logloss: 0.190868\n",
      "[628]\tvalid_0's multi_logloss: 0.190864\n",
      "[629]\tvalid_0's multi_logloss: 0.190858\n",
      "[630]\tvalid_0's multi_logloss: 0.190852\n",
      "[631]\tvalid_0's multi_logloss: 0.190843\n",
      "[632]\tvalid_0's multi_logloss: 0.190838\n",
      "[633]\tvalid_0's multi_logloss: 0.190831\n",
      "[634]\tvalid_0's multi_logloss: 0.190824\n",
      "[635]\tvalid_0's multi_logloss: 0.190817\n",
      "[636]\tvalid_0's multi_logloss: 0.190809\n",
      "[637]\tvalid_0's multi_logloss: 0.190802\n",
      "[638]\tvalid_0's multi_logloss: 0.190796\n",
      "[639]\tvalid_0's multi_logloss: 0.190795\n",
      "[640]\tvalid_0's multi_logloss: 0.190792\n",
      "[641]\tvalid_0's multi_logloss: 0.190787\n",
      "[642]\tvalid_0's multi_logloss: 0.190781\n",
      "[643]\tvalid_0's multi_logloss: 0.190778\n",
      "[644]\tvalid_0's multi_logloss: 0.190773\n",
      "[645]\tvalid_0's multi_logloss: 0.190768\n",
      "[646]\tvalid_0's multi_logloss: 0.190762\n",
      "[647]\tvalid_0's multi_logloss: 0.190756\n",
      "[648]\tvalid_0's multi_logloss: 0.190754\n",
      "[649]\tvalid_0's multi_logloss: 0.190748\n",
      "[650]\tvalid_0's multi_logloss: 0.190743\n",
      "[651]\tvalid_0's multi_logloss: 0.190741\n",
      "[652]\tvalid_0's multi_logloss: 0.190734\n",
      "[653]\tvalid_0's multi_logloss: 0.190728\n",
      "[654]\tvalid_0's multi_logloss: 0.190722\n",
      "[655]\tvalid_0's multi_logloss: 0.190718\n",
      "[656]\tvalid_0's multi_logloss: 0.19071\n",
      "[657]\tvalid_0's multi_logloss: 0.190707\n",
      "[658]\tvalid_0's multi_logloss: 0.190701\n",
      "[659]\tvalid_0's multi_logloss: 0.190696\n",
      "[660]\tvalid_0's multi_logloss: 0.190691\n",
      "[661]\tvalid_0's multi_logloss: 0.190686\n",
      "[662]\tvalid_0's multi_logloss: 0.190681\n",
      "[663]\tvalid_0's multi_logloss: 0.190676\n",
      "[664]\tvalid_0's multi_logloss: 0.190672\n",
      "[665]\tvalid_0's multi_logloss: 0.190666\n",
      "[666]\tvalid_0's multi_logloss: 0.190663\n",
      "[667]\tvalid_0's multi_logloss: 0.190662\n",
      "[668]\tvalid_0's multi_logloss: 0.190654\n",
      "[669]\tvalid_0's multi_logloss: 0.190658\n",
      "[670]\tvalid_0's multi_logloss: 0.190653\n",
      "[671]\tvalid_0's multi_logloss: 0.190646\n",
      "[672]\tvalid_0's multi_logloss: 0.190641\n",
      "[673]\tvalid_0's multi_logloss: 0.190638\n",
      "[674]\tvalid_0's multi_logloss: 0.190635\n",
      "[675]\tvalid_0's multi_logloss: 0.190617\n",
      "[676]\tvalid_0's multi_logloss: 0.190614\n",
      "[677]\tvalid_0's multi_logloss: 0.190609\n",
      "[678]\tvalid_0's multi_logloss: 0.190602\n",
      "[679]\tvalid_0's multi_logloss: 0.190593\n",
      "[680]\tvalid_0's multi_logloss: 0.190588\n",
      "[681]\tvalid_0's multi_logloss: 0.190579\n",
      "[682]\tvalid_0's multi_logloss: 0.190574\n",
      "[683]\tvalid_0's multi_logloss: 0.190568\n",
      "[684]\tvalid_0's multi_logloss: 0.190562\n",
      "[685]\tvalid_0's multi_logloss: 0.190557\n",
      "[686]\tvalid_0's multi_logloss: 0.190553\n",
      "[687]\tvalid_0's multi_logloss: 0.190545\n",
      "[688]\tvalid_0's multi_logloss: 0.19054\n",
      "[689]\tvalid_0's multi_logloss: 0.190537\n",
      "[690]\tvalid_0's multi_logloss: 0.190532\n",
      "[691]\tvalid_0's multi_logloss: 0.190528\n",
      "[692]\tvalid_0's multi_logloss: 0.190523\n",
      "[693]\tvalid_0's multi_logloss: 0.190515\n",
      "[694]\tvalid_0's multi_logloss: 0.190509\n",
      "[695]\tvalid_0's multi_logloss: 0.190502\n",
      "[696]\tvalid_0's multi_logloss: 0.190496\n",
      "[697]\tvalid_0's multi_logloss: 0.190495\n",
      "[698]\tvalid_0's multi_logloss: 0.19049\n",
      "[699]\tvalid_0's multi_logloss: 0.190485\n",
      "[700]\tvalid_0's multi_logloss: 0.190481\n",
      "[701]\tvalid_0's multi_logloss: 0.190476\n",
      "[702]\tvalid_0's multi_logloss: 0.190473\n",
      "[703]\tvalid_0's multi_logloss: 0.19047\n",
      "[704]\tvalid_0's multi_logloss: 0.190465\n",
      "[705]\tvalid_0's multi_logloss: 0.190462\n",
      "[706]\tvalid_0's multi_logloss: 0.190458\n",
      "[707]\tvalid_0's multi_logloss: 0.190456\n",
      "[708]\tvalid_0's multi_logloss: 0.190449\n",
      "[709]\tvalid_0's multi_logloss: 0.190442\n",
      "[710]\tvalid_0's multi_logloss: 0.190436\n",
      "[711]\tvalid_0's multi_logloss: 0.190422\n",
      "[712]\tvalid_0's multi_logloss: 0.190416\n",
      "[713]\tvalid_0's multi_logloss: 0.190409\n",
      "[714]\tvalid_0's multi_logloss: 0.190404\n",
      "[715]\tvalid_0's multi_logloss: 0.190401\n",
      "[716]\tvalid_0's multi_logloss: 0.190393\n",
      "[717]\tvalid_0's multi_logloss: 0.190388\n",
      "[718]\tvalid_0's multi_logloss: 0.190382\n",
      "[719]\tvalid_0's multi_logloss: 0.190378\n",
      "[720]\tvalid_0's multi_logloss: 0.190375\n",
      "[721]\tvalid_0's multi_logloss: 0.190367\n",
      "[722]\tvalid_0's multi_logloss: 0.190365\n",
      "[723]\tvalid_0's multi_logloss: 0.190361\n",
      "[724]\tvalid_0's multi_logloss: 0.190356\n",
      "[725]\tvalid_0's multi_logloss: 0.190351\n",
      "[726]\tvalid_0's multi_logloss: 0.190346\n",
      "[727]\tvalid_0's multi_logloss: 0.190346\n",
      "[728]\tvalid_0's multi_logloss: 0.190342\n",
      "[729]\tvalid_0's multi_logloss: 0.190338\n",
      "[730]\tvalid_0's multi_logloss: 0.190333\n",
      "[731]\tvalid_0's multi_logloss: 0.190329\n",
      "[732]\tvalid_0's multi_logloss: 0.190324\n",
      "[733]\tvalid_0's multi_logloss: 0.190321\n",
      "[734]\tvalid_0's multi_logloss: 0.190317\n",
      "[735]\tvalid_0's multi_logloss: 0.190312\n",
      "[736]\tvalid_0's multi_logloss: 0.190307\n",
      "[737]\tvalid_0's multi_logloss: 0.1903\n",
      "[738]\tvalid_0's multi_logloss: 0.190296\n",
      "[739]\tvalid_0's multi_logloss: 0.190287\n",
      "[740]\tvalid_0's multi_logloss: 0.190277\n",
      "[741]\tvalid_0's multi_logloss: 0.19027\n",
      "[742]\tvalid_0's multi_logloss: 0.190269\n",
      "[743]\tvalid_0's multi_logloss: 0.190265\n",
      "[744]\tvalid_0's multi_logloss: 0.190261\n",
      "[745]\tvalid_0's multi_logloss: 0.190253\n",
      "[746]\tvalid_0's multi_logloss: 0.19025\n",
      "[747]\tvalid_0's multi_logloss: 0.190246\n",
      "[748]\tvalid_0's multi_logloss: 0.190241\n",
      "[749]\tvalid_0's multi_logloss: 0.190236\n",
      "[750]\tvalid_0's multi_logloss: 0.190225\n",
      "[751]\tvalid_0's multi_logloss: 0.190223\n",
      "[752]\tvalid_0's multi_logloss: 0.190217\n",
      "[753]\tvalid_0's multi_logloss: 0.190213\n",
      "[754]\tvalid_0's multi_logloss: 0.19021\n",
      "[755]\tvalid_0's multi_logloss: 0.190203\n",
      "[756]\tvalid_0's multi_logloss: 0.190199\n",
      "[757]\tvalid_0's multi_logloss: 0.190195\n",
      "[758]\tvalid_0's multi_logloss: 0.190191\n",
      "[759]\tvalid_0's multi_logloss: 0.190184\n",
      "[760]\tvalid_0's multi_logloss: 0.190178\n",
      "[761]\tvalid_0's multi_logloss: 0.190172\n",
      "[762]\tvalid_0's multi_logloss: 0.190168\n",
      "[763]\tvalid_0's multi_logloss: 0.190161\n",
      "[764]\tvalid_0's multi_logloss: 0.190156\n",
      "[765]\tvalid_0's multi_logloss: 0.190152\n",
      "[766]\tvalid_0's multi_logloss: 0.190148\n",
      "[767]\tvalid_0's multi_logloss: 0.190144\n",
      "[768]\tvalid_0's multi_logloss: 0.190139\n",
      "[769]\tvalid_0's multi_logloss: 0.190134\n",
      "[770]\tvalid_0's multi_logloss: 0.190128\n",
      "[771]\tvalid_0's multi_logloss: 0.190124\n",
      "[772]\tvalid_0's multi_logloss: 0.190119\n",
      "[773]\tvalid_0's multi_logloss: 0.190113\n",
      "[774]\tvalid_0's multi_logloss: 0.190105\n",
      "[775]\tvalid_0's multi_logloss: 0.190101\n",
      "[776]\tvalid_0's multi_logloss: 0.190097\n",
      "[777]\tvalid_0's multi_logloss: 0.190091\n",
      "[778]\tvalid_0's multi_logloss: 0.190088\n",
      "[779]\tvalid_0's multi_logloss: 0.190084\n",
      "[780]\tvalid_0's multi_logloss: 0.19008\n",
      "[781]\tvalid_0's multi_logloss: 0.190078\n",
      "[782]\tvalid_0's multi_logloss: 0.190071\n",
      "[783]\tvalid_0's multi_logloss: 0.190065\n",
      "[784]\tvalid_0's multi_logloss: 0.19006\n",
      "[785]\tvalid_0's multi_logloss: 0.190055\n",
      "[786]\tvalid_0's multi_logloss: 0.190052\n",
      "[787]\tvalid_0's multi_logloss: 0.19005\n",
      "[788]\tvalid_0's multi_logloss: 0.190043\n",
      "[789]\tvalid_0's multi_logloss: 0.19004\n",
      "[790]\tvalid_0's multi_logloss: 0.190036\n",
      "[791]\tvalid_0's multi_logloss: 0.190031\n",
      "[792]\tvalid_0's multi_logloss: 0.190024\n",
      "[793]\tvalid_0's multi_logloss: 0.190021\n",
      "[794]\tvalid_0's multi_logloss: 0.190015\n",
      "[795]\tvalid_0's multi_logloss: 0.190013\n",
      "[796]\tvalid_0's multi_logloss: 0.190005\n",
      "[797]\tvalid_0's multi_logloss: 0.190002\n",
      "[798]\tvalid_0's multi_logloss: 0.189995\n",
      "[799]\tvalid_0's multi_logloss: 0.189989\n",
      "[800]\tvalid_0's multi_logloss: 0.189986\n",
      "[801]\tvalid_0's multi_logloss: 0.189982\n",
      "[802]\tvalid_0's multi_logloss: 0.189976\n",
      "[803]\tvalid_0's multi_logloss: 0.189975\n",
      "[804]\tvalid_0's multi_logloss: 0.189968\n",
      "[805]\tvalid_0's multi_logloss: 0.189962\n",
      "[806]\tvalid_0's multi_logloss: 0.189959\n",
      "[807]\tvalid_0's multi_logloss: 0.189949\n",
      "[808]\tvalid_0's multi_logloss: 0.189941\n",
      "[809]\tvalid_0's multi_logloss: 0.189937\n",
      "[810]\tvalid_0's multi_logloss: 0.189933\n",
      "[811]\tvalid_0's multi_logloss: 0.189931\n",
      "[812]\tvalid_0's multi_logloss: 0.189926\n",
      "[813]\tvalid_0's multi_logloss: 0.189925\n",
      "[814]\tvalid_0's multi_logloss: 0.189916\n",
      "[815]\tvalid_0's multi_logloss: 0.189908\n",
      "[816]\tvalid_0's multi_logloss: 0.189908\n",
      "[817]\tvalid_0's multi_logloss: 0.189899\n",
      "[818]\tvalid_0's multi_logloss: 0.189894\n",
      "[819]\tvalid_0's multi_logloss: 0.189891\n",
      "[820]\tvalid_0's multi_logloss: 0.189892\n",
      "[821]\tvalid_0's multi_logloss: 0.189889\n",
      "[822]\tvalid_0's multi_logloss: 0.189877\n",
      "[823]\tvalid_0's multi_logloss: 0.189871\n",
      "[824]\tvalid_0's multi_logloss: 0.189869\n",
      "[825]\tvalid_0's multi_logloss: 0.189864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[826]\tvalid_0's multi_logloss: 0.189858\n",
      "[827]\tvalid_0's multi_logloss: 0.189853\n",
      "[828]\tvalid_0's multi_logloss: 0.189841\n",
      "[829]\tvalid_0's multi_logloss: 0.189836\n",
      "[830]\tvalid_0's multi_logloss: 0.189835\n",
      "[831]\tvalid_0's multi_logloss: 0.189833\n",
      "[832]\tvalid_0's multi_logloss: 0.189828\n",
      "[833]\tvalid_0's multi_logloss: 0.189825\n",
      "[834]\tvalid_0's multi_logloss: 0.189819\n",
      "[835]\tvalid_0's multi_logloss: 0.189815\n",
      "[836]\tvalid_0's multi_logloss: 0.189811\n",
      "[837]\tvalid_0's multi_logloss: 0.189805\n",
      "[838]\tvalid_0's multi_logloss: 0.189797\n",
      "[839]\tvalid_0's multi_logloss: 0.189794\n",
      "[840]\tvalid_0's multi_logloss: 0.189791\n",
      "[841]\tvalid_0's multi_logloss: 0.189786\n",
      "[842]\tvalid_0's multi_logloss: 0.189782\n",
      "[843]\tvalid_0's multi_logloss: 0.189779\n",
      "[844]\tvalid_0's multi_logloss: 0.189775\n",
      "[845]\tvalid_0's multi_logloss: 0.189771\n",
      "[846]\tvalid_0's multi_logloss: 0.189767\n",
      "[847]\tvalid_0's multi_logloss: 0.18976\n",
      "[848]\tvalid_0's multi_logloss: 0.189756\n",
      "[849]\tvalid_0's multi_logloss: 0.189752\n",
      "[850]\tvalid_0's multi_logloss: 0.189747\n",
      "[851]\tvalid_0's multi_logloss: 0.189744\n",
      "[852]\tvalid_0's multi_logloss: 0.189742\n",
      "[853]\tvalid_0's multi_logloss: 0.189738\n",
      "[854]\tvalid_0's multi_logloss: 0.189741\n",
      "[855]\tvalid_0's multi_logloss: 0.189736\n",
      "[856]\tvalid_0's multi_logloss: 0.189734\n",
      "[857]\tvalid_0's multi_logloss: 0.189729\n",
      "[858]\tvalid_0's multi_logloss: 0.189725\n",
      "[859]\tvalid_0's multi_logloss: 0.189723\n",
      "[860]\tvalid_0's multi_logloss: 0.18972\n",
      "[861]\tvalid_0's multi_logloss: 0.189706\n",
      "[862]\tvalid_0's multi_logloss: 0.189703\n",
      "[863]\tvalid_0's multi_logloss: 0.189697\n",
      "[864]\tvalid_0's multi_logloss: 0.18969\n",
      "[865]\tvalid_0's multi_logloss: 0.189686\n",
      "[866]\tvalid_0's multi_logloss: 0.189677\n",
      "[867]\tvalid_0's multi_logloss: 0.189673\n",
      "[868]\tvalid_0's multi_logloss: 0.18967\n",
      "[869]\tvalid_0's multi_logloss: 0.189667\n",
      "[870]\tvalid_0's multi_logloss: 0.189663\n",
      "[871]\tvalid_0's multi_logloss: 0.189656\n",
      "[872]\tvalid_0's multi_logloss: 0.189653\n",
      "[873]\tvalid_0's multi_logloss: 0.189651\n",
      "[874]\tvalid_0's multi_logloss: 0.189647\n",
      "[875]\tvalid_0's multi_logloss: 0.189639\n",
      "[876]\tvalid_0's multi_logloss: 0.189634\n",
      "[877]\tvalid_0's multi_logloss: 0.189632\n",
      "[878]\tvalid_0's multi_logloss: 0.189627\n",
      "[879]\tvalid_0's multi_logloss: 0.18962\n",
      "[880]\tvalid_0's multi_logloss: 0.189615\n",
      "[881]\tvalid_0's multi_logloss: 0.189613\n",
      "[882]\tvalid_0's multi_logloss: 0.189608\n",
      "[883]\tvalid_0's multi_logloss: 0.189602\n",
      "[884]\tvalid_0's multi_logloss: 0.189602\n",
      "[885]\tvalid_0's multi_logloss: 0.189599\n",
      "[886]\tvalid_0's multi_logloss: 0.189596\n",
      "[887]\tvalid_0's multi_logloss: 0.189595\n",
      "[888]\tvalid_0's multi_logloss: 0.189591\n",
      "[889]\tvalid_0's multi_logloss: 0.189588\n",
      "[890]\tvalid_0's multi_logloss: 0.189586\n",
      "[891]\tvalid_0's multi_logloss: 0.189584\n",
      "[892]\tvalid_0's multi_logloss: 0.189581\n",
      "[893]\tvalid_0's multi_logloss: 0.189577\n",
      "[894]\tvalid_0's multi_logloss: 0.189574\n",
      "[895]\tvalid_0's multi_logloss: 0.18957\n",
      "[896]\tvalid_0's multi_logloss: 0.189568\n",
      "[897]\tvalid_0's multi_logloss: 0.189562\n",
      "[898]\tvalid_0's multi_logloss: 0.189559\n",
      "[899]\tvalid_0's multi_logloss: 0.189554\n",
      "[900]\tvalid_0's multi_logloss: 0.18955\n",
      "[901]\tvalid_0's multi_logloss: 0.189547\n",
      "[902]\tvalid_0's multi_logloss: 0.189545\n",
      "[903]\tvalid_0's multi_logloss: 0.189539\n",
      "[904]\tvalid_0's multi_logloss: 0.189533\n",
      "[905]\tvalid_0's multi_logloss: 0.18953\n",
      "[906]\tvalid_0's multi_logloss: 0.189523\n",
      "[907]\tvalid_0's multi_logloss: 0.189521\n",
      "[908]\tvalid_0's multi_logloss: 0.189514\n",
      "[909]\tvalid_0's multi_logloss: 0.189507\n",
      "[910]\tvalid_0's multi_logloss: 0.189504\n",
      "[911]\tvalid_0's multi_logloss: 0.189502\n",
      "[912]\tvalid_0's multi_logloss: 0.189494\n",
      "[913]\tvalid_0's multi_logloss: 0.189486\n",
      "[914]\tvalid_0's multi_logloss: 0.189484\n",
      "[915]\tvalid_0's multi_logloss: 0.189481\n",
      "[916]\tvalid_0's multi_logloss: 0.189477\n",
      "[917]\tvalid_0's multi_logloss: 0.189471\n",
      "[918]\tvalid_0's multi_logloss: 0.189465\n",
      "[919]\tvalid_0's multi_logloss: 0.18946\n",
      "[920]\tvalid_0's multi_logloss: 0.189456\n",
      "[921]\tvalid_0's multi_logloss: 0.18945\n",
      "[922]\tvalid_0's multi_logloss: 0.189448\n",
      "[923]\tvalid_0's multi_logloss: 0.189445\n",
      "[924]\tvalid_0's multi_logloss: 0.18944\n",
      "[925]\tvalid_0's multi_logloss: 0.189435\n",
      "[926]\tvalid_0's multi_logloss: 0.18943\n",
      "[927]\tvalid_0's multi_logloss: 0.189425\n",
      "[928]\tvalid_0's multi_logloss: 0.189422\n",
      "[929]\tvalid_0's multi_logloss: 0.189418\n",
      "[930]\tvalid_0's multi_logloss: 0.189414\n",
      "[931]\tvalid_0's multi_logloss: 0.189408\n",
      "[932]\tvalid_0's multi_logloss: 0.189401\n",
      "[933]\tvalid_0's multi_logloss: 0.189396\n",
      "[934]\tvalid_0's multi_logloss: 0.189393\n",
      "[935]\tvalid_0's multi_logloss: 0.189389\n",
      "[936]\tvalid_0's multi_logloss: 0.189386\n",
      "[937]\tvalid_0's multi_logloss: 0.18938\n",
      "[938]\tvalid_0's multi_logloss: 0.189382\n",
      "[939]\tvalid_0's multi_logloss: 0.189376\n",
      "[940]\tvalid_0's multi_logloss: 0.189372\n",
      "[941]\tvalid_0's multi_logloss: 0.189372\n",
      "[942]\tvalid_0's multi_logloss: 0.189368\n",
      "[943]\tvalid_0's multi_logloss: 0.189364\n",
      "[944]\tvalid_0's multi_logloss: 0.189359\n",
      "[945]\tvalid_0's multi_logloss: 0.189356\n",
      "[946]\tvalid_0's multi_logloss: 0.189353\n",
      "[947]\tvalid_0's multi_logloss: 0.189352\n",
      "[948]\tvalid_0's multi_logloss: 0.18935\n",
      "[949]\tvalid_0's multi_logloss: 0.189345\n",
      "[950]\tvalid_0's multi_logloss: 0.189342\n",
      "[951]\tvalid_0's multi_logloss: 0.189337\n",
      "[952]\tvalid_0's multi_logloss: 0.189333\n",
      "[953]\tvalid_0's multi_logloss: 0.189329\n",
      "[954]\tvalid_0's multi_logloss: 0.189325\n",
      "[955]\tvalid_0's multi_logloss: 0.18932\n",
      "[956]\tvalid_0's multi_logloss: 0.189317\n",
      "[957]\tvalid_0's multi_logloss: 0.189312\n",
      "[958]\tvalid_0's multi_logloss: 0.189308\n",
      "[959]\tvalid_0's multi_logloss: 0.189302\n",
      "[960]\tvalid_0's multi_logloss: 0.1893\n",
      "[961]\tvalid_0's multi_logloss: 0.189293\n",
      "[962]\tvalid_0's multi_logloss: 0.189291\n",
      "[963]\tvalid_0's multi_logloss: 0.189289\n",
      "[964]\tvalid_0's multi_logloss: 0.189287\n",
      "[965]\tvalid_0's multi_logloss: 0.189284\n",
      "[966]\tvalid_0's multi_logloss: 0.189278\n",
      "[967]\tvalid_0's multi_logloss: 0.189273\n",
      "[968]\tvalid_0's multi_logloss: 0.189269\n",
      "[969]\tvalid_0's multi_logloss: 0.189267\n",
      "[970]\tvalid_0's multi_logloss: 0.189261\n",
      "[971]\tvalid_0's multi_logloss: 0.189254\n",
      "[972]\tvalid_0's multi_logloss: 0.189252\n",
      "[973]\tvalid_0's multi_logloss: 0.189246\n",
      "[974]\tvalid_0's multi_logloss: 0.189241\n",
      "[975]\tvalid_0's multi_logloss: 0.189237\n",
      "[976]\tvalid_0's multi_logloss: 0.189235\n",
      "[977]\tvalid_0's multi_logloss: 0.189233\n",
      "[978]\tvalid_0's multi_logloss: 0.189228\n",
      "[979]\tvalid_0's multi_logloss: 0.189223\n",
      "[980]\tvalid_0's multi_logloss: 0.189219\n",
      "[981]\tvalid_0's multi_logloss: 0.189216\n",
      "[982]\tvalid_0's multi_logloss: 0.189213\n",
      "[983]\tvalid_0's multi_logloss: 0.189209\n",
      "[984]\tvalid_0's multi_logloss: 0.189204\n",
      "[985]\tvalid_0's multi_logloss: 0.1892\n",
      "[986]\tvalid_0's multi_logloss: 0.189193\n",
      "[987]\tvalid_0's multi_logloss: 0.189187\n",
      "[988]\tvalid_0's multi_logloss: 0.189184\n",
      "[989]\tvalid_0's multi_logloss: 0.189178\n",
      "[990]\tvalid_0's multi_logloss: 0.189173\n",
      "[991]\tvalid_0's multi_logloss: 0.189167\n",
      "[992]\tvalid_0's multi_logloss: 0.189164\n",
      "[993]\tvalid_0's multi_logloss: 0.18916\n",
      "[994]\tvalid_0's multi_logloss: 0.189154\n",
      "[995]\tvalid_0's multi_logloss: 0.18915\n",
      "[996]\tvalid_0's multi_logloss: 0.189145\n",
      "[997]\tvalid_0's multi_logloss: 0.189142\n",
      "[998]\tvalid_0's multi_logloss: 0.189136\n",
      "[999]\tvalid_0's multi_logloss: 0.189135\n",
      "[1000]\tvalid_0's multi_logloss: 0.18913\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's multi_logloss: 0.18913\n"
     ]
    }
   ],
   "source": [
    "train = lgbm.Dataset(X_trn_vld, label=y_trn_vld, feature_name=features)\n",
    "validate = lgbm.Dataset(X_eval_vld, label=y_eval_vld, feature_name=features, reference=train)\n",
    "\n",
    "# 다양한 실험을 통해 얻은 최적의 학습 parameter\n",
    "params_lgb = {\n",
    "    'task' : 'train',\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'objective' : 'multiclass',\n",
    "    'num_class': 17,\n",
    "    'metric' : {'multi_logloss'},\n",
    "    'is_training_metric': True,\n",
    "    'max_bin': 255,\n",
    "    'num_leaves' : 64,\n",
    "    'learning_rate' : 0.1,\n",
    "    'feature_fraction' : 0.8,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'min_sum_hessian_in_leaf': 5,\n",
    "    # 'num_threads': 16,\n",
    "}\n",
    "\n",
    "# XGBoost와 동일하게 훈련/검증 데이터를 기반으로 최적의 트리 개수를 계산한다\n",
    "model_lgb = lgbm.train(params_lgb, train, num_boost_round=1000, valid_sets=validate, early_stopping_rounds=20)\n",
    "best_iteration = model_lgb.best_iteration\n",
    "# 학습된 모델과 최적의 트리 개수 정보를 저장한다\n",
    "model_lgb.save_model(\"../model/lgbm.model.txt\")\n",
    "pickle.dump(best_iteration, open(\"../model/lgbm.model.meta\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696539, 16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증 데이터에 대한 예측 값을 구한다.\n",
    "preds_vld_lgb = model_lgb.predict(tst_vld[features], ntree_limit=best_iteration)\n",
    "\n",
    "preds_vld_lgb_16 = np.delete(preds_vld_lgb, 16, axis=1)\n",
    "preds_vld_lgb_16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_vld_lgb_16 = preds_vld_lgb_16 - tst_vld[[prod+'_prev' for prod in prods[target]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036521553551041475\n"
     ]
    }
   ],
   "source": [
    "result_lgb = predict_7_products(preds_vld_lgb_16.values)\n",
    "\n",
    "# 검증 데이터에서의 MAP@7 점수를 구한다. (0.03661802042010113)\n",
    "print(mapk(add_vld_list, result_lgb, 7, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data should be between -1 and 1 to be trained, so we scaled numeric features before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int8, int16, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(trn[features])\n",
    "X_trn_vld_norm = scaler.transform(X_trn_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval_vld_norm = scaler.transform(X_eval_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10765757, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trn_vld_matrix = trn_vld[[prod for prod in prods[target]]].values\n",
    "y_trn_vld_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(689132, 16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_eval_vld_matrix = eval_vld[[prod for prod in prods[target]]].values\n",
    "y_eval_vld_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               31232     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 302,096\n",
      "Trainable params: 302,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_nn = models.Sequential()\n",
    "model_nn.add(layers.Dense(512, activation='relu', input_shape=(60,)))\n",
    "model_nn.add(layers.Dropout(0.8))\n",
    "model_nn.add(layers.Dense(512, activation='relu'))\n",
    "model_nn.add(layers.Dropout(0.5))\n",
    "model_nn.add(layers.Dense(16, activation='softmax'))\n",
    "\n",
    "model_nn.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 10765757 samples, validate on 689132 samples\n",
      "Epoch 1/100\n",
      "10765757/10765757 [==============================] - 833s 77us/step - loss: 4.4280 - acc: 0.7962 - val_loss: 4.5847 - val_acc: 0.8338\n",
      "Epoch 2/100\n",
      "10765757/10765757 [==============================] - 839s 78us/step - loss: 6.0680 - acc: 0.8093 - val_loss: 5.8353 - val_acc: 0.8641\n",
      "Epoch 3/100\n",
      "10765757/10765757 [==============================] - 814s 76us/step - loss: 6.6630 - acc: 0.8154 - val_loss: 6.1597 - val_acc: 0.8705\n",
      "Epoch 4/100\n",
      "10765757/10765757 [==============================] - 830s 77us/step - loss: 6.9333 - acc: 0.8162 - val_loss: 6.5466 - val_acc: 0.8626\n",
      "Epoch 5/100\n",
      "10765757/10765757 [==============================] - 839s 78us/step - loss: 7.2520 - acc: 0.8153 - val_loss: 7.0494 - val_acc: 0.8623\n",
      "Epoch 6/100\n",
      "10765757/10765757 [==============================] - 830s 77us/step - loss: 7.5079 - acc: 0.8162 - val_loss: 7.4550 - val_acc: 0.8729\n",
      "Epoch 7/100\n",
      "10765757/10765757 [==============================] - 832s 77us/step - loss: 7.8727 - acc: 0.8193 - val_loss: 8.0043 - val_acc: 0.8766\n",
      "Epoch 8/100\n",
      "10765757/10765757 [==============================] - 831s 77us/step - loss: 8.1750 - acc: 0.8202 - val_loss: 8.1364 - val_acc: 0.8743\n",
      "Epoch 9/100\n",
      "10765757/10765757 [==============================] - 837s 78us/step - loss: 8.3627 - acc: 0.8181 - val_loss: 8.3968 - val_acc: 0.8779\n",
      "Epoch 10/100\n",
      "10765757/10765757 [==============================] - 827s 77us/step - loss: 8.4917 - acc: 0.8158 - val_loss: 8.5431 - val_acc: 0.8771\n",
      "Epoch 11/100\n",
      "10765757/10765757 [==============================] - 832s 77us/step - loss: 8.5457 - acc: 0.8170 - val_loss: 8.4976 - val_acc: 0.8727\n",
      "Epoch 12/100\n",
      "10765757/10765757 [==============================] - 830s 77us/step - loss: 8.5724 - acc: 0.8163 - val_loss: 8.5077 - val_acc: 0.8725\n",
      "Epoch 13/100\n",
      "10765757/10765757 [==============================] - 836s 78us/step - loss: 8.6056 - acc: 0.8145 - val_loss: 8.5786 - val_acc: 0.8725\n",
      "Epoch 14/100\n",
      "10765757/10765757 [==============================] - 830s 77us/step - loss: 8.6397 - acc: 0.8149 - val_loss: 8.6792 - val_acc: 0.8731\n",
      "Epoch 15/100\n",
      "10765757/10765757 [==============================] - 834s 77us/step - loss: 8.6493 - acc: 0.8180 - val_loss: 8.6499 - val_acc: 0.8736\n",
      "Epoch 16/100\n",
      "10765757/10765757 [==============================] - 825s 77us/step - loss: 8.6841 - acc: 0.8195 - val_loss: 8.7431 - val_acc: 0.8747\n",
      "Epoch 17/100\n",
      "10765757/10765757 [==============================] - 810s 75us/step - loss: 8.7430 - acc: 0.8178 - val_loss: 8.7675 - val_acc: 0.8755\n",
      "Epoch 18/100\n",
      "10765757/10765757 [==============================] - 816s 76us/step - loss: 8.7538 - acc: 0.8191 - val_loss: 8.8385 - val_acc: 0.8771\n",
      "Epoch 19/100\n",
      "10765757/10765757 [==============================] - 818s 76us/step - loss: 8.8443 - acc: 0.8196 - val_loss: 9.2628 - val_acc: 0.8763\n"
     ]
    }
   ],
   "source": [
    "callback_list = [EarlyStopping(monitor='val_acc', patience = 10)]\n",
    "model_nn.fit(X_trn_vld_norm, y_trn_vld_matrix, epochs=100, batch_size=64, callbacks=callback_list, validation_data=(X_eval_vld_norm ,y_eval_vld_matrix))\n",
    "\n",
    "# 학습한 모델을 저장한다.\n",
    "pickle.dump(model_nn, open(\"../model/neuralnetwork.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int8, int16, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_tst_vld_norm = scaler.transform(tst_vld[features])\n",
    "vld_preds_nn = model_nn.predict(X_tst_vld_norm, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products_from_nn(preds_prod):\n",
    "    result_vld = []\n",
    "\n",
    "    for ncodper, prds in zip(ncodpers_tst_vld, preds_prod):\n",
    "        r = [(ip,p) for ip, p in zip(target,prds) if p > 0]\n",
    "        r = sorted(r, key=lambda a:a[1], reverse=True)[:7]\n",
    "        result_vld.append([ip for ip,p in r])\n",
    "\n",
    "    return result_vld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00884893083733709"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_nn = get_products_from_nn(vld_preds_nn)\n",
    "\n",
    "# 검증 데이터에서의 MAP@7 점수를 구한다. (0.036466)\n",
    "mapk(add_vld_list, result_nn, 7, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Ensemble Model\n",
    "\n",
    "1) lightGBM + XGBoost + NN\n",
    "1) lightGBM + XGBoost\n",
    "2) NN + lightGBM\n",
    "3) NN + XGBoost \n",
    "* multiply neural network, lightGBM, xgb calculate sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018705250713864016\n"
     ]
    }
   ],
   "source": [
    "# 곱셈 후, 제곱근을 구하는 방식으로 앙상블을 수행한다\n",
    "preds_vld_ensemble = np.sqrt(np.multiply(np.multiply(preds_vld_xgb, preds_vld_lgb),vld_preds_nn))\n",
    "result_ensemble = predict_7_products(preds_vld_ensemble.values)\n",
    "\n",
    "# 검증 데이터에서의 MAP@7 점수를 구한다. (0.036466)\n",
    "print(mapk(add_vld_list, result_ensemble, 7, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each result, visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.67857\n",
      "[1]\ttrain-mlogloss:2.43528\n",
      "[2]\ttrain-mlogloss:2.26705\n",
      "[3]\ttrain-mlogloss:2.13088\n",
      "[4]\ttrain-mlogloss:2.01841\n",
      "[5]\ttrain-mlogloss:1.93183\n",
      "[6]\ttrain-mlogloss:1.8522\n",
      "[7]\ttrain-mlogloss:1.78537\n",
      "[8]\ttrain-mlogloss:1.72515\n",
      "[9]\ttrain-mlogloss:1.67057\n",
      "[10]\ttrain-mlogloss:1.62535\n",
      "[11]\ttrain-mlogloss:1.5841\n",
      "[12]\ttrain-mlogloss:1.54635\n",
      "[13]\ttrain-mlogloss:1.51283\n",
      "[14]\ttrain-mlogloss:1.48278\n",
      "[15]\ttrain-mlogloss:1.45422\n",
      "[16]\ttrain-mlogloss:1.428\n",
      "[17]\ttrain-mlogloss:1.40507\n",
      "[18]\ttrain-mlogloss:1.38329\n",
      "[19]\ttrain-mlogloss:1.36347\n",
      "[20]\ttrain-mlogloss:1.34513\n",
      "[21]\ttrain-mlogloss:1.32845\n",
      "[22]\ttrain-mlogloss:1.31424\n",
      "[23]\ttrain-mlogloss:1.30034\n",
      "[24]\ttrain-mlogloss:1.28795\n",
      "[25]\ttrain-mlogloss:1.2754\n",
      "[26]\ttrain-mlogloss:1.26436\n",
      "[27]\ttrain-mlogloss:1.25375\n",
      "[28]\ttrain-mlogloss:1.24416\n",
      "[29]\ttrain-mlogloss:1.2353\n",
      "[30]\ttrain-mlogloss:1.2268\n",
      "[31]\ttrain-mlogloss:1.21864\n",
      "[32]\ttrain-mlogloss:1.21123\n",
      "[33]\ttrain-mlogloss:1.20451\n",
      "[34]\ttrain-mlogloss:1.19766\n",
      "[35]\ttrain-mlogloss:1.19173\n",
      "[36]\ttrain-mlogloss:1.1862\n",
      "[37]\ttrain-mlogloss:1.18089\n",
      "[38]\ttrain-mlogloss:1.17586\n",
      "[39]\ttrain-mlogloss:1.17115\n",
      "[40]\ttrain-mlogloss:1.1667\n",
      "[41]\ttrain-mlogloss:1.16221\n",
      "[42]\ttrain-mlogloss:1.15824\n",
      "[43]\ttrain-mlogloss:1.15426\n",
      "[44]\ttrain-mlogloss:1.15087\n",
      "[45]\ttrain-mlogloss:1.14752\n",
      "[46]\ttrain-mlogloss:1.14404\n",
      "[47]\ttrain-mlogloss:1.14085\n",
      "[48]\ttrain-mlogloss:1.13786\n",
      "[49]\ttrain-mlogloss:1.13489\n",
      "[50]\ttrain-mlogloss:1.13216\n",
      "[51]\ttrain-mlogloss:1.1297\n",
      "[52]\ttrain-mlogloss:1.12724\n",
      "[53]\ttrain-mlogloss:1.12498\n",
      "[54]\ttrain-mlogloss:1.1227\n",
      "[55]\ttrain-mlogloss:1.12061\n",
      "[56]\ttrain-mlogloss:1.11867\n",
      "[57]\ttrain-mlogloss:1.1167\n",
      "[58]\ttrain-mlogloss:1.11485\n",
      "[59]\ttrain-mlogloss:1.11305\n",
      "[60]\ttrain-mlogloss:1.11138\n",
      "[61]\ttrain-mlogloss:1.10977\n",
      "[62]\ttrain-mlogloss:1.10819\n",
      "[63]\ttrain-mlogloss:1.10676\n",
      "[64]\ttrain-mlogloss:1.1053\n",
      "[65]\ttrain-mlogloss:1.10381\n",
      "[66]\ttrain-mlogloss:1.10254\n",
      "[67]\ttrain-mlogloss:1.10121\n",
      "[68]\ttrain-mlogloss:1.09991\n",
      "[69]\ttrain-mlogloss:1.09875\n",
      "[70]\ttrain-mlogloss:1.09766\n",
      "[71]\ttrain-mlogloss:1.09656\n",
      "[72]\ttrain-mlogloss:1.09549\n",
      "[73]\ttrain-mlogloss:1.09449\n",
      "[74]\ttrain-mlogloss:1.09342\n",
      "[75]\ttrain-mlogloss:1.09244\n",
      "[76]\ttrain-mlogloss:1.09153\n",
      "[77]\ttrain-mlogloss:1.09065\n",
      "[78]\ttrain-mlogloss:1.08984\n",
      "[79]\ttrain-mlogloss:1.08894\n",
      "[80]\ttrain-mlogloss:1.08814\n",
      "[81]\ttrain-mlogloss:1.08733\n",
      "[82]\ttrain-mlogloss:1.08656\n",
      "[83]\ttrain-mlogloss:1.0859\n",
      "[84]\ttrain-mlogloss:1.08516\n",
      "[85]\ttrain-mlogloss:1.08438\n",
      "[86]\ttrain-mlogloss:1.08377\n",
      "[87]\ttrain-mlogloss:1.0831\n",
      "[88]\ttrain-mlogloss:1.08242\n",
      "[89]\ttrain-mlogloss:1.08186\n",
      "[90]\ttrain-mlogloss:1.08124\n",
      "[91]\ttrain-mlogloss:1.08062\n",
      "[92]\ttrain-mlogloss:1.08007\n",
      "[93]\ttrain-mlogloss:1.07952\n",
      "[94]\ttrain-mlogloss:1.07889\n",
      "[95]\ttrain-mlogloss:1.07838\n",
      "[96]\ttrain-mlogloss:1.0778\n",
      "[97]\ttrain-mlogloss:1.07723\n",
      "[98]\ttrain-mlogloss:1.07679\n",
      "[99]\ttrain-mlogloss:1.07627\n",
      "[100]\ttrain-mlogloss:1.07581\n",
      "[101]\ttrain-mlogloss:1.07528\n",
      "[102]\ttrain-mlogloss:1.07482\n",
      "[103]\ttrain-mlogloss:1.07436\n",
      "[104]\ttrain-mlogloss:1.07387\n",
      "[105]\ttrain-mlogloss:1.07348\n",
      "[106]\ttrain-mlogloss:1.07306\n",
      "[107]\ttrain-mlogloss:1.07264\n",
      "[108]\ttrain-mlogloss:1.07224\n",
      "[109]\ttrain-mlogloss:1.07174\n",
      "[110]\ttrain-mlogloss:1.07139\n",
      "[111]\ttrain-mlogloss:1.07101\n",
      "[112]\ttrain-mlogloss:1.07063\n",
      "[113]\ttrain-mlogloss:1.07021\n",
      "[114]\ttrain-mlogloss:1.06984\n",
      "[115]\ttrain-mlogloss:1.0695\n",
      "[116]\ttrain-mlogloss:1.06909\n",
      "[117]\ttrain-mlogloss:1.06874\n",
      "[118]\ttrain-mlogloss:1.06838\n",
      "[119]\ttrain-mlogloss:1.06806\n",
      "[120]\ttrain-mlogloss:1.06764\n",
      "[121]\ttrain-mlogloss:1.06731\n",
      "[122]\ttrain-mlogloss:1.06691\n",
      "[123]\ttrain-mlogloss:1.06657\n",
      "[124]\ttrain-mlogloss:1.0662\n",
      "[125]\ttrain-mlogloss:1.06591\n",
      "[126]\ttrain-mlogloss:1.06553\n",
      "[127]\ttrain-mlogloss:1.06518\n",
      "[128]\ttrain-mlogloss:1.06483\n",
      "[129]\ttrain-mlogloss:1.06445\n",
      "[130]\ttrain-mlogloss:1.06408\n",
      "[131]\ttrain-mlogloss:1.06379\n",
      "[132]\ttrain-mlogloss:1.06354\n",
      "[133]\ttrain-mlogloss:1.06326\n",
      "[134]\ttrain-mlogloss:1.06287\n",
      "[135]\ttrain-mlogloss:1.06251\n",
      "[136]\ttrain-mlogloss:1.06222\n",
      "[137]\ttrain-mlogloss:1.06184\n",
      "[138]\ttrain-mlogloss:1.06154\n",
      "[139]\ttrain-mlogloss:1.06121\n",
      "[140]\ttrain-mlogloss:1.06089\n",
      "[141]\ttrain-mlogloss:1.06059\n",
      "[142]\ttrain-mlogloss:1.06025\n",
      "[143]\ttrain-mlogloss:1.05986\n",
      "[144]\ttrain-mlogloss:1.05947\n",
      "[145]\ttrain-mlogloss:1.05916\n",
      "[146]\ttrain-mlogloss:1.05889\n",
      "[147]\ttrain-mlogloss:1.05845\n",
      "[148]\ttrain-mlogloss:1.05807\n",
      "[149]\ttrain-mlogloss:1.05776\n",
      "[150]\ttrain-mlogloss:1.05744\n",
      "[151]\ttrain-mlogloss:1.0571\n",
      "[152]\ttrain-mlogloss:1.05679\n",
      "[153]\ttrain-mlogloss:1.05645\n",
      "[154]\ttrain-mlogloss:1.05609\n",
      "[155]\ttrain-mlogloss:1.05567\n",
      "[156]\ttrain-mlogloss:1.05534\n",
      "[157]\ttrain-mlogloss:1.05508\n",
      "[158]\ttrain-mlogloss:1.05475\n",
      "[159]\ttrain-mlogloss:1.05442\n",
      "[160]\ttrain-mlogloss:1.05405\n",
      "[161]\ttrain-mlogloss:1.05378\n",
      "[162]\ttrain-mlogloss:1.05346\n",
      "[163]\ttrain-mlogloss:1.05318\n",
      "[164]\ttrain-mlogloss:1.05285\n",
      "[165]\ttrain-mlogloss:1.05247\n",
      "[166]\ttrain-mlogloss:1.05209\n",
      "[167]\ttrain-mlogloss:1.05168\n",
      "[168]\ttrain-mlogloss:1.05132\n",
      "[169]\ttrain-mlogloss:1.05103\n",
      "[170]\ttrain-mlogloss:1.05073\n",
      "[171]\ttrain-mlogloss:1.05037\n",
      "[172]\ttrain-mlogloss:1.05003\n",
      "[173]\ttrain-mlogloss:1.04968\n",
      "[174]\ttrain-mlogloss:1.04936\n",
      "[175]\ttrain-mlogloss:1.04912\n",
      "[176]\ttrain-mlogloss:1.04873\n",
      "[177]\ttrain-mlogloss:1.04844\n",
      "[178]\ttrain-mlogloss:1.04806\n",
      "[179]\ttrain-mlogloss:1.04771\n",
      "[180]\ttrain-mlogloss:1.0474\n",
      "[181]\ttrain-mlogloss:1.0471\n",
      "[182]\ttrain-mlogloss:1.04673\n",
      "[183]\ttrain-mlogloss:1.04647\n",
      "[184]\ttrain-mlogloss:1.04614\n",
      "[185]\ttrain-mlogloss:1.04574\n",
      "[186]\ttrain-mlogloss:1.04537\n",
      "[187]\ttrain-mlogloss:1.04509\n",
      "[188]\ttrain-mlogloss:1.04481\n",
      "[189]\ttrain-mlogloss:1.04448\n",
      "[190]\ttrain-mlogloss:1.04414\n",
      "[191]\ttrain-mlogloss:1.04382\n",
      "[192]\ttrain-mlogloss:1.0435\n",
      "[193]\ttrain-mlogloss:1.04314\n",
      "[194]\ttrain-mlogloss:1.04276\n",
      "[195]\ttrain-mlogloss:1.04235\n",
      "[196]\ttrain-mlogloss:1.04201\n",
      "[197]\ttrain-mlogloss:1.04177\n",
      "[198]\ttrain-mlogloss:1.04149\n",
      "[199]\ttrain-mlogloss:1.04116\n",
      "[200]\ttrain-mlogloss:1.04079\n",
      "[201]\ttrain-mlogloss:1.04046\n",
      "[202]\ttrain-mlogloss:1.04017\n",
      "[203]\ttrain-mlogloss:1.03986\n",
      "[204]\ttrain-mlogloss:1.03942\n",
      "[205]\ttrain-mlogloss:1.03909\n",
      "[206]\ttrain-mlogloss:1.03872\n",
      "[207]\ttrain-mlogloss:1.03834\n",
      "[208]\ttrain-mlogloss:1.03811\n",
      "[209]\ttrain-mlogloss:1.03781\n",
      "[210]\ttrain-mlogloss:1.03743\n",
      "[211]\ttrain-mlogloss:1.03715\n",
      "[212]\ttrain-mlogloss:1.03681\n",
      "[213]\ttrain-mlogloss:1.03647\n",
      "[214]\ttrain-mlogloss:1.03613\n",
      "[215]\ttrain-mlogloss:1.03586\n",
      "[216]\ttrain-mlogloss:1.03554\n",
      "[217]\ttrain-mlogloss:1.0352\n",
      "[218]\ttrain-mlogloss:1.03487\n",
      "[219]\ttrain-mlogloss:1.03457\n",
      "[220]\ttrain-mlogloss:1.03436\n",
      "[221]\ttrain-mlogloss:1.03409\n",
      "[222]\ttrain-mlogloss:1.03374\n",
      "[223]\ttrain-mlogloss:1.03341\n",
      "[224]\ttrain-mlogloss:1.03302\n",
      "[225]\ttrain-mlogloss:1.03271\n",
      "[226]\ttrain-mlogloss:1.0324\n",
      "[227]\ttrain-mlogloss:1.03211\n",
      "[228]\ttrain-mlogloss:1.03185\n",
      "[229]\ttrain-mlogloss:1.03158\n",
      "[230]\ttrain-mlogloss:1.03129\n",
      "[231]\ttrain-mlogloss:1.0309\n",
      "[232]\ttrain-mlogloss:1.03059\n",
      "[233]\ttrain-mlogloss:1.03031\n",
      "[234]\ttrain-mlogloss:1.03009\n",
      "[235]\ttrain-mlogloss:1.02981\n",
      "[236]\ttrain-mlogloss:1.02948\n",
      "[237]\ttrain-mlogloss:1.02917\n",
      "[238]\ttrain-mlogloss:1.02881\n",
      "[239]\ttrain-mlogloss:1.02852\n",
      "[240]\ttrain-mlogloss:1.02831\n",
      "[241]\ttrain-mlogloss:1.02796\n",
      "[242]\ttrain-mlogloss:1.02763\n",
      "[243]\ttrain-mlogloss:1.02733\n",
      "[244]\ttrain-mlogloss:1.027\n",
      "[245]\ttrain-mlogloss:1.02676\n",
      "[246]\ttrain-mlogloss:1.02642\n",
      "[247]\ttrain-mlogloss:1.02622\n",
      "[248]\ttrain-mlogloss:1.02595\n",
      "[249]\ttrain-mlogloss:1.02568\n",
      "[250]\ttrain-mlogloss:1.02541\n",
      "[251]\ttrain-mlogloss:1.02499\n",
      "[252]\ttrain-mlogloss:1.02469\n",
      "[253]\ttrain-mlogloss:1.0244\n",
      "[254]\ttrain-mlogloss:1.02412\n",
      "[255]\ttrain-mlogloss:1.0238\n",
      "[256]\ttrain-mlogloss:1.02345\n",
      "[257]\ttrain-mlogloss:1.02307\n",
      "[258]\ttrain-mlogloss:1.02281\n",
      "[259]\ttrain-mlogloss:1.02251\n",
      "[260]\ttrain-mlogloss:1.02216\n",
      "[261]\ttrain-mlogloss:1.02184\n",
      "[262]\ttrain-mlogloss:1.02159\n",
      "[263]\ttrain-mlogloss:1.02132\n",
      "[264]\ttrain-mlogloss:1.02106\n",
      "[265]\ttrain-mlogloss:1.02078\n",
      "[266]\ttrain-mlogloss:1.02051\n",
      "[267]\ttrain-mlogloss:1.02021\n",
      "[268]\ttrain-mlogloss:1.01988\n",
      "[269]\ttrain-mlogloss:1.01957\n",
      "[270]\ttrain-mlogloss:1.01937\n",
      "[271]\ttrain-mlogloss:1.01902\n",
      "[272]\ttrain-mlogloss:1.01869\n",
      "[273]\ttrain-mlogloss:1.01845\n",
      "[274]\ttrain-mlogloss:1.01808\n",
      "[275]\ttrain-mlogloss:1.01783\n",
      "[276]\ttrain-mlogloss:1.01765\n",
      "[277]\ttrain-mlogloss:1.01735\n",
      "[278]\ttrain-mlogloss:1.01707\n",
      "[279]\ttrain-mlogloss:1.01681\n",
      "[280]\ttrain-mlogloss:1.01645\n",
      "[281]\ttrain-mlogloss:1.01622\n",
      "[282]\ttrain-mlogloss:1.01599\n",
      "[283]\ttrain-mlogloss:1.01573\n",
      "[284]\ttrain-mlogloss:1.01544\n",
      "[285]\ttrain-mlogloss:1.01511\n",
      "[286]\ttrain-mlogloss:1.01472\n",
      "[287]\ttrain-mlogloss:1.01444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[288]\ttrain-mlogloss:1.01417\n",
      "[289]\ttrain-mlogloss:1.01388\n",
      "[290]\ttrain-mlogloss:1.01358\n",
      "[291]\ttrain-mlogloss:1.01327\n",
      "[292]\ttrain-mlogloss:1.01301\n",
      "[293]\ttrain-mlogloss:1.01274\n",
      "[294]\ttrain-mlogloss:1.01243\n",
      "[295]\ttrain-mlogloss:1.01216\n",
      "[296]\ttrain-mlogloss:1.01192\n",
      "[297]\ttrain-mlogloss:1.01167\n",
      "[298]\ttrain-mlogloss:1.01145\n",
      "[299]\ttrain-mlogloss:1.01123\n",
      "[300]\ttrain-mlogloss:1.01094\n",
      "[301]\ttrain-mlogloss:1.01072\n",
      "[302]\ttrain-mlogloss:1.01048\n",
      "[303]\ttrain-mlogloss:1.01024\n",
      "[304]\ttrain-mlogloss:1.01003\n",
      "[305]\ttrain-mlogloss:1.00982\n",
      "[306]\ttrain-mlogloss:1.0096\n",
      "[307]\ttrain-mlogloss:1.00932\n",
      "[308]\ttrain-mlogloss:1.00906\n",
      "[309]\ttrain-mlogloss:1.00873\n",
      "[310]\ttrain-mlogloss:1.0085\n",
      "[311]\ttrain-mlogloss:1.00818\n",
      "[312]\ttrain-mlogloss:1.00798\n",
      "[313]\ttrain-mlogloss:1.00775\n",
      "[314]\ttrain-mlogloss:1.00747\n",
      "[315]\ttrain-mlogloss:1.0072\n",
      "[316]\ttrain-mlogloss:1.00698\n",
      "[317]\ttrain-mlogloss:1.00673\n",
      "[318]\ttrain-mlogloss:1.00642\n",
      "[319]\ttrain-mlogloss:1.00619\n",
      "[320]\ttrain-mlogloss:1.00591\n",
      "[321]\ttrain-mlogloss:1.00566\n",
      "[322]\ttrain-mlogloss:1.00544\n",
      "[323]\ttrain-mlogloss:1.00518\n",
      "[324]\ttrain-mlogloss:1.00494\n",
      "[325]\ttrain-mlogloss:1.0047\n",
      "[326]\ttrain-mlogloss:1.00443\n",
      "[327]\ttrain-mlogloss:1.00409\n",
      "[328]\ttrain-mlogloss:1.00388\n",
      "[329]\ttrain-mlogloss:1.00369\n",
      "[330]\ttrain-mlogloss:1.00342\n",
      "[331]\ttrain-mlogloss:1.00321\n",
      "[332]\ttrain-mlogloss:1.00299\n",
      "[333]\ttrain-mlogloss:1.00271\n",
      "[334]\ttrain-mlogloss:1.00247\n",
      "[335]\ttrain-mlogloss:1.00228\n",
      "[336]\ttrain-mlogloss:1.00205\n",
      "[337]\ttrain-mlogloss:1.00178\n",
      "[338]\ttrain-mlogloss:1.00157\n",
      "[339]\ttrain-mlogloss:1.00137\n",
      "[340]\ttrain-mlogloss:1.00111\n",
      "[341]\ttrain-mlogloss:1.00093\n",
      "[342]\ttrain-mlogloss:1.00072\n",
      "[343]\ttrain-mlogloss:1.00049\n",
      "[344]\ttrain-mlogloss:1.00027\n",
      "[345]\ttrain-mlogloss:1.00008\n",
      "[346]\ttrain-mlogloss:0.99982\n",
      "[347]\ttrain-mlogloss:0.999605\n",
      "[348]\ttrain-mlogloss:0.999371\n",
      "[349]\ttrain-mlogloss:0.999129\n",
      "[350]\ttrain-mlogloss:0.998889\n",
      "[351]\ttrain-mlogloss:0.998657\n",
      "[352]\ttrain-mlogloss:0.998444\n",
      "[353]\ttrain-mlogloss:0.998265\n",
      "[354]\ttrain-mlogloss:0.99808\n",
      "[355]\ttrain-mlogloss:0.997798\n",
      "[356]\ttrain-mlogloss:0.997553\n",
      "[357]\ttrain-mlogloss:0.997339\n",
      "[358]\ttrain-mlogloss:0.997163\n",
      "[359]\ttrain-mlogloss:0.996901\n",
      "[360]\ttrain-mlogloss:0.996661\n",
      "[361]\ttrain-mlogloss:0.996408\n",
      "[362]\ttrain-mlogloss:0.996144\n",
      "[363]\ttrain-mlogloss:0.995836\n",
      "[364]\ttrain-mlogloss:0.995669\n",
      "[365]\ttrain-mlogloss:0.995444\n",
      "[366]\ttrain-mlogloss:0.995217\n",
      "[367]\ttrain-mlogloss:0.995065\n",
      "[368]\ttrain-mlogloss:0.994834\n",
      "[369]\ttrain-mlogloss:0.994641\n",
      "[370]\ttrain-mlogloss:0.994393\n",
      "[371]\ttrain-mlogloss:0.994188\n",
      "[372]\ttrain-mlogloss:0.993977\n",
      "[373]\ttrain-mlogloss:0.993774\n",
      "[374]\ttrain-mlogloss:0.993568\n",
      "[375]\ttrain-mlogloss:0.993359\n",
      "[376]\ttrain-mlogloss:0.993132\n",
      "[377]\ttrain-mlogloss:0.992947\n",
      "[378]\ttrain-mlogloss:0.992716\n",
      "[379]\ttrain-mlogloss:0.992369\n",
      "[380]\ttrain-mlogloss:0.992132\n",
      "[381]\ttrain-mlogloss:0.99191\n",
      "[382]\ttrain-mlogloss:0.991707\n",
      "[383]\ttrain-mlogloss:0.991511\n",
      "[384]\ttrain-mlogloss:0.991241\n",
      "[385]\ttrain-mlogloss:0.991039\n",
      "[386]\ttrain-mlogloss:0.990765\n",
      "[387]\ttrain-mlogloss:0.99057\n",
      "[388]\ttrain-mlogloss:0.990352\n",
      "[389]\ttrain-mlogloss:0.990142\n",
      "[390]\ttrain-mlogloss:0.989908\n",
      "[391]\ttrain-mlogloss:0.989661\n",
      "[392]\ttrain-mlogloss:0.989477\n",
      "[393]\ttrain-mlogloss:0.989209\n",
      "[394]\ttrain-mlogloss:0.989002\n",
      "[395]\ttrain-mlogloss:0.98882\n",
      "[396]\ttrain-mlogloss:0.988585\n",
      "[397]\ttrain-mlogloss:0.988382\n",
      "[398]\ttrain-mlogloss:0.98814\n",
      "[399]\ttrain-mlogloss:0.98786\n",
      "[400]\ttrain-mlogloss:0.987655\n",
      "[401]\ttrain-mlogloss:0.98741\n",
      "[402]\ttrain-mlogloss:0.987194\n",
      "[403]\ttrain-mlogloss:0.986987\n",
      "[404]\ttrain-mlogloss:0.986758\n",
      "[405]\ttrain-mlogloss:0.98653\n",
      "[406]\ttrain-mlogloss:0.986245\n",
      "[407]\ttrain-mlogloss:0.986034\n",
      "[408]\ttrain-mlogloss:0.985777\n",
      "[409]\ttrain-mlogloss:0.985465\n",
      "[410]\ttrain-mlogloss:0.985298\n",
      "[411]\ttrain-mlogloss:0.985069\n",
      "[412]\ttrain-mlogloss:0.984832\n",
      "[413]\ttrain-mlogloss:0.984612\n",
      "[414]\ttrain-mlogloss:0.984347\n",
      "[415]\ttrain-mlogloss:0.984107\n",
      "[416]\ttrain-mlogloss:0.983901\n",
      "[417]\ttrain-mlogloss:0.983672\n",
      "[418]\ttrain-mlogloss:0.98346\n",
      "[419]\ttrain-mlogloss:0.983223\n",
      "[420]\ttrain-mlogloss:0.983039\n",
      "[421]\ttrain-mlogloss:0.98284\n",
      "[422]\ttrain-mlogloss:0.982634\n",
      "[423]\ttrain-mlogloss:0.982451\n",
      "[424]\ttrain-mlogloss:0.982237\n",
      "[425]\ttrain-mlogloss:0.982053\n",
      "[426]\ttrain-mlogloss:0.981931\n",
      "[427]\ttrain-mlogloss:0.981758\n",
      "[428]\ttrain-mlogloss:0.981564\n",
      "[429]\ttrain-mlogloss:0.981395\n",
      "[430]\ttrain-mlogloss:0.981201\n",
      "[431]\ttrain-mlogloss:0.980951\n",
      "[432]\ttrain-mlogloss:0.980724\n",
      "[433]\ttrain-mlogloss:0.980527\n",
      "[434]\ttrain-mlogloss:0.980366\n",
      "[435]\ttrain-mlogloss:0.980165\n",
      "[436]\ttrain-mlogloss:0.980028\n",
      "[437]\ttrain-mlogloss:0.979793\n",
      "[438]\ttrain-mlogloss:0.979545\n",
      "[439]\ttrain-mlogloss:0.979371\n",
      "[440]\ttrain-mlogloss:0.979171\n",
      "[441]\ttrain-mlogloss:0.97901\n",
      "[442]\ttrain-mlogloss:0.97886\n",
      "[443]\ttrain-mlogloss:0.978659\n",
      "[444]\ttrain-mlogloss:0.978463\n",
      "[445]\ttrain-mlogloss:0.978272\n",
      "[446]\ttrain-mlogloss:0.978123\n",
      "[447]\ttrain-mlogloss:0.977931\n",
      "[448]\ttrain-mlogloss:0.977755\n",
      "[449]\ttrain-mlogloss:0.977574\n",
      "[450]\ttrain-mlogloss:0.97734\n",
      "[451]\ttrain-mlogloss:0.977109\n",
      "[452]\ttrain-mlogloss:0.976898\n",
      "[453]\ttrain-mlogloss:0.976656\n",
      "[454]\ttrain-mlogloss:0.976486\n",
      "[455]\ttrain-mlogloss:0.976329\n",
      "[456]\ttrain-mlogloss:0.976158\n",
      "[457]\ttrain-mlogloss:0.97601\n",
      "[458]\ttrain-mlogloss:0.97584\n",
      "[459]\ttrain-mlogloss:0.975644\n",
      "[460]\ttrain-mlogloss:0.975502\n",
      "[461]\ttrain-mlogloss:0.975253\n",
      "[462]\ttrain-mlogloss:0.975091\n",
      "[463]\ttrain-mlogloss:0.974923\n",
      "[464]\ttrain-mlogloss:0.974684\n",
      "[465]\ttrain-mlogloss:0.97447\n",
      "[466]\ttrain-mlogloss:0.974226\n",
      "[467]\ttrain-mlogloss:0.974049\n",
      "[468]\ttrain-mlogloss:0.973818\n",
      "[469]\ttrain-mlogloss:0.973628\n",
      "[470]\ttrain-mlogloss:0.973485\n",
      "[471]\ttrain-mlogloss:0.973318\n",
      "[472]\ttrain-mlogloss:0.973177\n",
      "[473]\ttrain-mlogloss:0.97302\n",
      "[474]\ttrain-mlogloss:0.97284\n",
      "[475]\ttrain-mlogloss:0.972667\n",
      "[476]\ttrain-mlogloss:0.972451\n",
      "[477]\ttrain-mlogloss:0.972204\n",
      "[478]\ttrain-mlogloss:0.972006\n",
      "[479]\ttrain-mlogloss:0.971806\n",
      "[480]\ttrain-mlogloss:0.971632\n",
      "[481]\ttrain-mlogloss:0.971484\n",
      "[482]\ttrain-mlogloss:0.97135\n",
      "[483]\ttrain-mlogloss:0.971188\n",
      "[484]\ttrain-mlogloss:0.971005\n",
      "[485]\ttrain-mlogloss:0.970781\n",
      "[486]\ttrain-mlogloss:0.970602\n",
      "[487]\ttrain-mlogloss:0.970435\n",
      "[488]\ttrain-mlogloss:0.970243\n",
      "[489]\ttrain-mlogloss:0.970095\n",
      "[490]\ttrain-mlogloss:0.96983\n",
      "[491]\ttrain-mlogloss:0.9696\n",
      "[492]\ttrain-mlogloss:0.969388\n",
      "[493]\ttrain-mlogloss:0.969205\n",
      "[494]\ttrain-mlogloss:0.969004\n",
      "[495]\ttrain-mlogloss:0.968812\n",
      "[496]\ttrain-mlogloss:0.968587\n",
      "[497]\ttrain-mlogloss:0.968455\n",
      "[498]\ttrain-mlogloss:0.968253\n",
      "[499]\ttrain-mlogloss:0.968123\n",
      "[500]\ttrain-mlogloss:0.967879\n",
      "[501]\ttrain-mlogloss:0.967713\n",
      "[502]\ttrain-mlogloss:0.967503\n",
      "[503]\ttrain-mlogloss:0.96727\n",
      "[504]\ttrain-mlogloss:0.967078\n",
      "[505]\ttrain-mlogloss:0.966911\n",
      "[506]\ttrain-mlogloss:0.966659\n",
      "[507]\ttrain-mlogloss:0.966399\n",
      "[508]\ttrain-mlogloss:0.966146\n",
      "[509]\ttrain-mlogloss:0.965953\n",
      "[510]\ttrain-mlogloss:0.965802\n",
      "[511]\ttrain-mlogloss:0.965635\n",
      "[512]\ttrain-mlogloss:0.965467\n",
      "[513]\ttrain-mlogloss:0.965282\n",
      "[514]\ttrain-mlogloss:0.965081\n",
      "[515]\ttrain-mlogloss:0.964921\n",
      "[516]\ttrain-mlogloss:0.964712\n",
      "[517]\ttrain-mlogloss:0.964581\n",
      "[518]\ttrain-mlogloss:0.964418\n",
      "[519]\ttrain-mlogloss:0.964266\n",
      "[520]\ttrain-mlogloss:0.964081\n",
      "[521]\ttrain-mlogloss:0.963908\n",
      "[522]\ttrain-mlogloss:0.963798\n",
      "[523]\ttrain-mlogloss:0.963631\n",
      "[524]\ttrain-mlogloss:0.963408\n",
      "[525]\ttrain-mlogloss:0.963258\n",
      "[526]\ttrain-mlogloss:0.96308\n",
      "[527]\ttrain-mlogloss:0.962904\n",
      "[528]\ttrain-mlogloss:0.962702\n",
      "[529]\ttrain-mlogloss:0.96249\n",
      "[530]\ttrain-mlogloss:0.962304\n",
      "[531]\ttrain-mlogloss:0.962126\n",
      "[532]\ttrain-mlogloss:0.961942\n",
      "[533]\ttrain-mlogloss:0.961804\n",
      "[534]\ttrain-mlogloss:0.961688\n",
      "[535]\ttrain-mlogloss:0.961503\n",
      "[536]\ttrain-mlogloss:0.96135\n",
      "[537]\ttrain-mlogloss:0.961174\n",
      "[538]\ttrain-mlogloss:0.961019\n",
      "[539]\ttrain-mlogloss:0.9608\n",
      "[540]\ttrain-mlogloss:0.960669\n",
      "[541]\ttrain-mlogloss:0.960519\n",
      "[542]\ttrain-mlogloss:0.960397\n",
      "[543]\ttrain-mlogloss:0.960251\n",
      "[544]\ttrain-mlogloss:0.96007\n",
      "[545]\ttrain-mlogloss:0.959875\n",
      "[546]\ttrain-mlogloss:0.959662\n",
      "[547]\ttrain-mlogloss:0.959455\n",
      "[548]\ttrain-mlogloss:0.959277\n",
      "[549]\ttrain-mlogloss:0.959125\n",
      "[550]\ttrain-mlogloss:0.958922\n",
      "[551]\ttrain-mlogloss:0.958715\n",
      "[552]\ttrain-mlogloss:0.95854\n",
      "[553]\ttrain-mlogloss:0.958341\n",
      "[554]\ttrain-mlogloss:0.958204\n",
      "[555]\ttrain-mlogloss:0.957977\n",
      "[556]\ttrain-mlogloss:0.957781\n",
      "[557]\ttrain-mlogloss:0.957645\n",
      "[558]\ttrain-mlogloss:0.957413\n",
      "[559]\ttrain-mlogloss:0.957219\n",
      "[560]\ttrain-mlogloss:0.957012\n",
      "[561]\ttrain-mlogloss:0.956835\n",
      "[562]\ttrain-mlogloss:0.95665\n",
      "[563]\ttrain-mlogloss:0.956468\n",
      "[564]\ttrain-mlogloss:0.956334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[565]\ttrain-mlogloss:0.956165\n",
      "[566]\ttrain-mlogloss:0.956041\n",
      "[567]\ttrain-mlogloss:0.955857\n",
      "[568]\ttrain-mlogloss:0.955697\n",
      "[569]\ttrain-mlogloss:0.955584\n",
      "[570]\ttrain-mlogloss:0.95544\n",
      "[571]\ttrain-mlogloss:0.955288\n",
      "[572]\ttrain-mlogloss:0.955145\n",
      "[573]\ttrain-mlogloss:0.954946\n",
      "[574]\ttrain-mlogloss:0.954797\n",
      "[575]\ttrain-mlogloss:0.954673\n",
      "[576]\ttrain-mlogloss:0.954503\n",
      "[577]\ttrain-mlogloss:0.954376\n",
      "[578]\ttrain-mlogloss:0.954198\n",
      "[579]\ttrain-mlogloss:0.954079\n",
      "[580]\ttrain-mlogloss:0.953948\n",
      "[581]\ttrain-mlogloss:0.953766\n",
      "[582]\ttrain-mlogloss:0.953572\n",
      "[583]\ttrain-mlogloss:0.953385\n",
      "[584]\ttrain-mlogloss:0.953281\n",
      "[585]\ttrain-mlogloss:0.953087\n",
      "[586]\ttrain-mlogloss:0.952973\n",
      "[587]\ttrain-mlogloss:0.952797\n",
      "[588]\ttrain-mlogloss:0.952588\n",
      "[589]\ttrain-mlogloss:0.952429\n",
      "[590]\ttrain-mlogloss:0.952232\n",
      "[591]\ttrain-mlogloss:0.952055\n",
      "[592]\ttrain-mlogloss:0.951889\n",
      "[593]\ttrain-mlogloss:0.951658\n",
      "[594]\ttrain-mlogloss:0.951503\n",
      "[595]\ttrain-mlogloss:0.951333\n",
      "[596]\ttrain-mlogloss:0.951209\n",
      "[597]\ttrain-mlogloss:0.951071\n",
      "[598]\ttrain-mlogloss:0.950905\n",
      "[599]\ttrain-mlogloss:0.950723\n",
      "[600]\ttrain-mlogloss:0.950559\n",
      "[601]\ttrain-mlogloss:0.950441\n",
      "[602]\ttrain-mlogloss:0.950275\n",
      "[603]\ttrain-mlogloss:0.95012\n",
      "[604]\ttrain-mlogloss:0.950004\n",
      "[605]\ttrain-mlogloss:0.949857\n",
      "[606]\ttrain-mlogloss:0.949708\n",
      "[607]\ttrain-mlogloss:0.949566\n",
      "[608]\ttrain-mlogloss:0.949422\n",
      "[609]\ttrain-mlogloss:0.949283\n",
      "[610]\ttrain-mlogloss:0.949136\n",
      "[611]\ttrain-mlogloss:0.948952\n",
      "[612]\ttrain-mlogloss:0.948756\n",
      "[613]\ttrain-mlogloss:0.94858\n",
      "[614]\ttrain-mlogloss:0.948417\n",
      "[615]\ttrain-mlogloss:0.948269\n",
      "[616]\ttrain-mlogloss:0.948119\n",
      "[617]\ttrain-mlogloss:0.947933\n",
      "[618]\ttrain-mlogloss:0.94771\n",
      "[619]\ttrain-mlogloss:0.947532\n",
      "[620]\ttrain-mlogloss:0.947289\n",
      "[621]\ttrain-mlogloss:0.947148\n",
      "[622]\ttrain-mlogloss:0.947008\n",
      "[623]\ttrain-mlogloss:0.946842\n",
      "[624]\ttrain-mlogloss:0.946695\n",
      "[625]\ttrain-mlogloss:0.946537\n",
      "[626]\ttrain-mlogloss:0.946401\n",
      "[627]\ttrain-mlogloss:0.946249\n",
      "[628]\ttrain-mlogloss:0.946047\n",
      "[629]\ttrain-mlogloss:0.945876\n",
      "[630]\ttrain-mlogloss:0.9457\n",
      "[631]\ttrain-mlogloss:0.94552\n",
      "[632]\ttrain-mlogloss:0.945362\n",
      "[633]\ttrain-mlogloss:0.945192\n",
      "[634]\ttrain-mlogloss:0.945046\n",
      "[635]\ttrain-mlogloss:0.944937\n",
      "[636]\ttrain-mlogloss:0.94472\n",
      "[637]\ttrain-mlogloss:0.944602\n",
      "[638]\ttrain-mlogloss:0.944456\n",
      "[639]\ttrain-mlogloss:0.944305\n",
      "[640]\ttrain-mlogloss:0.944165\n",
      "[641]\ttrain-mlogloss:0.944019\n",
      "[642]\ttrain-mlogloss:0.943806\n",
      "[643]\ttrain-mlogloss:0.943624\n",
      "[644]\ttrain-mlogloss:0.943484\n",
      "[645]\ttrain-mlogloss:0.943358\n",
      "[646]\ttrain-mlogloss:0.94319\n",
      "[647]\ttrain-mlogloss:0.943055\n",
      "[648]\ttrain-mlogloss:0.942876\n",
      "[649]\ttrain-mlogloss:0.942742\n",
      "[650]\ttrain-mlogloss:0.942558\n",
      "[651]\ttrain-mlogloss:0.942324\n",
      "[652]\ttrain-mlogloss:0.9422\n",
      "[653]\ttrain-mlogloss:0.942073\n",
      "[654]\ttrain-mlogloss:0.941952\n",
      "[655]\ttrain-mlogloss:0.9418\n",
      "[656]\ttrain-mlogloss:0.941595\n",
      "[657]\ttrain-mlogloss:0.94145\n",
      "[658]\ttrain-mlogloss:0.941274\n",
      "[659]\ttrain-mlogloss:0.941101\n",
      "[660]\ttrain-mlogloss:0.940923\n",
      "[661]\ttrain-mlogloss:0.940704\n",
      "[662]\ttrain-mlogloss:0.940539\n",
      "[663]\ttrain-mlogloss:0.940386\n",
      "[664]\ttrain-mlogloss:0.94021\n",
      "[665]\ttrain-mlogloss:0.940052\n",
      "[666]\ttrain-mlogloss:0.939921\n",
      "[667]\ttrain-mlogloss:0.939814\n",
      "[668]\ttrain-mlogloss:0.939689\n",
      "[669]\ttrain-mlogloss:0.939565\n",
      "[670]\ttrain-mlogloss:0.939416\n",
      "[671]\ttrain-mlogloss:0.939278\n",
      "[672]\ttrain-mlogloss:0.939092\n",
      "[673]\ttrain-mlogloss:0.938897\n",
      "[674]\ttrain-mlogloss:0.938718\n",
      "[675]\ttrain-mlogloss:0.938556\n",
      "[676]\ttrain-mlogloss:0.938411\n",
      "[677]\ttrain-mlogloss:0.938238\n",
      "[678]\ttrain-mlogloss:0.938079\n",
      "[679]\ttrain-mlogloss:0.937958\n",
      "[680]\ttrain-mlogloss:0.937773\n",
      "[681]\ttrain-mlogloss:0.937621\n",
      "[682]\ttrain-mlogloss:0.937483\n",
      "[683]\ttrain-mlogloss:0.93735\n",
      "[684]\ttrain-mlogloss:0.937214\n",
      "[685]\ttrain-mlogloss:0.937056\n",
      "[686]\ttrain-mlogloss:0.936868\n",
      "[687]\ttrain-mlogloss:0.936717\n",
      "[688]\ttrain-mlogloss:0.936592\n",
      "[689]\ttrain-mlogloss:0.936444\n",
      "[690]\ttrain-mlogloss:0.936252\n",
      "[691]\ttrain-mlogloss:0.936119\n",
      "[692]\ttrain-mlogloss:0.935981\n",
      "[693]\ttrain-mlogloss:0.935862\n",
      "[694]\ttrain-mlogloss:0.93568\n",
      "[695]\ttrain-mlogloss:0.935529\n",
      "[696]\ttrain-mlogloss:0.935401\n",
      "[697]\ttrain-mlogloss:0.935259\n",
      "[698]\ttrain-mlogloss:0.935108\n",
      "[699]\ttrain-mlogloss:0.934957\n",
      "[700]\ttrain-mlogloss:0.934852\n",
      "[701]\ttrain-mlogloss:0.934641\n",
      "[702]\ttrain-mlogloss:0.934461\n",
      "[703]\ttrain-mlogloss:0.934271\n",
      "[704]\ttrain-mlogloss:0.934139\n",
      "[705]\ttrain-mlogloss:0.933991\n",
      "[706]\ttrain-mlogloss:0.933753\n",
      "[707]\ttrain-mlogloss:0.93361\n",
      "[708]\ttrain-mlogloss:0.933473\n",
      "[709]\ttrain-mlogloss:0.933334\n",
      "[710]\ttrain-mlogloss:0.93316\n",
      "[711]\ttrain-mlogloss:0.933045\n",
      "[712]\ttrain-mlogloss:0.932867\n",
      "[713]\ttrain-mlogloss:0.932697\n",
      "[714]\ttrain-mlogloss:0.932491\n",
      "[715]\ttrain-mlogloss:0.932379\n",
      "[716]\ttrain-mlogloss:0.932259\n",
      "[717]\ttrain-mlogloss:0.932108\n",
      "[718]\ttrain-mlogloss:0.931904\n",
      "[719]\ttrain-mlogloss:0.931797\n",
      "[720]\ttrain-mlogloss:0.931703\n",
      "[721]\ttrain-mlogloss:0.931519\n",
      "[722]\ttrain-mlogloss:0.931369\n",
      "[723]\ttrain-mlogloss:0.931216\n",
      "[724]\ttrain-mlogloss:0.93106\n",
      "[725]\ttrain-mlogloss:0.930912\n",
      "[726]\ttrain-mlogloss:0.930805\n",
      "[727]\ttrain-mlogloss:0.930671\n",
      "[728]\ttrain-mlogloss:0.930522\n",
      "[729]\ttrain-mlogloss:0.93041\n",
      "[730]\ttrain-mlogloss:0.930303\n",
      "[731]\ttrain-mlogloss:0.930202\n",
      "[732]\ttrain-mlogloss:0.930111\n",
      "[733]\ttrain-mlogloss:0.929976\n",
      "[734]\ttrain-mlogloss:0.929854\n",
      "[735]\ttrain-mlogloss:0.92975\n",
      "[736]\ttrain-mlogloss:0.929623\n",
      "[737]\ttrain-mlogloss:0.929482\n",
      "[738]\ttrain-mlogloss:0.929338\n",
      "[739]\ttrain-mlogloss:0.929139\n",
      "[740]\ttrain-mlogloss:0.928955\n",
      "[741]\ttrain-mlogloss:0.928775\n",
      "[742]\ttrain-mlogloss:0.92862\n",
      "[743]\ttrain-mlogloss:0.928483\n",
      "[744]\ttrain-mlogloss:0.928329\n",
      "[745]\ttrain-mlogloss:0.928211\n",
      "[746]\ttrain-mlogloss:0.928081\n",
      "[747]\ttrain-mlogloss:0.927864\n",
      "[748]\ttrain-mlogloss:0.927765\n",
      "[749]\ttrain-mlogloss:0.927679\n",
      "[750]\ttrain-mlogloss:0.927487\n",
      "[751]\ttrain-mlogloss:0.927367\n",
      "[752]\ttrain-mlogloss:0.927256\n",
      "[753]\ttrain-mlogloss:0.927116\n",
      "[754]\ttrain-mlogloss:0.927035\n",
      "[755]\ttrain-mlogloss:0.926865\n",
      "[756]\ttrain-mlogloss:0.9267\n",
      "[757]\ttrain-mlogloss:0.926569\n",
      "[758]\ttrain-mlogloss:0.926394\n",
      "[759]\ttrain-mlogloss:0.926209\n",
      "[760]\ttrain-mlogloss:0.926026\n",
      "[761]\ttrain-mlogloss:0.925887\n",
      "[762]\ttrain-mlogloss:0.925664\n",
      "[763]\ttrain-mlogloss:0.925533\n",
      "[764]\ttrain-mlogloss:0.925355\n",
      "[765]\ttrain-mlogloss:0.925184\n",
      "[766]\ttrain-mlogloss:0.925019\n",
      "[767]\ttrain-mlogloss:0.924868\n",
      "[768]\ttrain-mlogloss:0.924716\n",
      "[769]\ttrain-mlogloss:0.924581\n",
      "[770]\ttrain-mlogloss:0.924407\n",
      "[771]\ttrain-mlogloss:0.924227\n",
      "[772]\ttrain-mlogloss:0.924099\n",
      "[773]\ttrain-mlogloss:0.923976\n",
      "[774]\ttrain-mlogloss:0.923894\n",
      "[775]\ttrain-mlogloss:0.9237\n",
      "[776]\ttrain-mlogloss:0.923564\n",
      "[777]\ttrain-mlogloss:0.923433\n",
      "[778]\ttrain-mlogloss:0.923261\n",
      "[779]\ttrain-mlogloss:0.923081\n",
      "[780]\ttrain-mlogloss:0.923007\n",
      "[781]\ttrain-mlogloss:0.922892\n",
      "[782]\ttrain-mlogloss:0.922772\n",
      "[783]\ttrain-mlogloss:0.922598\n",
      "[784]\ttrain-mlogloss:0.922475\n",
      "[785]\ttrain-mlogloss:0.922383\n",
      "[786]\ttrain-mlogloss:0.922259\n",
      "[787]\ttrain-mlogloss:0.922133\n",
      "[788]\ttrain-mlogloss:0.922022\n",
      "[789]\ttrain-mlogloss:0.921862\n",
      "[790]\ttrain-mlogloss:0.921754\n",
      "[791]\ttrain-mlogloss:0.9216\n",
      "[792]\ttrain-mlogloss:0.921475\n",
      "[793]\ttrain-mlogloss:0.921321\n",
      "[794]\ttrain-mlogloss:0.921144\n",
      "[795]\ttrain-mlogloss:0.920931\n",
      "[796]\ttrain-mlogloss:0.920712\n",
      "[797]\ttrain-mlogloss:0.92059\n",
      "[798]\ttrain-mlogloss:0.920398\n",
      "[799]\ttrain-mlogloss:0.92021\n",
      "[800]\ttrain-mlogloss:0.920081\n",
      "[801]\ttrain-mlogloss:0.919915\n",
      "[802]\ttrain-mlogloss:0.919747\n",
      "[803]\ttrain-mlogloss:0.919608\n",
      "[804]\ttrain-mlogloss:0.91943\n",
      "[805]\ttrain-mlogloss:0.919206\n",
      "[806]\ttrain-mlogloss:0.919041\n",
      "[807]\ttrain-mlogloss:0.918917\n",
      "[808]\ttrain-mlogloss:0.918737\n",
      "[809]\ttrain-mlogloss:0.918527\n",
      "[810]\ttrain-mlogloss:0.918392\n",
      "[811]\ttrain-mlogloss:0.918276\n",
      "[812]\ttrain-mlogloss:0.918173\n",
      "[813]\ttrain-mlogloss:0.91803\n",
      "[814]\ttrain-mlogloss:0.917908\n",
      "[815]\ttrain-mlogloss:0.917769\n",
      "[816]\ttrain-mlogloss:0.917641\n",
      "[817]\ttrain-mlogloss:0.917494\n",
      "[818]\ttrain-mlogloss:0.91737\n",
      "[819]\ttrain-mlogloss:0.917242\n",
      "[820]\ttrain-mlogloss:0.917121\n",
      "[821]\ttrain-mlogloss:0.916983\n",
      "[822]\ttrain-mlogloss:0.91687\n",
      "[823]\ttrain-mlogloss:0.916795\n",
      "[824]\ttrain-mlogloss:0.916632\n",
      "[825]\ttrain-mlogloss:0.916509\n",
      "[826]\ttrain-mlogloss:0.916366\n",
      "[827]\ttrain-mlogloss:0.916199\n",
      "[828]\ttrain-mlogloss:0.916105\n",
      "[829]\ttrain-mlogloss:0.915989\n",
      "[830]\ttrain-mlogloss:0.915801\n",
      "[831]\ttrain-mlogloss:0.915686\n",
      "[832]\ttrain-mlogloss:0.915608\n",
      "[833]\ttrain-mlogloss:0.915491\n",
      "[834]\ttrain-mlogloss:0.915369\n",
      "[835]\ttrain-mlogloss:0.915206\n",
      "[836]\ttrain-mlogloss:0.915045\n",
      "[837]\ttrain-mlogloss:0.914886\n",
      "[838]\ttrain-mlogloss:0.914736\n",
      "[839]\ttrain-mlogloss:0.914587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[840]\ttrain-mlogloss:0.914456\n",
      "[841]\ttrain-mlogloss:0.914306\n",
      "[842]\ttrain-mlogloss:0.914154\n",
      "[843]\ttrain-mlogloss:0.914025\n",
      "[844]\ttrain-mlogloss:0.913873\n",
      "[845]\ttrain-mlogloss:0.913719\n",
      "[846]\ttrain-mlogloss:0.913594\n",
      "[847]\ttrain-mlogloss:0.913463\n",
      "[848]\ttrain-mlogloss:0.91334\n",
      "[849]\ttrain-mlogloss:0.913222\n",
      "[850]\ttrain-mlogloss:0.913061\n",
      "[851]\ttrain-mlogloss:0.912963\n",
      "[852]\ttrain-mlogloss:0.912862\n",
      "[853]\ttrain-mlogloss:0.912664\n",
      "[854]\ttrain-mlogloss:0.912547\n",
      "[855]\ttrain-mlogloss:0.91234\n",
      "[856]\ttrain-mlogloss:0.912205\n",
      "[857]\ttrain-mlogloss:0.912085\n",
      "[858]\ttrain-mlogloss:0.911944\n",
      "[859]\ttrain-mlogloss:0.911806\n",
      "[860]\ttrain-mlogloss:0.911662\n",
      "[861]\ttrain-mlogloss:0.911524\n",
      "[862]\ttrain-mlogloss:0.9114\n",
      "[863]\ttrain-mlogloss:0.911277\n",
      "[864]\ttrain-mlogloss:0.911126\n",
      "[865]\ttrain-mlogloss:0.911021\n",
      "[866]\ttrain-mlogloss:0.910905\n",
      "[867]\ttrain-mlogloss:0.910776\n",
      "[868]\ttrain-mlogloss:0.910671\n",
      "[869]\ttrain-mlogloss:0.910555\n",
      "[870]\ttrain-mlogloss:0.910414\n",
      "[871]\ttrain-mlogloss:0.910296\n",
      "[872]\ttrain-mlogloss:0.910207\n",
      "[873]\ttrain-mlogloss:0.910099\n",
      "[874]\ttrain-mlogloss:0.909973\n",
      "[875]\ttrain-mlogloss:0.909862\n",
      "[876]\ttrain-mlogloss:0.909729\n",
      "[877]\ttrain-mlogloss:0.909644\n",
      "[878]\ttrain-mlogloss:0.909486\n",
      "[879]\ttrain-mlogloss:0.90932\n",
      "[880]\ttrain-mlogloss:0.909222\n",
      "[881]\ttrain-mlogloss:0.909077\n",
      "[882]\ttrain-mlogloss:0.90889\n",
      "[883]\ttrain-mlogloss:0.908791\n",
      "[884]\ttrain-mlogloss:0.908693\n",
      "[885]\ttrain-mlogloss:0.908544\n",
      "[886]\ttrain-mlogloss:0.908432\n",
      "[887]\ttrain-mlogloss:0.908309\n",
      "[888]\ttrain-mlogloss:0.908205\n",
      "[889]\ttrain-mlogloss:0.908092\n",
      "[890]\ttrain-mlogloss:0.908004\n",
      "[891]\ttrain-mlogloss:0.907825\n",
      "[892]\ttrain-mlogloss:0.907701\n",
      "[893]\ttrain-mlogloss:0.907531\n",
      "[894]\ttrain-mlogloss:0.907403\n",
      "[895]\ttrain-mlogloss:0.907281\n",
      "[896]\ttrain-mlogloss:0.907132\n",
      "[897]\ttrain-mlogloss:0.906969\n",
      "[898]\ttrain-mlogloss:0.906891\n",
      "[899]\ttrain-mlogloss:0.906727\n",
      "[900]\ttrain-mlogloss:0.906588\n",
      "[901]\ttrain-mlogloss:0.906436\n",
      "[902]\ttrain-mlogloss:0.906277\n",
      "[903]\ttrain-mlogloss:0.906176\n",
      "[904]\ttrain-mlogloss:0.906049\n",
      "[905]\ttrain-mlogloss:0.905887\n",
      "[906]\ttrain-mlogloss:0.90577\n",
      "[907]\ttrain-mlogloss:0.905673\n",
      "[908]\ttrain-mlogloss:0.905529\n",
      "[909]\ttrain-mlogloss:0.905372\n",
      "[910]\ttrain-mlogloss:0.905274\n",
      "[911]\ttrain-mlogloss:0.905149\n",
      "[912]\ttrain-mlogloss:0.90506\n",
      "[913]\ttrain-mlogloss:0.904932\n",
      "[914]\ttrain-mlogloss:0.904839\n",
      "[915]\ttrain-mlogloss:0.904739\n",
      "[916]\ttrain-mlogloss:0.904635\n",
      "[917]\ttrain-mlogloss:0.904493\n",
      "[918]\ttrain-mlogloss:0.904383\n",
      "[919]\ttrain-mlogloss:0.904299\n",
      "[920]\ttrain-mlogloss:0.904208\n",
      "[921]\ttrain-mlogloss:0.904107\n",
      "[922]\ttrain-mlogloss:0.904023\n",
      "[923]\ttrain-mlogloss:0.903912\n",
      "[924]\ttrain-mlogloss:0.903831\n",
      "[925]\ttrain-mlogloss:0.903726\n",
      "[926]\ttrain-mlogloss:0.90359\n",
      "[927]\ttrain-mlogloss:0.903487\n",
      "[928]\ttrain-mlogloss:0.903378\n",
      "[929]\ttrain-mlogloss:0.903258\n",
      "[930]\ttrain-mlogloss:0.903157\n",
      "[931]\ttrain-mlogloss:0.903026\n",
      "[932]\ttrain-mlogloss:0.902866\n",
      "[933]\ttrain-mlogloss:0.902685\n",
      "[934]\ttrain-mlogloss:0.902568\n",
      "[935]\ttrain-mlogloss:0.90245\n",
      "[936]\ttrain-mlogloss:0.902357\n",
      "[937]\ttrain-mlogloss:0.902275\n",
      "[938]\ttrain-mlogloss:0.902149\n",
      "[939]\ttrain-mlogloss:0.901994\n",
      "[940]\ttrain-mlogloss:0.901855\n",
      "[941]\ttrain-mlogloss:0.901727\n",
      "[942]\ttrain-mlogloss:0.901576\n",
      "[943]\ttrain-mlogloss:0.90143\n",
      "[944]\ttrain-mlogloss:0.901288\n",
      "[945]\ttrain-mlogloss:0.901152\n",
      "[946]\ttrain-mlogloss:0.901026\n",
      "[947]\ttrain-mlogloss:0.90093\n",
      "[948]\ttrain-mlogloss:0.900861\n",
      "[949]\ttrain-mlogloss:0.900733\n",
      "[950]\ttrain-mlogloss:0.900588\n",
      "[951]\ttrain-mlogloss:0.900443\n",
      "[952]\ttrain-mlogloss:0.900313\n",
      "[953]\ttrain-mlogloss:0.900209\n",
      "[954]\ttrain-mlogloss:0.900103\n",
      "[955]\ttrain-mlogloss:0.899984\n",
      "[956]\ttrain-mlogloss:0.899851\n",
      "[957]\ttrain-mlogloss:0.89972\n",
      "[958]\ttrain-mlogloss:0.899625\n",
      "[959]\ttrain-mlogloss:0.899517\n",
      "[960]\ttrain-mlogloss:0.899388\n",
      "[961]\ttrain-mlogloss:0.899281\n",
      "[962]\ttrain-mlogloss:0.899186\n",
      "[963]\ttrain-mlogloss:0.899014\n",
      "[964]\ttrain-mlogloss:0.898883\n",
      "[965]\ttrain-mlogloss:0.898777\n",
      "[966]\ttrain-mlogloss:0.898649\n",
      "[967]\ttrain-mlogloss:0.898529\n",
      "[968]\ttrain-mlogloss:0.898336\n",
      "[969]\ttrain-mlogloss:0.898246\n",
      "[970]\ttrain-mlogloss:0.898046\n",
      "[971]\ttrain-mlogloss:0.897916\n",
      "[972]\ttrain-mlogloss:0.897797\n",
      "[973]\ttrain-mlogloss:0.89763\n",
      "[974]\ttrain-mlogloss:0.897474\n",
      "[975]\ttrain-mlogloss:0.897357\n",
      "[976]\ttrain-mlogloss:0.897214\n",
      "[977]\ttrain-mlogloss:0.897089\n",
      "[978]\ttrain-mlogloss:0.896933\n",
      "[979]\ttrain-mlogloss:0.896816\n",
      "[980]\ttrain-mlogloss:0.896655\n",
      "[981]\ttrain-mlogloss:0.896563\n",
      "[982]\ttrain-mlogloss:0.896463\n",
      "[983]\ttrain-mlogloss:0.896351\n",
      "[984]\ttrain-mlogloss:0.89625\n",
      "[985]\ttrain-mlogloss:0.896145\n",
      "[986]\ttrain-mlogloss:0.896028\n",
      "[987]\ttrain-mlogloss:0.895883\n",
      "[988]\ttrain-mlogloss:0.895759\n",
      "[989]\ttrain-mlogloss:0.89566\n",
      "[990]\ttrain-mlogloss:0.895569\n",
      "[991]\ttrain-mlogloss:0.895443\n",
      "[992]\ttrain-mlogloss:0.895295\n",
      "[993]\ttrain-mlogloss:0.89516\n",
      "[994]\ttrain-mlogloss:0.895018\n",
      "[995]\ttrain-mlogloss:0.894882\n",
      "[996]\ttrain-mlogloss:0.894757\n",
      "[997]\ttrain-mlogloss:0.894634\n",
      "[998]\ttrain-mlogloss:0.894522\n",
      "Feature importance:\n",
      "('renta', 216748)\n",
      "('antiguedad', 136916)\n",
      "('age', 133136)\n",
      "('fecha_alta_month', 80890)\n",
      "('nomprov', 75061)\n",
      "('renta_log', 67975)\n",
      "('fecha_alta_year', 61270)\n",
      "('canal_entrada', 56583)\n",
      "('sexo', 19700)\n",
      "('segmento', 17961)\n",
      "('ind_cco_fin_ult1_prev', 17479)\n",
      "('ind_recibo_ult1_prev', 16241)\n",
      "('ind_cno_fin_ult1_prev', 13899)\n",
      "('ind_ecue_fin_ult1_prev', 13441)\n",
      "('ind_reca_fin_ult1_prev', 9605)\n",
      "('ind_tjcr_fin_ult1_prev', 9458)\n",
      "('ind_ctop_fin_ult1_prev', 8470)\n",
      "('tiprel_1mes', 7425)\n",
      "('ind_dela_fin_ult1_prev', 7259)\n",
      "('ind_nom_pens_ult1_prev', 6459)\n",
      "('ind_ctpp_fin_ult1_prev', 5725)\n",
      "('ind_valo_fin_ult1_prev', 5615)\n",
      "('ind_actividad_cliente', 5064)\n",
      "('ind_nomina_ult1_prev', 4983)\n",
      "('ind_fond_fin_ult1_prev', 4303)\n",
      "('indext', 4191)\n",
      "('ind_ctma_fin_ult1_prev', 3558)\n",
      "('pais_residencia', 3290)\n",
      "('ind_plan_fin_ult1_prev', 3263)\n",
      "('ind_nuevo', 3237)\n",
      "('ind_hip_fin_ult1_prev', 2018)\n",
      "('ind_deco_fin_ult1_prev', 1965)\n",
      "('ind_empleado', 1859)\n",
      "('ind_viv_fin_ult1_prev', 1307)\n",
      "('ind_deme_fin_ult1_prev', 1011)\n",
      "('ind_cder_fin_ult1_prev', 837)\n",
      "('indrel', 796)\n",
      "('ind_pres_fin_ult1_prev', 787)\n",
      "('ind_ctju_fin_ult1_prev', 758)\n",
      "('indfall', 495)\n",
      "('ind_ahor_fin_ult1_prev', 247)\n",
      "('ind_aval_fin_ult1_prev', 239)\n",
      "('indresi', 167)\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 모델을 전체 훈련 데이터로 재학습한다!\n",
    "X_all = XY.as_matrix(columns=features)\n",
    "Y_all = XY.as_matrix(columns=['y'])\n",
    "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "watch_list = [(dall, 'train')]\n",
    "\n",
    "# XGBoost 모델 재학습!\n",
    "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list )\n",
    "\n",
    "# 변수 중요도를 출력해본다. 예상하던 변수가 상위로 올라와 있는가?\n",
    "print(\"Feature importance:\")\n",
    "for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "    print(kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"../model/xgb.baseline_bestnumtree.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lightGBM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 훈련 데이터에는 늘어난 양만큼 트리 개수를 늘린다\n",
    "best_iteration = int(best_iteration * len(XY_all) / len(XY_train))\n",
    "\n",
    "# 전체 훈련 데이터에 대한 LightGBM 전용 데이터를 생성한다\n",
    "all_train = lgbm.Dataset(XY_all[list(features)], label=XY_all[\"y\"], weight=XY_all[\"weight\"], feature_name=features)\n",
    "\n",
    "# LightGBM 모델 학습!\n",
    "model = lgbm.train(params, all_train, num_boost_round=best_iteration)\n",
    "model.save_model(\"../model/lgbm.all.model.txt\")\n",
    "\n",
    "# LightGBM 모델이 제공하는 변수 중요도 기능을 통해 변수 중요도를 출력한다\n",
    "print(\"Feature importance by split:\")\n",
    "for kv in sorted([(k,v) for k,v in zip(features, model.feature_importance(\"split\"))], key=lambda kv: kv[1], reverse=True):\n",
    "print(kv)\n",
    "print(\"Feature importance by gain:\")\n",
    "for kv in sorted([(k,v) for k,v in zip(features, model.feature_importance(\"gain\"))], key=lambda kv: kv[1], reverse=True):\n",
    "print(kv)\n",
    "\n",
    "# 테스트 데이터에 대한 예측 결과물을 return한다\n",
    "y_lightgbm = model.predict(test_df[list(features)], num_iteration=best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tst = tst[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
    "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
    "ncodpers_tst = tst.as_matrix(columns=['ncodpers'])\n",
    "preds_tst = preds_tst - tst.as_matrix(columns=[prod + '_prev' for prod in prods])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Products for customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일을 생성한다.\n",
    "submit_file = open('../model/xgb_baseline_0731_1', 'w')\n",
    "submit_file.write('ncodpers,added_products\\n')\n",
    "Y_ret=[]\n",
    "\n",
    "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    y_prods = [p for y,p,ip in y_prods]\n",
    "    data = '{},{}\\n'.format(int(ncodper), ' '.join(y_prods))\n",
    "    Y_ret.append(data)\n",
    "    submit_file.write(data)\n",
    "    \n",
    "submit_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
