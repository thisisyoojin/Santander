{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santander Product Recommendation\n",
    "\n",
    "### Part 3. Evaluation\n",
    "\n",
    "The important process to build the good model is evaluating the model to compare the performances.\n",
    "\n",
    "### 3-1.  Prepare Data\n",
    "\n",
    "First, we load data previously pre-processed and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras import models, layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/meta_data.pkl', 'rb') as fin:\n",
    "    meta = pickle.load(fin)\n",
    "\n",
    "features = meta['features']\n",
    "target = meta['target']\n",
    "prods = meta['prods']\n",
    "\n",
    "with open('../input/processed_data.pkl', 'rb') as finn:\n",
    "    data = pickle.load(finn)\n",
    "\n",
    "#validation data\n",
    "tst_all = data['tst_all']\n",
    "\n",
    "#training data\n",
    "trn = data['trn_all']\n",
    "\n",
    "del meta, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input/vlds.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'vld_all': trn\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Prepare Evaluation  Method\n",
    "\n",
    "Our evaluation method is MAP@7, which compare predictions and the actually purchased product list, and average them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncodpers_tst_vld = tst_vld['ncodpers'].values\n",
    "\n",
    "def get_purchased_products():    \n",
    "\n",
    "    # get newly bought products on the validation data\n",
    "    for prod in prods:\n",
    "        prev = prod + '_prev'\n",
    "        padd = prod + '_add'\n",
    "        tst_vld[padd] = tst_vld[prod] - tst_vld[prev]\n",
    "    \n",
    "    add_vld = tst_vld[[prod + '_add' for prod in prods]].values\n",
    "    add_vld_list = [list() for i in range(len(ncodpers_tst_vld))]\n",
    "\n",
    "    count_vld = 0\n",
    "    for ncodper in range(len(ncodpers_tst_vld)):\n",
    "        for prod in range(len(prods)):\n",
    "            if add_vld[ncodper, prod] > 0:\n",
    "                add_vld_list[ncodper].append(prod)\n",
    "                count_vld += 1\n",
    "                \n",
    "    return add_vld_list\n",
    "\n",
    "add_vld_list = get_purchased_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 7 products wigh high values based on the prediction\n",
    "def predict_7_products(preds_vld):\n",
    "    result_vld = []\n",
    "    for ncodper, pred in zip(ncodpers_tst_vld, preds_vld):\n",
    "        y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods[target], target)]\n",
    "        y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "        result_vld.append([ip for y,p,ip in y_prods])\n",
    "    \n",
    "    return result_vld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=7, default=0.0):\n",
    "\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        # When prediction is in actual products list and not duplicated, it scores\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return default\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=7, default=0.0):\n",
    "    return np.mean([apk(a, p, k, default) for a, p in zip(actual, predicted)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04266379915553903\n"
     ]
    }
   ],
   "source": [
    "# Get the highest score from validation data(0.042613)\n",
    "print(mapk(add_vld_list, add_vld_list, 7, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Build Simple Model\n",
    "\n",
    "For preprocessing all data, we assemble train data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10765757, 60), (10765757,), (689132, 60), (689132,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vld_date = '2016-05-28'\n",
    "\n",
    "trn_vld = trn[trn['fecha_dato'] < vld_date]\n",
    "eval_vld = trn[trn['fecha_dato']==vld_date]\n",
    "\n",
    "X_trn_vld = trn_vld[features].values\n",
    "y_trn_vld = trn_vld['target'].values\n",
    "\n",
    "X_eval_vld = eval_vld[features].values\n",
    "y_eval_vld = eval_vld['target'].values\n",
    "\n",
    "X_trn_vld.shape, y_trn_vld.shape, X_eval_vld.shape, y_eval_vld.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.11779\teval-mlogloss:2.09076\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:1.82085\teval-mlogloss:1.75636\n",
      "[2]\ttrain-mlogloss:1.63405\teval-mlogloss:1.52409\n",
      "[3]\ttrain-mlogloss:1.44439\teval-mlogloss:1.34769\n",
      "[4]\ttrain-mlogloss:1.30098\teval-mlogloss:1.2078\n",
      "[5]\ttrain-mlogloss:1.21081\teval-mlogloss:1.08793\n",
      "[6]\ttrain-mlogloss:1.11172\teval-mlogloss:0.989476\n",
      "[7]\ttrain-mlogloss:1.02199\teval-mlogloss:0.903919\n",
      "[8]\ttrain-mlogloss:0.948211\teval-mlogloss:0.828923\n",
      "[9]\ttrain-mlogloss:0.892847\teval-mlogloss:0.763686\n",
      "[10]\ttrain-mlogloss:0.834503\teval-mlogloss:0.705944\n",
      "[11]\ttrain-mlogloss:0.791048\teval-mlogloss:0.655371\n",
      "[12]\ttrain-mlogloss:0.748521\teval-mlogloss:0.610169\n",
      "[13]\ttrain-mlogloss:0.713106\teval-mlogloss:0.569988\n",
      "[14]\ttrain-mlogloss:0.6793\teval-mlogloss:0.534392\n",
      "[15]\ttrain-mlogloss:0.642269\teval-mlogloss:0.501605\n",
      "[16]\ttrain-mlogloss:0.617519\teval-mlogloss:0.472721\n",
      "[17]\ttrain-mlogloss:0.589075\teval-mlogloss:0.447039\n",
      "[18]\ttrain-mlogloss:0.571274\teval-mlogloss:0.423796\n",
      "[19]\ttrain-mlogloss:0.551139\teval-mlogloss:0.402494\n",
      "[20]\ttrain-mlogloss:0.535602\teval-mlogloss:0.383711\n",
      "[21]\ttrain-mlogloss:0.524963\teval-mlogloss:0.366933\n",
      "[22]\ttrain-mlogloss:0.509045\teval-mlogloss:0.351844\n",
      "[23]\ttrain-mlogloss:0.499826\teval-mlogloss:0.337971\n",
      "[24]\ttrain-mlogloss:0.486401\teval-mlogloss:0.325247\n",
      "[25]\ttrain-mlogloss:0.469134\teval-mlogloss:0.314352\n",
      "[26]\ttrain-mlogloss:0.457152\teval-mlogloss:0.303718\n",
      "[27]\ttrain-mlogloss:0.449309\teval-mlogloss:0.294732\n",
      "[28]\ttrain-mlogloss:0.441771\teval-mlogloss:0.286492\n",
      "[29]\ttrain-mlogloss:0.433104\teval-mlogloss:0.278901\n",
      "[30]\ttrain-mlogloss:0.425916\teval-mlogloss:0.27201\n",
      "[31]\ttrain-mlogloss:0.419311\teval-mlogloss:0.26562\n",
      "[32]\ttrain-mlogloss:0.414246\teval-mlogloss:0.259958\n",
      "[33]\ttrain-mlogloss:0.407912\teval-mlogloss:0.255132\n",
      "[34]\ttrain-mlogloss:0.403963\teval-mlogloss:0.250246\n",
      "[35]\ttrain-mlogloss:0.400184\teval-mlogloss:0.245742\n",
      "[36]\ttrain-mlogloss:0.395429\teval-mlogloss:0.242064\n",
      "[37]\ttrain-mlogloss:0.392208\teval-mlogloss:0.238402\n",
      "[38]\ttrain-mlogloss:0.389698\teval-mlogloss:0.23517\n",
      "[39]\ttrain-mlogloss:0.387573\teval-mlogloss:0.232401\n",
      "[40]\ttrain-mlogloss:0.385082\teval-mlogloss:0.229355\n",
      "[41]\ttrain-mlogloss:0.38319\teval-mlogloss:0.227127\n",
      "[42]\ttrain-mlogloss:0.379942\teval-mlogloss:0.225044\n",
      "[43]\ttrain-mlogloss:0.378271\teval-mlogloss:0.222965\n",
      "[44]\ttrain-mlogloss:0.376474\teval-mlogloss:0.220729\n",
      "[45]\ttrain-mlogloss:0.375036\teval-mlogloss:0.219129\n",
      "[46]\ttrain-mlogloss:0.373934\teval-mlogloss:0.217618\n",
      "[47]\ttrain-mlogloss:0.372656\teval-mlogloss:0.216234\n",
      "[48]\ttrain-mlogloss:0.371642\teval-mlogloss:0.214986\n",
      "[49]\ttrain-mlogloss:0.369915\teval-mlogloss:0.213292\n",
      "[50]\ttrain-mlogloss:0.368999\teval-mlogloss:0.212186\n",
      "[51]\ttrain-mlogloss:0.367961\teval-mlogloss:0.211164\n",
      "[52]\ttrain-mlogloss:0.367212\teval-mlogloss:0.210257\n",
      "[53]\ttrain-mlogloss:0.366176\teval-mlogloss:0.209348\n",
      "[54]\ttrain-mlogloss:0.365449\teval-mlogloss:0.208606\n",
      "[55]\ttrain-mlogloss:0.364867\teval-mlogloss:0.207874\n",
      "[56]\ttrain-mlogloss:0.364117\teval-mlogloss:0.207153\n",
      "[57]\ttrain-mlogloss:0.363535\teval-mlogloss:0.206526\n",
      "[58]\ttrain-mlogloss:0.363031\teval-mlogloss:0.205938\n",
      "[59]\ttrain-mlogloss:0.362598\teval-mlogloss:0.205415\n",
      "[60]\ttrain-mlogloss:0.361941\teval-mlogloss:0.204833\n",
      "[61]\ttrain-mlogloss:0.361224\teval-mlogloss:0.204203\n",
      "[62]\ttrain-mlogloss:0.360531\teval-mlogloss:0.203465\n",
      "[63]\ttrain-mlogloss:0.360065\teval-mlogloss:0.20298\n",
      "[64]\ttrain-mlogloss:0.359667\teval-mlogloss:0.202558\n",
      "[65]\ttrain-mlogloss:0.359301\teval-mlogloss:0.202172\n",
      "[66]\ttrain-mlogloss:0.358948\teval-mlogloss:0.201813\n",
      "[67]\ttrain-mlogloss:0.358632\teval-mlogloss:0.20146\n",
      "[68]\ttrain-mlogloss:0.358303\teval-mlogloss:0.201126\n",
      "[69]\ttrain-mlogloss:0.357929\teval-mlogloss:0.200842\n",
      "[70]\ttrain-mlogloss:0.357584\teval-mlogloss:0.200557\n",
      "[71]\ttrain-mlogloss:0.357331\teval-mlogloss:0.200318\n",
      "[72]\ttrain-mlogloss:0.357089\teval-mlogloss:0.200085\n",
      "[73]\ttrain-mlogloss:0.356838\teval-mlogloss:0.199858\n",
      "[74]\ttrain-mlogloss:0.356596\teval-mlogloss:0.199639\n",
      "[75]\ttrain-mlogloss:0.356367\teval-mlogloss:0.199417\n",
      "[76]\ttrain-mlogloss:0.356166\teval-mlogloss:0.199214\n",
      "[77]\ttrain-mlogloss:0.355963\teval-mlogloss:0.19903\n",
      "[78]\ttrain-mlogloss:0.355762\teval-mlogloss:0.198857\n",
      "[79]\ttrain-mlogloss:0.35558\teval-mlogloss:0.198689\n",
      "[80]\ttrain-mlogloss:0.355417\teval-mlogloss:0.198548\n",
      "[81]\ttrain-mlogloss:0.355251\teval-mlogloss:0.198393\n",
      "[82]\ttrain-mlogloss:0.355082\teval-mlogloss:0.198251\n",
      "[83]\ttrain-mlogloss:0.354924\teval-mlogloss:0.198132\n",
      "[84]\ttrain-mlogloss:0.354779\teval-mlogloss:0.198014\n",
      "[85]\ttrain-mlogloss:0.354648\teval-mlogloss:0.197902\n",
      "[86]\ttrain-mlogloss:0.354514\teval-mlogloss:0.197788\n",
      "[87]\ttrain-mlogloss:0.354382\teval-mlogloss:0.197674\n",
      "[88]\ttrain-mlogloss:0.354259\teval-mlogloss:0.197565\n",
      "[89]\ttrain-mlogloss:0.354148\teval-mlogloss:0.197465\n",
      "[90]\ttrain-mlogloss:0.354032\teval-mlogloss:0.197369\n",
      "[91]\ttrain-mlogloss:0.353927\teval-mlogloss:0.19729\n",
      "[92]\ttrain-mlogloss:0.353816\teval-mlogloss:0.197194\n",
      "[93]\ttrain-mlogloss:0.353716\teval-mlogloss:0.197117\n",
      "[94]\ttrain-mlogloss:0.353606\teval-mlogloss:0.197036\n",
      "[95]\ttrain-mlogloss:0.353499\teval-mlogloss:0.196937\n",
      "[96]\ttrain-mlogloss:0.353402\teval-mlogloss:0.196863\n",
      "[97]\ttrain-mlogloss:0.353299\teval-mlogloss:0.196781\n",
      "[98]\ttrain-mlogloss:0.353207\teval-mlogloss:0.196709\n",
      "[99]\ttrain-mlogloss:0.35312\teval-mlogloss:0.196641\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameter for XGBoost model\n",
    "param_xgb = {\n",
    "    #'booster': 'gbtree',\n",
    "    'max_depth': 8,\n",
    "    'nthread': 4,\n",
    "    'num_class': 17,\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'eta': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'seed': 2018,\n",
    "    }\n",
    "\n",
    "# Convert train, validation data to fit in the XGBoost model\n",
    "dtrn = xgb.DMatrix(X_trn_vld, label=y_trn_vld, feature_names=features)\n",
    "dvld = xgb.DMatrix(X_eval_vld, label=y_eval_vld, feature_names=features)\n",
    "\n",
    "# Train the XGBoost model\n",
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "model_xgb = xgb.train(param_xgb, dtrn, num_boost_round=100, evals=watch_list, early_stopping_rounds=10)\n",
    "best_ntree_limit = model_xgb.best_ntree_limit\n",
    "\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model_xgb, open(\"../model/xgb.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03609679275470783\n"
     ]
    }
   ],
   "source": [
    "# Getting the prediction with the validation data\n",
    "X_tst_vld = tst_vld[features].values\n",
    "X_tst_vld = xgb.DMatrix(X_tst_vld, feature_names=features)\n",
    "preds_vld_xgb = model_xgb.predict(X_tst_vld, ntree_limit=best_ntree_limit)\n",
    "\n",
    "# Choose predictions only for top 16 classes\n",
    "preds_vld_xgb_16 = np.delete(preds_vld_xgb, 16, axis=1)\n",
    "\n",
    "# It is impossible to purchase products already did, so subtract 1 from the prediction\n",
    "preds_vld_xgb_16 = preds_vld_xgb_16 - tst_vld[[prod+'_prev' for prod in prods[target]]]\n",
    "\n",
    "# Get 7 products and calculate MAP@7 with the predictions (0.03609)\n",
    "result_xgb = predict_7_products(preds_vld_xgb_16.values)\n",
    "print(mapk(add_vld_list, result_xgb, 7, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 0.367338\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.345466\n",
      "[3]\tvalid_0's multi_logloss: 0.330157\n",
      "[4]\tvalid_0's multi_logloss: 0.316865\n",
      "[5]\tvalid_0's multi_logloss: 0.303929\n",
      "[6]\tvalid_0's multi_logloss: 0.293882\n",
      "[7]\tvalid_0's multi_logloss: 0.285786\n",
      "[8]\tvalid_0's multi_logloss: 0.277765\n",
      "[9]\tvalid_0's multi_logloss: 0.270836\n",
      "[10]\tvalid_0's multi_logloss: 0.264364\n",
      "[11]\tvalid_0's multi_logloss: 0.258532\n",
      "[12]\tvalid_0's multi_logloss: 0.253337\n",
      "[13]\tvalid_0's multi_logloss: 0.248596\n",
      "[14]\tvalid_0's multi_logloss: 0.244373\n",
      "[15]\tvalid_0's multi_logloss: 0.240536\n",
      "[16]\tvalid_0's multi_logloss: 0.237038\n",
      "[17]\tvalid_0's multi_logloss: 0.233878\n",
      "[18]\tvalid_0's multi_logloss: 0.230963\n",
      "[19]\tvalid_0's multi_logloss: 0.228342\n",
      "[20]\tvalid_0's multi_logloss: 0.22607\n",
      "[21]\tvalid_0's multi_logloss: 0.223855\n",
      "[22]\tvalid_0's multi_logloss: 0.221916\n",
      "[23]\tvalid_0's multi_logloss: 0.22005\n",
      "[24]\tvalid_0's multi_logloss: 0.218437\n",
      "[25]\tvalid_0's multi_logloss: 0.216943\n",
      "[26]\tvalid_0's multi_logloss: 0.215703\n",
      "[27]\tvalid_0's multi_logloss: 0.214354\n",
      "[28]\tvalid_0's multi_logloss: 0.213185\n",
      "[29]\tvalid_0's multi_logloss: 0.212087\n",
      "[30]\tvalid_0's multi_logloss: 0.211138\n",
      "[31]\tvalid_0's multi_logloss: 0.210171\n",
      "[32]\tvalid_0's multi_logloss: 0.209257\n",
      "[33]\tvalid_0's multi_logloss: 0.208468\n",
      "[34]\tvalid_0's multi_logloss: 0.207806\n",
      "[35]\tvalid_0's multi_logloss: 0.207085\n",
      "[36]\tvalid_0's multi_logloss: 0.206421\n",
      "[37]\tvalid_0's multi_logloss: 0.205809\n",
      "[38]\tvalid_0's multi_logloss: 0.205221\n",
      "[39]\tvalid_0's multi_logloss: 0.204715\n",
      "[40]\tvalid_0's multi_logloss: 0.204272\n",
      "[41]\tvalid_0's multi_logloss: 0.203806\n",
      "[42]\tvalid_0's multi_logloss: 0.203388\n",
      "[43]\tvalid_0's multi_logloss: 0.202963\n",
      "[44]\tvalid_0's multi_logloss: 0.202574\n",
      "[45]\tvalid_0's multi_logloss: 0.202267\n",
      "[46]\tvalid_0's multi_logloss: 0.20193\n",
      "[47]\tvalid_0's multi_logloss: 0.201608\n",
      "[48]\tvalid_0's multi_logloss: 0.201355\n",
      "[49]\tvalid_0's multi_logloss: 0.201125\n",
      "[50]\tvalid_0's multi_logloss: 0.200861\n",
      "[51]\tvalid_0's multi_logloss: 0.200598\n",
      "[52]\tvalid_0's multi_logloss: 0.200406\n",
      "[53]\tvalid_0's multi_logloss: 0.200176\n",
      "[54]\tvalid_0's multi_logloss: 0.199961\n",
      "[55]\tvalid_0's multi_logloss: 0.199746\n",
      "[56]\tvalid_0's multi_logloss: 0.199554\n",
      "[57]\tvalid_0's multi_logloss: 0.199379\n",
      "[58]\tvalid_0's multi_logloss: 0.199211\n",
      "[59]\tvalid_0's multi_logloss: 0.19904\n",
      "[60]\tvalid_0's multi_logloss: 0.198877\n",
      "[61]\tvalid_0's multi_logloss: 0.198743\n",
      "[62]\tvalid_0's multi_logloss: 0.198602\n",
      "[63]\tvalid_0's multi_logloss: 0.198465\n",
      "[64]\tvalid_0's multi_logloss: 0.198322\n",
      "[65]\tvalid_0's multi_logloss: 0.1982\n",
      "[66]\tvalid_0's multi_logloss: 0.198081\n",
      "[67]\tvalid_0's multi_logloss: 0.197967\n",
      "[68]\tvalid_0's multi_logloss: 0.19785\n",
      "[69]\tvalid_0's multi_logloss: 0.197747\n",
      "[70]\tvalid_0's multi_logloss: 0.197647\n",
      "[71]\tvalid_0's multi_logloss: 0.197561\n",
      "[72]\tvalid_0's multi_logloss: 0.197477\n",
      "[73]\tvalid_0's multi_logloss: 0.19741\n",
      "[74]\tvalid_0's multi_logloss: 0.197322\n",
      "[75]\tvalid_0's multi_logloss: 0.197264\n",
      "[76]\tvalid_0's multi_logloss: 0.197203\n",
      "[77]\tvalid_0's multi_logloss: 0.197127\n",
      "[78]\tvalid_0's multi_logloss: 0.197054\n",
      "[79]\tvalid_0's multi_logloss: 0.19699\n",
      "[80]\tvalid_0's multi_logloss: 0.196929\n",
      "[81]\tvalid_0's multi_logloss: 0.196875\n",
      "[82]\tvalid_0's multi_logloss: 0.19681\n",
      "[83]\tvalid_0's multi_logloss: 0.196753\n",
      "[84]\tvalid_0's multi_logloss: 0.19669\n",
      "[85]\tvalid_0's multi_logloss: 0.196631\n",
      "[86]\tvalid_0's multi_logloss: 0.196593\n",
      "[87]\tvalid_0's multi_logloss: 0.196535\n",
      "[88]\tvalid_0's multi_logloss: 0.196485\n",
      "[89]\tvalid_0's multi_logloss: 0.196442\n",
      "[90]\tvalid_0's multi_logloss: 0.196397\n",
      "[91]\tvalid_0's multi_logloss: 0.19635\n",
      "[92]\tvalid_0's multi_logloss: 0.196306\n",
      "[93]\tvalid_0's multi_logloss: 0.196263\n",
      "[94]\tvalid_0's multi_logloss: 0.196216\n",
      "[95]\tvalid_0's multi_logloss: 0.196168\n",
      "[96]\tvalid_0's multi_logloss: 0.196132\n",
      "[97]\tvalid_0's multi_logloss: 0.196103\n",
      "[98]\tvalid_0's multi_logloss: 0.196062\n",
      "[99]\tvalid_0's multi_logloss: 0.196024\n",
      "[100]\tvalid_0's multi_logloss: 0.19599\n",
      "[101]\tvalid_0's multi_logloss: 0.195957\n",
      "[102]\tvalid_0's multi_logloss: 0.195922\n",
      "[103]\tvalid_0's multi_logloss: 0.195891\n",
      "[104]\tvalid_0's multi_logloss: 0.195862\n",
      "[105]\tvalid_0's multi_logloss: 0.195831\n",
      "[106]\tvalid_0's multi_logloss: 0.195795\n",
      "[107]\tvalid_0's multi_logloss: 0.195758\n",
      "[108]\tvalid_0's multi_logloss: 0.195725\n",
      "[109]\tvalid_0's multi_logloss: 0.195694\n",
      "[110]\tvalid_0's multi_logloss: 0.195661\n",
      "[111]\tvalid_0's multi_logloss: 0.195622\n",
      "[112]\tvalid_0's multi_logloss: 0.195594\n",
      "[113]\tvalid_0's multi_logloss: 0.195562\n",
      "[114]\tvalid_0's multi_logloss: 0.195532\n",
      "[115]\tvalid_0's multi_logloss: 0.195506\n",
      "[116]\tvalid_0's multi_logloss: 0.19548\n",
      "[117]\tvalid_0's multi_logloss: 0.19546\n",
      "[118]\tvalid_0's multi_logloss: 0.195438\n",
      "[119]\tvalid_0's multi_logloss: 0.195417\n",
      "[120]\tvalid_0's multi_logloss: 0.195392\n",
      "[121]\tvalid_0's multi_logloss: 0.195368\n",
      "[122]\tvalid_0's multi_logloss: 0.195344\n",
      "[123]\tvalid_0's multi_logloss: 0.19532\n",
      "[124]\tvalid_0's multi_logloss: 0.1953\n",
      "[125]\tvalid_0's multi_logloss: 0.195278\n",
      "[126]\tvalid_0's multi_logloss: 0.195255\n",
      "[127]\tvalid_0's multi_logloss: 0.195237\n",
      "[128]\tvalid_0's multi_logloss: 0.195217\n",
      "[129]\tvalid_0's multi_logloss: 0.195198\n",
      "[130]\tvalid_0's multi_logloss: 0.195177\n",
      "[131]\tvalid_0's multi_logloss: 0.19516\n",
      "[132]\tvalid_0's multi_logloss: 0.19514\n",
      "[133]\tvalid_0's multi_logloss: 0.195119\n",
      "[134]\tvalid_0's multi_logloss: 0.195099\n",
      "[135]\tvalid_0's multi_logloss: 0.195087\n",
      "[136]\tvalid_0's multi_logloss: 0.195065\n",
      "[137]\tvalid_0's multi_logloss: 0.195042\n",
      "[138]\tvalid_0's multi_logloss: 0.195028\n",
      "[139]\tvalid_0's multi_logloss: 0.195008\n",
      "[140]\tvalid_0's multi_logloss: 0.194991\n",
      "[141]\tvalid_0's multi_logloss: 0.19497\n",
      "[142]\tvalid_0's multi_logloss: 0.194947\n",
      "[143]\tvalid_0's multi_logloss: 0.194933\n",
      "[144]\tvalid_0's multi_logloss: 0.194916\n",
      "[145]\tvalid_0's multi_logloss: 0.194898\n",
      "[146]\tvalid_0's multi_logloss: 0.194874\n",
      "[147]\tvalid_0's multi_logloss: 0.194858\n",
      "[148]\tvalid_0's multi_logloss: 0.194839\n",
      "[149]\tvalid_0's multi_logloss: 0.194816\n",
      "[150]\tvalid_0's multi_logloss: 0.194803\n",
      "[151]\tvalid_0's multi_logloss: 0.194789\n",
      "[152]\tvalid_0's multi_logloss: 0.194776\n",
      "[153]\tvalid_0's multi_logloss: 0.194759\n",
      "[154]\tvalid_0's multi_logloss: 0.194737\n",
      "[155]\tvalid_0's multi_logloss: 0.194714\n",
      "[156]\tvalid_0's multi_logloss: 0.1947\n",
      "[157]\tvalid_0's multi_logloss: 0.194685\n",
      "[158]\tvalid_0's multi_logloss: 0.194669\n",
      "[159]\tvalid_0's multi_logloss: 0.19466\n",
      "[160]\tvalid_0's multi_logloss: 0.194645\n",
      "[161]\tvalid_0's multi_logloss: 0.194631\n",
      "[162]\tvalid_0's multi_logloss: 0.194615\n",
      "[163]\tvalid_0's multi_logloss: 0.194601\n",
      "[164]\tvalid_0's multi_logloss: 0.194588\n",
      "[165]\tvalid_0's multi_logloss: 0.194575\n",
      "[166]\tvalid_0's multi_logloss: 0.194558\n",
      "[167]\tvalid_0's multi_logloss: 0.194536\n",
      "[168]\tvalid_0's multi_logloss: 0.194523\n",
      "[169]\tvalid_0's multi_logloss: 0.194508\n",
      "[170]\tvalid_0's multi_logloss: 0.194497\n",
      "[171]\tvalid_0's multi_logloss: 0.194486\n",
      "[172]\tvalid_0's multi_logloss: 0.194472\n",
      "[173]\tvalid_0's multi_logloss: 0.194459\n",
      "[174]\tvalid_0's multi_logloss: 0.194449\n",
      "[175]\tvalid_0's multi_logloss: 0.194435\n",
      "[176]\tvalid_0's multi_logloss: 0.194424\n",
      "[177]\tvalid_0's multi_logloss: 0.19441\n",
      "[178]\tvalid_0's multi_logloss: 0.194396\n",
      "[179]\tvalid_0's multi_logloss: 0.194383\n",
      "[180]\tvalid_0's multi_logloss: 0.194369\n",
      "[181]\tvalid_0's multi_logloss: 0.194358\n",
      "[182]\tvalid_0's multi_logloss: 0.19434\n",
      "[183]\tvalid_0's multi_logloss: 0.194328\n",
      "[184]\tvalid_0's multi_logloss: 0.194316\n",
      "[185]\tvalid_0's multi_logloss: 0.194309\n",
      "[186]\tvalid_0's multi_logloss: 0.194291\n",
      "[187]\tvalid_0's multi_logloss: 0.194281\n",
      "[188]\tvalid_0's multi_logloss: 0.194269\n",
      "[189]\tvalid_0's multi_logloss: 0.19426\n",
      "[190]\tvalid_0's multi_logloss: 0.194246\n",
      "[191]\tvalid_0's multi_logloss: 0.194237\n",
      "[192]\tvalid_0's multi_logloss: 0.194226\n",
      "[193]\tvalid_0's multi_logloss: 0.194214\n",
      "[194]\tvalid_0's multi_logloss: 0.194203\n",
      "[195]\tvalid_0's multi_logloss: 0.19419\n",
      "[196]\tvalid_0's multi_logloss: 0.194177\n",
      "[197]\tvalid_0's multi_logloss: 0.194165\n",
      "[198]\tvalid_0's multi_logloss: 0.194155\n",
      "[199]\tvalid_0's multi_logloss: 0.194145\n",
      "[200]\tvalid_0's multi_logloss: 0.194122\n",
      "[201]\tvalid_0's multi_logloss: 0.19411\n",
      "[202]\tvalid_0's multi_logloss: 0.194091\n",
      "[203]\tvalid_0's multi_logloss: 0.194079\n",
      "[204]\tvalid_0's multi_logloss: 0.194069\n",
      "[205]\tvalid_0's multi_logloss: 0.194059\n",
      "[206]\tvalid_0's multi_logloss: 0.194051\n",
      "[207]\tvalid_0's multi_logloss: 0.194039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208]\tvalid_0's multi_logloss: 0.194018\n",
      "[209]\tvalid_0's multi_logloss: 0.194009\n",
      "[210]\tvalid_0's multi_logloss: 0.193993\n",
      "[211]\tvalid_0's multi_logloss: 0.193983\n",
      "[212]\tvalid_0's multi_logloss: 0.193968\n",
      "[213]\tvalid_0's multi_logloss: 0.193955\n",
      "[214]\tvalid_0's multi_logloss: 0.193947\n",
      "[215]\tvalid_0's multi_logloss: 0.193938\n",
      "[216]\tvalid_0's multi_logloss: 0.193927\n",
      "[217]\tvalid_0's multi_logloss: 0.193915\n",
      "[218]\tvalid_0's multi_logloss: 0.193901\n",
      "[219]\tvalid_0's multi_logloss: 0.193892\n",
      "[220]\tvalid_0's multi_logloss: 0.193878\n",
      "[221]\tvalid_0's multi_logloss: 0.193866\n",
      "[222]\tvalid_0's multi_logloss: 0.193855\n",
      "[223]\tvalid_0's multi_logloss: 0.193841\n",
      "[224]\tvalid_0's multi_logloss: 0.193835\n",
      "[225]\tvalid_0's multi_logloss: 0.193826\n",
      "[226]\tvalid_0's multi_logloss: 0.193816\n",
      "[227]\tvalid_0's multi_logloss: 0.193806\n",
      "[228]\tvalid_0's multi_logloss: 0.193794\n",
      "[229]\tvalid_0's multi_logloss: 0.193785\n",
      "[230]\tvalid_0's multi_logloss: 0.193776\n",
      "[231]\tvalid_0's multi_logloss: 0.193765\n",
      "[232]\tvalid_0's multi_logloss: 0.193752\n",
      "[233]\tvalid_0's multi_logloss: 0.193745\n",
      "[234]\tvalid_0's multi_logloss: 0.193734\n",
      "[235]\tvalid_0's multi_logloss: 0.193727\n",
      "[236]\tvalid_0's multi_logloss: 0.193717\n",
      "[237]\tvalid_0's multi_logloss: 0.193709\n",
      "[238]\tvalid_0's multi_logloss: 0.193696\n",
      "[239]\tvalid_0's multi_logloss: 0.193686\n",
      "[240]\tvalid_0's multi_logloss: 0.193678\n",
      "[241]\tvalid_0's multi_logloss: 0.193668\n",
      "[242]\tvalid_0's multi_logloss: 0.193653\n",
      "[243]\tvalid_0's multi_logloss: 0.19364\n",
      "[244]\tvalid_0's multi_logloss: 0.193631\n",
      "[245]\tvalid_0's multi_logloss: 0.193623\n",
      "[246]\tvalid_0's multi_logloss: 0.193608\n",
      "[247]\tvalid_0's multi_logloss: 0.193603\n",
      "[248]\tvalid_0's multi_logloss: 0.193592\n",
      "[249]\tvalid_0's multi_logloss: 0.193583\n",
      "[250]\tvalid_0's multi_logloss: 0.193574\n",
      "[251]\tvalid_0's multi_logloss: 0.193563\n",
      "[252]\tvalid_0's multi_logloss: 0.193554\n",
      "[253]\tvalid_0's multi_logloss: 0.193541\n",
      "[254]\tvalid_0's multi_logloss: 0.193531\n",
      "[255]\tvalid_0's multi_logloss: 0.193522\n",
      "[256]\tvalid_0's multi_logloss: 0.193508\n",
      "[257]\tvalid_0's multi_logloss: 0.1935\n",
      "[258]\tvalid_0's multi_logloss: 0.193489\n",
      "[259]\tvalid_0's multi_logloss: 0.193483\n",
      "[260]\tvalid_0's multi_logloss: 0.193473\n",
      "[261]\tvalid_0's multi_logloss: 0.193464\n",
      "[262]\tvalid_0's multi_logloss: 0.193452\n",
      "[263]\tvalid_0's multi_logloss: 0.193443\n",
      "[264]\tvalid_0's multi_logloss: 0.193439\n",
      "[265]\tvalid_0's multi_logloss: 0.193424\n",
      "[266]\tvalid_0's multi_logloss: 0.193412\n",
      "[267]\tvalid_0's multi_logloss: 0.193403\n",
      "[268]\tvalid_0's multi_logloss: 0.193396\n",
      "[269]\tvalid_0's multi_logloss: 0.193386\n",
      "[270]\tvalid_0's multi_logloss: 0.193372\n",
      "[271]\tvalid_0's multi_logloss: 0.193363\n",
      "[272]\tvalid_0's multi_logloss: 0.193351\n",
      "[273]\tvalid_0's multi_logloss: 0.193342\n",
      "[274]\tvalid_0's multi_logloss: 0.193329\n",
      "[275]\tvalid_0's multi_logloss: 0.193316\n",
      "[276]\tvalid_0's multi_logloss: 0.19331\n",
      "[277]\tvalid_0's multi_logloss: 0.193305\n",
      "[278]\tvalid_0's multi_logloss: 0.193297\n",
      "[279]\tvalid_0's multi_logloss: 0.193287\n",
      "[280]\tvalid_0's multi_logloss: 0.193274\n",
      "[281]\tvalid_0's multi_logloss: 0.193265\n",
      "[282]\tvalid_0's multi_logloss: 0.193263\n",
      "[283]\tvalid_0's multi_logloss: 0.193248\n",
      "[284]\tvalid_0's multi_logloss: 0.193237\n",
      "[285]\tvalid_0's multi_logloss: 0.193231\n",
      "[286]\tvalid_0's multi_logloss: 0.19322\n",
      "[287]\tvalid_0's multi_logloss: 0.193213\n",
      "[288]\tvalid_0's multi_logloss: 0.193193\n",
      "[289]\tvalid_0's multi_logloss: 0.193186\n",
      "[290]\tvalid_0's multi_logloss: 0.193181\n",
      "[291]\tvalid_0's multi_logloss: 0.193171\n",
      "[292]\tvalid_0's multi_logloss: 0.193163\n",
      "[293]\tvalid_0's multi_logloss: 0.193157\n",
      "[294]\tvalid_0's multi_logloss: 0.193151\n",
      "[295]\tvalid_0's multi_logloss: 0.193142\n",
      "[296]\tvalid_0's multi_logloss: 0.19313\n",
      "[297]\tvalid_0's multi_logloss: 0.193119\n",
      "[298]\tvalid_0's multi_logloss: 0.193109\n",
      "[299]\tvalid_0's multi_logloss: 0.193098\n",
      "[300]\tvalid_0's multi_logloss: 0.193092\n",
      "[301]\tvalid_0's multi_logloss: 0.193082\n",
      "[302]\tvalid_0's multi_logloss: 0.193072\n",
      "[303]\tvalid_0's multi_logloss: 0.193065\n",
      "[304]\tvalid_0's multi_logloss: 0.193054\n",
      "[305]\tvalid_0's multi_logloss: 0.193044\n",
      "[306]\tvalid_0's multi_logloss: 0.193039\n",
      "[307]\tvalid_0's multi_logloss: 0.193033\n",
      "[308]\tvalid_0's multi_logloss: 0.193023\n",
      "[309]\tvalid_0's multi_logloss: 0.193014\n",
      "[310]\tvalid_0's multi_logloss: 0.193008\n",
      "[311]\tvalid_0's multi_logloss: 0.192997\n",
      "[312]\tvalid_0's multi_logloss: 0.192988\n",
      "[313]\tvalid_0's multi_logloss: 0.192977\n",
      "[314]\tvalid_0's multi_logloss: 0.192969\n",
      "[315]\tvalid_0's multi_logloss: 0.19296\n",
      "[316]\tvalid_0's multi_logloss: 0.192951\n",
      "[317]\tvalid_0's multi_logloss: 0.192939\n",
      "[318]\tvalid_0's multi_logloss: 0.192929\n",
      "[319]\tvalid_0's multi_logloss: 0.192924\n",
      "[320]\tvalid_0's multi_logloss: 0.192918\n",
      "[321]\tvalid_0's multi_logloss: 0.192908\n",
      "[322]\tvalid_0's multi_logloss: 0.1929\n",
      "[323]\tvalid_0's multi_logloss: 0.192894\n",
      "[324]\tvalid_0's multi_logloss: 0.192891\n",
      "[325]\tvalid_0's multi_logloss: 0.192885\n",
      "[326]\tvalid_0's multi_logloss: 0.192872\n",
      "[327]\tvalid_0's multi_logloss: 0.192864\n",
      "[328]\tvalid_0's multi_logloss: 0.192851\n",
      "[329]\tvalid_0's multi_logloss: 0.192843\n",
      "[330]\tvalid_0's multi_logloss: 0.192833\n",
      "[331]\tvalid_0's multi_logloss: 0.192822\n",
      "[332]\tvalid_0's multi_logloss: 0.192817\n",
      "[333]\tvalid_0's multi_logloss: 0.192807\n",
      "[334]\tvalid_0's multi_logloss: 0.192793\n",
      "[335]\tvalid_0's multi_logloss: 0.192783\n",
      "[336]\tvalid_0's multi_logloss: 0.192773\n",
      "[337]\tvalid_0's multi_logloss: 0.192761\n",
      "[338]\tvalid_0's multi_logloss: 0.192754\n",
      "[339]\tvalid_0's multi_logloss: 0.192753\n",
      "[340]\tvalid_0's multi_logloss: 0.192751\n",
      "[341]\tvalid_0's multi_logloss: 0.192746\n",
      "[342]\tvalid_0's multi_logloss: 0.192736\n",
      "[343]\tvalid_0's multi_logloss: 0.192737\n",
      "[344]\tvalid_0's multi_logloss: 0.19272\n",
      "[345]\tvalid_0's multi_logloss: 0.192708\n",
      "[346]\tvalid_0's multi_logloss: 0.1927\n",
      "[347]\tvalid_0's multi_logloss: 0.192694\n",
      "[348]\tvalid_0's multi_logloss: 0.192688\n",
      "[349]\tvalid_0's multi_logloss: 0.192686\n",
      "[350]\tvalid_0's multi_logloss: 0.192678\n",
      "[351]\tvalid_0's multi_logloss: 0.192666\n",
      "[352]\tvalid_0's multi_logloss: 0.192658\n",
      "[353]\tvalid_0's multi_logloss: 0.192654\n",
      "[354]\tvalid_0's multi_logloss: 0.192647\n",
      "[355]\tvalid_0's multi_logloss: 0.192639\n",
      "[356]\tvalid_0's multi_logloss: 0.192635\n",
      "[357]\tvalid_0's multi_logloss: 0.192625\n",
      "[358]\tvalid_0's multi_logloss: 0.192619\n",
      "[359]\tvalid_0's multi_logloss: 0.192612\n",
      "[360]\tvalid_0's multi_logloss: 0.1926\n",
      "[361]\tvalid_0's multi_logloss: 0.192596\n",
      "[362]\tvalid_0's multi_logloss: 0.192591\n",
      "[363]\tvalid_0's multi_logloss: 0.192581\n",
      "[364]\tvalid_0's multi_logloss: 0.192569\n",
      "[365]\tvalid_0's multi_logloss: 0.192561\n",
      "[366]\tvalid_0's multi_logloss: 0.192551\n",
      "[367]\tvalid_0's multi_logloss: 0.192546\n",
      "[368]\tvalid_0's multi_logloss: 0.192541\n",
      "[369]\tvalid_0's multi_logloss: 0.192534\n",
      "[370]\tvalid_0's multi_logloss: 0.192525\n",
      "[371]\tvalid_0's multi_logloss: 0.192519\n",
      "[372]\tvalid_0's multi_logloss: 0.19251\n",
      "[373]\tvalid_0's multi_logloss: 0.192501\n",
      "[374]\tvalid_0's multi_logloss: 0.192492\n",
      "[375]\tvalid_0's multi_logloss: 0.192481\n",
      "[376]\tvalid_0's multi_logloss: 0.19247\n",
      "[377]\tvalid_0's multi_logloss: 0.192465\n",
      "[378]\tvalid_0's multi_logloss: 0.192458\n",
      "[379]\tvalid_0's multi_logloss: 0.192448\n",
      "[380]\tvalid_0's multi_logloss: 0.192442\n",
      "[381]\tvalid_0's multi_logloss: 0.192435\n",
      "[382]\tvalid_0's multi_logloss: 0.192421\n",
      "[383]\tvalid_0's multi_logloss: 0.192413\n",
      "[384]\tvalid_0's multi_logloss: 0.192406\n",
      "[385]\tvalid_0's multi_logloss: 0.192397\n",
      "[386]\tvalid_0's multi_logloss: 0.192389\n",
      "[387]\tvalid_0's multi_logloss: 0.192383\n",
      "[388]\tvalid_0's multi_logloss: 0.192375\n",
      "[389]\tvalid_0's multi_logloss: 0.192366\n",
      "[390]\tvalid_0's multi_logloss: 0.192363\n",
      "[391]\tvalid_0's multi_logloss: 0.192355\n",
      "[392]\tvalid_0's multi_logloss: 0.192343\n",
      "[393]\tvalid_0's multi_logloss: 0.192334\n",
      "[394]\tvalid_0's multi_logloss: 0.192325\n",
      "[395]\tvalid_0's multi_logloss: 0.192315\n",
      "[396]\tvalid_0's multi_logloss: 0.192307\n",
      "[397]\tvalid_0's multi_logloss: 0.192303\n",
      "[398]\tvalid_0's multi_logloss: 0.192301\n",
      "[399]\tvalid_0's multi_logloss: 0.19229\n",
      "[400]\tvalid_0's multi_logloss: 0.192284\n",
      "[401]\tvalid_0's multi_logloss: 0.192283\n",
      "[402]\tvalid_0's multi_logloss: 0.192279\n",
      "[403]\tvalid_0's multi_logloss: 0.192286\n",
      "[404]\tvalid_0's multi_logloss: 0.192279\n",
      "[405]\tvalid_0's multi_logloss: 0.192272\n",
      "[406]\tvalid_0's multi_logloss: 0.192264\n",
      "[407]\tvalid_0's multi_logloss: 0.192257\n",
      "[408]\tvalid_0's multi_logloss: 0.192248\n",
      "[409]\tvalid_0's multi_logloss: 0.19224\n",
      "[410]\tvalid_0's multi_logloss: 0.192237\n",
      "[411]\tvalid_0's multi_logloss: 0.19223\n",
      "[412]\tvalid_0's multi_logloss: 0.192224\n",
      "[413]\tvalid_0's multi_logloss: 0.192216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[414]\tvalid_0's multi_logloss: 0.192209\n",
      "[415]\tvalid_0's multi_logloss: 0.192202\n",
      "[416]\tvalid_0's multi_logloss: 0.192192\n",
      "[417]\tvalid_0's multi_logloss: 0.192183\n",
      "[418]\tvalid_0's multi_logloss: 0.192177\n",
      "[419]\tvalid_0's multi_logloss: 0.192168\n",
      "[420]\tvalid_0's multi_logloss: 0.192161\n",
      "[421]\tvalid_0's multi_logloss: 0.19215\n",
      "[422]\tvalid_0's multi_logloss: 0.192144\n",
      "[423]\tvalid_0's multi_logloss: 0.192135\n",
      "[424]\tvalid_0's multi_logloss: 0.192127\n",
      "[425]\tvalid_0's multi_logloss: 0.192122\n",
      "[426]\tvalid_0's multi_logloss: 0.192109\n",
      "[427]\tvalid_0's multi_logloss: 0.192101\n",
      "[428]\tvalid_0's multi_logloss: 0.192094\n",
      "[429]\tvalid_0's multi_logloss: 0.19209\n",
      "[430]\tvalid_0's multi_logloss: 0.192082\n",
      "[431]\tvalid_0's multi_logloss: 0.192073\n",
      "[432]\tvalid_0's multi_logloss: 0.192067\n",
      "[433]\tvalid_0's multi_logloss: 0.192061\n",
      "[434]\tvalid_0's multi_logloss: 0.192053\n",
      "[435]\tvalid_0's multi_logloss: 0.192034\n",
      "[436]\tvalid_0's multi_logloss: 0.192026\n",
      "[437]\tvalid_0's multi_logloss: 0.192019\n",
      "[438]\tvalid_0's multi_logloss: 0.192012\n",
      "[439]\tvalid_0's multi_logloss: 0.192006\n",
      "[440]\tvalid_0's multi_logloss: 0.191996\n",
      "[441]\tvalid_0's multi_logloss: 0.191993\n",
      "[442]\tvalid_0's multi_logloss: 0.191986\n",
      "[443]\tvalid_0's multi_logloss: 0.191978\n",
      "[444]\tvalid_0's multi_logloss: 0.19197\n",
      "[445]\tvalid_0's multi_logloss: 0.191965\n",
      "[446]\tvalid_0's multi_logloss: 0.191957\n",
      "[447]\tvalid_0's multi_logloss: 0.191955\n",
      "[448]\tvalid_0's multi_logloss: 0.191951\n",
      "[449]\tvalid_0's multi_logloss: 0.191943\n",
      "[450]\tvalid_0's multi_logloss: 0.191938\n",
      "[451]\tvalid_0's multi_logloss: 0.191932\n",
      "[452]\tvalid_0's multi_logloss: 0.191926\n",
      "[453]\tvalid_0's multi_logloss: 0.191917\n",
      "[454]\tvalid_0's multi_logloss: 0.191907\n",
      "[455]\tvalid_0's multi_logloss: 0.191901\n",
      "[456]\tvalid_0's multi_logloss: 0.19189\n",
      "[457]\tvalid_0's multi_logloss: 0.191893\n",
      "[458]\tvalid_0's multi_logloss: 0.191888\n",
      "[459]\tvalid_0's multi_logloss: 0.191884\n",
      "[460]\tvalid_0's multi_logloss: 0.191877\n",
      "[461]\tvalid_0's multi_logloss: 0.191872\n",
      "[462]\tvalid_0's multi_logloss: 0.191862\n",
      "[463]\tvalid_0's multi_logloss: 0.191856\n",
      "[464]\tvalid_0's multi_logloss: 0.19185\n",
      "[465]\tvalid_0's multi_logloss: 0.191844\n",
      "[466]\tvalid_0's multi_logloss: 0.191838\n",
      "[467]\tvalid_0's multi_logloss: 0.191827\n",
      "[468]\tvalid_0's multi_logloss: 0.191818\n",
      "[469]\tvalid_0's multi_logloss: 0.191811\n",
      "[470]\tvalid_0's multi_logloss: 0.191806\n",
      "[471]\tvalid_0's multi_logloss: 0.191796\n",
      "[472]\tvalid_0's multi_logloss: 0.191788\n",
      "[473]\tvalid_0's multi_logloss: 0.191784\n",
      "[474]\tvalid_0's multi_logloss: 0.19178\n",
      "[475]\tvalid_0's multi_logloss: 0.191774\n",
      "[476]\tvalid_0's multi_logloss: 0.191767\n",
      "[477]\tvalid_0's multi_logloss: 0.191757\n",
      "[478]\tvalid_0's multi_logloss: 0.191762\n",
      "[479]\tvalid_0's multi_logloss: 0.191757\n",
      "[480]\tvalid_0's multi_logloss: 0.191748\n",
      "[481]\tvalid_0's multi_logloss: 0.191743\n",
      "[482]\tvalid_0's multi_logloss: 0.19174\n",
      "[483]\tvalid_0's multi_logloss: 0.191732\n",
      "[484]\tvalid_0's multi_logloss: 0.191727\n",
      "[485]\tvalid_0's multi_logloss: 0.191721\n",
      "[486]\tvalid_0's multi_logloss: 0.191714\n",
      "[487]\tvalid_0's multi_logloss: 0.191709\n",
      "[488]\tvalid_0's multi_logloss: 0.191707\n",
      "[489]\tvalid_0's multi_logloss: 0.191708\n",
      "[490]\tvalid_0's multi_logloss: 0.191702\n",
      "[491]\tvalid_0's multi_logloss: 0.191689\n",
      "[492]\tvalid_0's multi_logloss: 0.19168\n",
      "[493]\tvalid_0's multi_logloss: 0.191672\n",
      "[494]\tvalid_0's multi_logloss: 0.191663\n",
      "[495]\tvalid_0's multi_logloss: 0.191659\n",
      "[496]\tvalid_0's multi_logloss: 0.191655\n",
      "[497]\tvalid_0's multi_logloss: 0.191648\n",
      "[498]\tvalid_0's multi_logloss: 0.191641\n",
      "[499]\tvalid_0's multi_logloss: 0.191634\n",
      "[500]\tvalid_0's multi_logloss: 0.191628\n",
      "[501]\tvalid_0's multi_logloss: 0.191619\n",
      "[502]\tvalid_0's multi_logloss: 0.191614\n",
      "[503]\tvalid_0's multi_logloss: 0.19161\n",
      "[504]\tvalid_0's multi_logloss: 0.191602\n",
      "[505]\tvalid_0's multi_logloss: 0.191592\n",
      "[506]\tvalid_0's multi_logloss: 0.191578\n",
      "[507]\tvalid_0's multi_logloss: 0.191573\n",
      "[508]\tvalid_0's multi_logloss: 0.191564\n",
      "[509]\tvalid_0's multi_logloss: 0.191545\n",
      "[510]\tvalid_0's multi_logloss: 0.191539\n",
      "[511]\tvalid_0's multi_logloss: 0.191532\n",
      "[512]\tvalid_0's multi_logloss: 0.191525\n",
      "[513]\tvalid_0's multi_logloss: 0.191519\n",
      "[514]\tvalid_0's multi_logloss: 0.191513\n",
      "[515]\tvalid_0's multi_logloss: 0.191509\n",
      "[516]\tvalid_0's multi_logloss: 0.191502\n",
      "[517]\tvalid_0's multi_logloss: 0.191494\n",
      "[518]\tvalid_0's multi_logloss: 0.191486\n",
      "[519]\tvalid_0's multi_logloss: 0.191484\n",
      "[520]\tvalid_0's multi_logloss: 0.19148\n",
      "[521]\tvalid_0's multi_logloss: 0.191471\n",
      "[522]\tvalid_0's multi_logloss: 0.191466\n",
      "[523]\tvalid_0's multi_logloss: 0.191464\n",
      "[524]\tvalid_0's multi_logloss: 0.191461\n",
      "[525]\tvalid_0's multi_logloss: 0.191454\n",
      "[526]\tvalid_0's multi_logloss: 0.191445\n",
      "[527]\tvalid_0's multi_logloss: 0.191449\n",
      "[528]\tvalid_0's multi_logloss: 0.191443\n",
      "[529]\tvalid_0's multi_logloss: 0.191437\n",
      "[530]\tvalid_0's multi_logloss: 0.191432\n",
      "[531]\tvalid_0's multi_logloss: 0.191427\n",
      "[532]\tvalid_0's multi_logloss: 0.191423\n",
      "[533]\tvalid_0's multi_logloss: 0.191415\n",
      "[534]\tvalid_0's multi_logloss: 0.191407\n",
      "[535]\tvalid_0's multi_logloss: 0.191403\n",
      "[536]\tvalid_0's multi_logloss: 0.191397\n",
      "[537]\tvalid_0's multi_logloss: 0.191388\n",
      "[538]\tvalid_0's multi_logloss: 0.191386\n",
      "[539]\tvalid_0's multi_logloss: 0.191379\n",
      "[540]\tvalid_0's multi_logloss: 0.191372\n",
      "[541]\tvalid_0's multi_logloss: 0.191367\n",
      "[542]\tvalid_0's multi_logloss: 0.191361\n",
      "[543]\tvalid_0's multi_logloss: 0.191357\n",
      "[544]\tvalid_0's multi_logloss: 0.19135\n",
      "[545]\tvalid_0's multi_logloss: 0.191348\n",
      "[546]\tvalid_0's multi_logloss: 0.191342\n",
      "[547]\tvalid_0's multi_logloss: 0.191338\n",
      "[548]\tvalid_0's multi_logloss: 0.191334\n",
      "[549]\tvalid_0's multi_logloss: 0.191324\n",
      "[550]\tvalid_0's multi_logloss: 0.191317\n",
      "[551]\tvalid_0's multi_logloss: 0.191311\n",
      "[552]\tvalid_0's multi_logloss: 0.191302\n",
      "[553]\tvalid_0's multi_logloss: 0.191299\n",
      "[554]\tvalid_0's multi_logloss: 0.191288\n",
      "[555]\tvalid_0's multi_logloss: 0.191285\n",
      "[556]\tvalid_0's multi_logloss: 0.191271\n",
      "[557]\tvalid_0's multi_logloss: 0.191264\n",
      "[558]\tvalid_0's multi_logloss: 0.191262\n",
      "[559]\tvalid_0's multi_logloss: 0.191256\n",
      "[560]\tvalid_0's multi_logloss: 0.19125\n",
      "[561]\tvalid_0's multi_logloss: 0.191246\n",
      "[562]\tvalid_0's multi_logloss: 0.191239\n",
      "[563]\tvalid_0's multi_logloss: 0.191235\n",
      "[564]\tvalid_0's multi_logloss: 0.191229\n",
      "[565]\tvalid_0's multi_logloss: 0.191221\n",
      "[566]\tvalid_0's multi_logloss: 0.191214\n",
      "[567]\tvalid_0's multi_logloss: 0.19121\n",
      "[568]\tvalid_0's multi_logloss: 0.1912\n",
      "[569]\tvalid_0's multi_logloss: 0.191194\n",
      "[570]\tvalid_0's multi_logloss: 0.191187\n",
      "[571]\tvalid_0's multi_logloss: 0.191183\n",
      "[572]\tvalid_0's multi_logloss: 0.191175\n",
      "[573]\tvalid_0's multi_logloss: 0.19117\n",
      "[574]\tvalid_0's multi_logloss: 0.191164\n",
      "[575]\tvalid_0's multi_logloss: 0.191157\n",
      "[576]\tvalid_0's multi_logloss: 0.19115\n",
      "[577]\tvalid_0's multi_logloss: 0.191142\n",
      "[578]\tvalid_0's multi_logloss: 0.191139\n",
      "[579]\tvalid_0's multi_logloss: 0.191135\n",
      "[580]\tvalid_0's multi_logloss: 0.19113\n",
      "[581]\tvalid_0's multi_logloss: 0.191121\n",
      "[582]\tvalid_0's multi_logloss: 0.191115\n",
      "[583]\tvalid_0's multi_logloss: 0.19111\n",
      "[584]\tvalid_0's multi_logloss: 0.191108\n",
      "[585]\tvalid_0's multi_logloss: 0.191102\n",
      "[586]\tvalid_0's multi_logloss: 0.191099\n",
      "[587]\tvalid_0's multi_logloss: 0.191094\n",
      "[588]\tvalid_0's multi_logloss: 0.191088\n",
      "[589]\tvalid_0's multi_logloss: 0.191084\n",
      "[590]\tvalid_0's multi_logloss: 0.191078\n",
      "[591]\tvalid_0's multi_logloss: 0.191067\n",
      "[592]\tvalid_0's multi_logloss: 0.191065\n",
      "[593]\tvalid_0's multi_logloss: 0.191062\n",
      "[594]\tvalid_0's multi_logloss: 0.191055\n",
      "[595]\tvalid_0's multi_logloss: 0.191048\n",
      "[596]\tvalid_0's multi_logloss: 0.191042\n",
      "[597]\tvalid_0's multi_logloss: 0.191035\n",
      "[598]\tvalid_0's multi_logloss: 0.19103\n",
      "[599]\tvalid_0's multi_logloss: 0.191027\n",
      "[600]\tvalid_0's multi_logloss: 0.191017\n",
      "[601]\tvalid_0's multi_logloss: 0.191008\n",
      "[602]\tvalid_0's multi_logloss: 0.191003\n",
      "[603]\tvalid_0's multi_logloss: 0.190998\n",
      "[604]\tvalid_0's multi_logloss: 0.190991\n",
      "[605]\tvalid_0's multi_logloss: 0.190985\n",
      "[606]\tvalid_0's multi_logloss: 0.190979\n",
      "[607]\tvalid_0's multi_logloss: 0.190972\n",
      "[608]\tvalid_0's multi_logloss: 0.190966\n",
      "[609]\tvalid_0's multi_logloss: 0.190958\n",
      "[610]\tvalid_0's multi_logloss: 0.190954\n",
      "[611]\tvalid_0's multi_logloss: 0.190949\n",
      "[612]\tvalid_0's multi_logloss: 0.190945\n",
      "[613]\tvalid_0's multi_logloss: 0.19094\n",
      "[614]\tvalid_0's multi_logloss: 0.190935\n",
      "[615]\tvalid_0's multi_logloss: 0.190931\n",
      "[616]\tvalid_0's multi_logloss: 0.190925\n",
      "[617]\tvalid_0's multi_logloss: 0.190922\n",
      "[618]\tvalid_0's multi_logloss: 0.190918\n",
      "[619]\tvalid_0's multi_logloss: 0.190915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[620]\tvalid_0's multi_logloss: 0.190909\n",
      "[621]\tvalid_0's multi_logloss: 0.190904\n",
      "[622]\tvalid_0's multi_logloss: 0.190897\n",
      "[623]\tvalid_0's multi_logloss: 0.19089\n",
      "[624]\tvalid_0's multi_logloss: 0.190884\n",
      "[625]\tvalid_0's multi_logloss: 0.190879\n",
      "[626]\tvalid_0's multi_logloss: 0.190874\n",
      "[627]\tvalid_0's multi_logloss: 0.190868\n",
      "[628]\tvalid_0's multi_logloss: 0.190864\n",
      "[629]\tvalid_0's multi_logloss: 0.190858\n",
      "[630]\tvalid_0's multi_logloss: 0.190852\n",
      "[631]\tvalid_0's multi_logloss: 0.190843\n",
      "[632]\tvalid_0's multi_logloss: 0.190838\n",
      "[633]\tvalid_0's multi_logloss: 0.190831\n",
      "[634]\tvalid_0's multi_logloss: 0.190824\n",
      "[635]\tvalid_0's multi_logloss: 0.190817\n",
      "[636]\tvalid_0's multi_logloss: 0.190809\n",
      "[637]\tvalid_0's multi_logloss: 0.190802\n",
      "[638]\tvalid_0's multi_logloss: 0.190796\n",
      "[639]\tvalid_0's multi_logloss: 0.190795\n",
      "[640]\tvalid_0's multi_logloss: 0.190792\n",
      "[641]\tvalid_0's multi_logloss: 0.190787\n",
      "[642]\tvalid_0's multi_logloss: 0.190781\n",
      "[643]\tvalid_0's multi_logloss: 0.190778\n",
      "[644]\tvalid_0's multi_logloss: 0.190773\n",
      "[645]\tvalid_0's multi_logloss: 0.190768\n",
      "[646]\tvalid_0's multi_logloss: 0.190762\n",
      "[647]\tvalid_0's multi_logloss: 0.190756\n",
      "[648]\tvalid_0's multi_logloss: 0.190754\n",
      "[649]\tvalid_0's multi_logloss: 0.190748\n",
      "[650]\tvalid_0's multi_logloss: 0.190743\n",
      "[651]\tvalid_0's multi_logloss: 0.190741\n",
      "[652]\tvalid_0's multi_logloss: 0.190734\n",
      "[653]\tvalid_0's multi_logloss: 0.190728\n",
      "[654]\tvalid_0's multi_logloss: 0.190722\n",
      "[655]\tvalid_0's multi_logloss: 0.190718\n",
      "[656]\tvalid_0's multi_logloss: 0.19071\n",
      "[657]\tvalid_0's multi_logloss: 0.190707\n",
      "[658]\tvalid_0's multi_logloss: 0.190701\n",
      "[659]\tvalid_0's multi_logloss: 0.190696\n",
      "[660]\tvalid_0's multi_logloss: 0.190691\n",
      "[661]\tvalid_0's multi_logloss: 0.190686\n",
      "[662]\tvalid_0's multi_logloss: 0.190681\n",
      "[663]\tvalid_0's multi_logloss: 0.190676\n",
      "[664]\tvalid_0's multi_logloss: 0.190672\n",
      "[665]\tvalid_0's multi_logloss: 0.190666\n",
      "[666]\tvalid_0's multi_logloss: 0.190663\n",
      "[667]\tvalid_0's multi_logloss: 0.190662\n",
      "[668]\tvalid_0's multi_logloss: 0.190654\n",
      "[669]\tvalid_0's multi_logloss: 0.190658\n",
      "[670]\tvalid_0's multi_logloss: 0.190653\n",
      "[671]\tvalid_0's multi_logloss: 0.190646\n",
      "[672]\tvalid_0's multi_logloss: 0.190641\n",
      "[673]\tvalid_0's multi_logloss: 0.190638\n",
      "[674]\tvalid_0's multi_logloss: 0.190635\n",
      "[675]\tvalid_0's multi_logloss: 0.190617\n",
      "[676]\tvalid_0's multi_logloss: 0.190614\n",
      "[677]\tvalid_0's multi_logloss: 0.190609\n",
      "[678]\tvalid_0's multi_logloss: 0.190602\n",
      "[679]\tvalid_0's multi_logloss: 0.190593\n",
      "[680]\tvalid_0's multi_logloss: 0.190588\n",
      "[681]\tvalid_0's multi_logloss: 0.190579\n",
      "[682]\tvalid_0's multi_logloss: 0.190574\n",
      "[683]\tvalid_0's multi_logloss: 0.190568\n",
      "[684]\tvalid_0's multi_logloss: 0.190562\n",
      "[685]\tvalid_0's multi_logloss: 0.190557\n",
      "[686]\tvalid_0's multi_logloss: 0.190553\n",
      "[687]\tvalid_0's multi_logloss: 0.190545\n",
      "[688]\tvalid_0's multi_logloss: 0.19054\n",
      "[689]\tvalid_0's multi_logloss: 0.190537\n",
      "[690]\tvalid_0's multi_logloss: 0.190532\n",
      "[691]\tvalid_0's multi_logloss: 0.190528\n",
      "[692]\tvalid_0's multi_logloss: 0.190523\n",
      "[693]\tvalid_0's multi_logloss: 0.190515\n",
      "[694]\tvalid_0's multi_logloss: 0.190509\n",
      "[695]\tvalid_0's multi_logloss: 0.190502\n",
      "[696]\tvalid_0's multi_logloss: 0.190496\n",
      "[697]\tvalid_0's multi_logloss: 0.190495\n",
      "[698]\tvalid_0's multi_logloss: 0.19049\n",
      "[699]\tvalid_0's multi_logloss: 0.190485\n",
      "[700]\tvalid_0's multi_logloss: 0.190481\n",
      "[701]\tvalid_0's multi_logloss: 0.190476\n",
      "[702]\tvalid_0's multi_logloss: 0.190473\n",
      "[703]\tvalid_0's multi_logloss: 0.19047\n",
      "[704]\tvalid_0's multi_logloss: 0.190465\n",
      "[705]\tvalid_0's multi_logloss: 0.190462\n",
      "[706]\tvalid_0's multi_logloss: 0.190458\n",
      "[707]\tvalid_0's multi_logloss: 0.190456\n",
      "[708]\tvalid_0's multi_logloss: 0.190449\n",
      "[709]\tvalid_0's multi_logloss: 0.190442\n",
      "[710]\tvalid_0's multi_logloss: 0.190436\n",
      "[711]\tvalid_0's multi_logloss: 0.190422\n",
      "[712]\tvalid_0's multi_logloss: 0.190416\n",
      "[713]\tvalid_0's multi_logloss: 0.190409\n",
      "[714]\tvalid_0's multi_logloss: 0.190404\n",
      "[715]\tvalid_0's multi_logloss: 0.190401\n",
      "[716]\tvalid_0's multi_logloss: 0.190393\n",
      "[717]\tvalid_0's multi_logloss: 0.190388\n",
      "[718]\tvalid_0's multi_logloss: 0.190382\n",
      "[719]\tvalid_0's multi_logloss: 0.190378\n",
      "[720]\tvalid_0's multi_logloss: 0.190375\n",
      "[721]\tvalid_0's multi_logloss: 0.190367\n",
      "[722]\tvalid_0's multi_logloss: 0.190365\n",
      "[723]\tvalid_0's multi_logloss: 0.190361\n",
      "[724]\tvalid_0's multi_logloss: 0.190356\n",
      "[725]\tvalid_0's multi_logloss: 0.190351\n",
      "[726]\tvalid_0's multi_logloss: 0.190346\n",
      "[727]\tvalid_0's multi_logloss: 0.190346\n",
      "[728]\tvalid_0's multi_logloss: 0.190342\n",
      "[729]\tvalid_0's multi_logloss: 0.190338\n",
      "[730]\tvalid_0's multi_logloss: 0.190333\n",
      "[731]\tvalid_0's multi_logloss: 0.190329\n",
      "[732]\tvalid_0's multi_logloss: 0.190324\n",
      "[733]\tvalid_0's multi_logloss: 0.190321\n",
      "[734]\tvalid_0's multi_logloss: 0.190317\n",
      "[735]\tvalid_0's multi_logloss: 0.190312\n",
      "[736]\tvalid_0's multi_logloss: 0.190307\n",
      "[737]\tvalid_0's multi_logloss: 0.1903\n",
      "[738]\tvalid_0's multi_logloss: 0.190296\n",
      "[739]\tvalid_0's multi_logloss: 0.190287\n",
      "[740]\tvalid_0's multi_logloss: 0.190277\n",
      "[741]\tvalid_0's multi_logloss: 0.19027\n",
      "[742]\tvalid_0's multi_logloss: 0.190269\n",
      "[743]\tvalid_0's multi_logloss: 0.190265\n",
      "[744]\tvalid_0's multi_logloss: 0.190261\n",
      "[745]\tvalid_0's multi_logloss: 0.190253\n",
      "[746]\tvalid_0's multi_logloss: 0.19025\n",
      "[747]\tvalid_0's multi_logloss: 0.190246\n",
      "[748]\tvalid_0's multi_logloss: 0.190241\n",
      "[749]\tvalid_0's multi_logloss: 0.190236\n",
      "[750]\tvalid_0's multi_logloss: 0.190225\n",
      "[751]\tvalid_0's multi_logloss: 0.190223\n",
      "[752]\tvalid_0's multi_logloss: 0.190217\n",
      "[753]\tvalid_0's multi_logloss: 0.190213\n",
      "[754]\tvalid_0's multi_logloss: 0.19021\n",
      "[755]\tvalid_0's multi_logloss: 0.190203\n",
      "[756]\tvalid_0's multi_logloss: 0.190199\n",
      "[757]\tvalid_0's multi_logloss: 0.190195\n",
      "[758]\tvalid_0's multi_logloss: 0.190191\n",
      "[759]\tvalid_0's multi_logloss: 0.190184\n",
      "[760]\tvalid_0's multi_logloss: 0.190178\n",
      "[761]\tvalid_0's multi_logloss: 0.190172\n",
      "[762]\tvalid_0's multi_logloss: 0.190168\n",
      "[763]\tvalid_0's multi_logloss: 0.190161\n",
      "[764]\tvalid_0's multi_logloss: 0.190156\n",
      "[765]\tvalid_0's multi_logloss: 0.190152\n",
      "[766]\tvalid_0's multi_logloss: 0.190148\n",
      "[767]\tvalid_0's multi_logloss: 0.190144\n",
      "[768]\tvalid_0's multi_logloss: 0.190139\n",
      "[769]\tvalid_0's multi_logloss: 0.190134\n",
      "[770]\tvalid_0's multi_logloss: 0.190128\n",
      "[771]\tvalid_0's multi_logloss: 0.190124\n",
      "[772]\tvalid_0's multi_logloss: 0.190119\n",
      "[773]\tvalid_0's multi_logloss: 0.190113\n",
      "[774]\tvalid_0's multi_logloss: 0.190105\n",
      "[775]\tvalid_0's multi_logloss: 0.190101\n",
      "[776]\tvalid_0's multi_logloss: 0.190097\n",
      "[777]\tvalid_0's multi_logloss: 0.190091\n",
      "[778]\tvalid_0's multi_logloss: 0.190088\n",
      "[779]\tvalid_0's multi_logloss: 0.190084\n",
      "[780]\tvalid_0's multi_logloss: 0.19008\n",
      "[781]\tvalid_0's multi_logloss: 0.190078\n",
      "[782]\tvalid_0's multi_logloss: 0.190071\n",
      "[783]\tvalid_0's multi_logloss: 0.190065\n",
      "[784]\tvalid_0's multi_logloss: 0.19006\n",
      "[785]\tvalid_0's multi_logloss: 0.190055\n",
      "[786]\tvalid_0's multi_logloss: 0.190052\n",
      "[787]\tvalid_0's multi_logloss: 0.19005\n",
      "[788]\tvalid_0's multi_logloss: 0.190043\n",
      "[789]\tvalid_0's multi_logloss: 0.19004\n",
      "[790]\tvalid_0's multi_logloss: 0.190036\n",
      "[791]\tvalid_0's multi_logloss: 0.190031\n",
      "[792]\tvalid_0's multi_logloss: 0.190024\n",
      "[793]\tvalid_0's multi_logloss: 0.190021\n",
      "[794]\tvalid_0's multi_logloss: 0.190015\n",
      "[795]\tvalid_0's multi_logloss: 0.190013\n",
      "[796]\tvalid_0's multi_logloss: 0.190005\n",
      "[797]\tvalid_0's multi_logloss: 0.190002\n",
      "[798]\tvalid_0's multi_logloss: 0.189995\n",
      "[799]\tvalid_0's multi_logloss: 0.189989\n",
      "[800]\tvalid_0's multi_logloss: 0.189986\n",
      "[801]\tvalid_0's multi_logloss: 0.189982\n",
      "[802]\tvalid_0's multi_logloss: 0.189976\n",
      "[803]\tvalid_0's multi_logloss: 0.189975\n",
      "[804]\tvalid_0's multi_logloss: 0.189968\n",
      "[805]\tvalid_0's multi_logloss: 0.189962\n",
      "[806]\tvalid_0's multi_logloss: 0.189959\n",
      "[807]\tvalid_0's multi_logloss: 0.189949\n",
      "[808]\tvalid_0's multi_logloss: 0.189941\n",
      "[809]\tvalid_0's multi_logloss: 0.189937\n",
      "[810]\tvalid_0's multi_logloss: 0.189933\n",
      "[811]\tvalid_0's multi_logloss: 0.189931\n",
      "[812]\tvalid_0's multi_logloss: 0.189926\n",
      "[813]\tvalid_0's multi_logloss: 0.189925\n",
      "[814]\tvalid_0's multi_logloss: 0.189916\n",
      "[815]\tvalid_0's multi_logloss: 0.189908\n",
      "[816]\tvalid_0's multi_logloss: 0.189908\n",
      "[817]\tvalid_0's multi_logloss: 0.189899\n",
      "[818]\tvalid_0's multi_logloss: 0.189894\n",
      "[819]\tvalid_0's multi_logloss: 0.189891\n",
      "[820]\tvalid_0's multi_logloss: 0.189892\n",
      "[821]\tvalid_0's multi_logloss: 0.189889\n",
      "[822]\tvalid_0's multi_logloss: 0.189877\n",
      "[823]\tvalid_0's multi_logloss: 0.189871\n",
      "[824]\tvalid_0's multi_logloss: 0.189869\n",
      "[825]\tvalid_0's multi_logloss: 0.189864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[826]\tvalid_0's multi_logloss: 0.189858\n",
      "[827]\tvalid_0's multi_logloss: 0.189853\n",
      "[828]\tvalid_0's multi_logloss: 0.189841\n",
      "[829]\tvalid_0's multi_logloss: 0.189836\n",
      "[830]\tvalid_0's multi_logloss: 0.189835\n",
      "[831]\tvalid_0's multi_logloss: 0.189833\n",
      "[832]\tvalid_0's multi_logloss: 0.189828\n",
      "[833]\tvalid_0's multi_logloss: 0.189825\n",
      "[834]\tvalid_0's multi_logloss: 0.189819\n",
      "[835]\tvalid_0's multi_logloss: 0.189815\n",
      "[836]\tvalid_0's multi_logloss: 0.189811\n",
      "[837]\tvalid_0's multi_logloss: 0.189805\n",
      "[838]\tvalid_0's multi_logloss: 0.189797\n",
      "[839]\tvalid_0's multi_logloss: 0.189794\n",
      "[840]\tvalid_0's multi_logloss: 0.189791\n",
      "[841]\tvalid_0's multi_logloss: 0.189786\n",
      "[842]\tvalid_0's multi_logloss: 0.189782\n",
      "[843]\tvalid_0's multi_logloss: 0.189779\n",
      "[844]\tvalid_0's multi_logloss: 0.189775\n",
      "[845]\tvalid_0's multi_logloss: 0.189771\n",
      "[846]\tvalid_0's multi_logloss: 0.189767\n",
      "[847]\tvalid_0's multi_logloss: 0.18976\n",
      "[848]\tvalid_0's multi_logloss: 0.189756\n",
      "[849]\tvalid_0's multi_logloss: 0.189752\n",
      "[850]\tvalid_0's multi_logloss: 0.189747\n",
      "[851]\tvalid_0's multi_logloss: 0.189744\n",
      "[852]\tvalid_0's multi_logloss: 0.189742\n",
      "[853]\tvalid_0's multi_logloss: 0.189738\n",
      "[854]\tvalid_0's multi_logloss: 0.189741\n",
      "[855]\tvalid_0's multi_logloss: 0.189736\n",
      "[856]\tvalid_0's multi_logloss: 0.189734\n",
      "[857]\tvalid_0's multi_logloss: 0.189729\n",
      "[858]\tvalid_0's multi_logloss: 0.189725\n",
      "[859]\tvalid_0's multi_logloss: 0.189723\n",
      "[860]\tvalid_0's multi_logloss: 0.18972\n",
      "[861]\tvalid_0's multi_logloss: 0.189706\n",
      "[862]\tvalid_0's multi_logloss: 0.189703\n",
      "[863]\tvalid_0's multi_logloss: 0.189697\n",
      "[864]\tvalid_0's multi_logloss: 0.18969\n",
      "[865]\tvalid_0's multi_logloss: 0.189686\n",
      "[866]\tvalid_0's multi_logloss: 0.189677\n",
      "[867]\tvalid_0's multi_logloss: 0.189673\n",
      "[868]\tvalid_0's multi_logloss: 0.18967\n",
      "[869]\tvalid_0's multi_logloss: 0.189667\n",
      "[870]\tvalid_0's multi_logloss: 0.189663\n",
      "[871]\tvalid_0's multi_logloss: 0.189656\n",
      "[872]\tvalid_0's multi_logloss: 0.189653\n",
      "[873]\tvalid_0's multi_logloss: 0.189651\n",
      "[874]\tvalid_0's multi_logloss: 0.189647\n",
      "[875]\tvalid_0's multi_logloss: 0.189639\n",
      "[876]\tvalid_0's multi_logloss: 0.189634\n",
      "[877]\tvalid_0's multi_logloss: 0.189632\n",
      "[878]\tvalid_0's multi_logloss: 0.189627\n",
      "[879]\tvalid_0's multi_logloss: 0.18962\n",
      "[880]\tvalid_0's multi_logloss: 0.189615\n",
      "[881]\tvalid_0's multi_logloss: 0.189613\n",
      "[882]\tvalid_0's multi_logloss: 0.189608\n",
      "[883]\tvalid_0's multi_logloss: 0.189602\n",
      "[884]\tvalid_0's multi_logloss: 0.189602\n",
      "[885]\tvalid_0's multi_logloss: 0.189599\n",
      "[886]\tvalid_0's multi_logloss: 0.189596\n",
      "[887]\tvalid_0's multi_logloss: 0.189595\n",
      "[888]\tvalid_0's multi_logloss: 0.189591\n",
      "[889]\tvalid_0's multi_logloss: 0.189588\n",
      "[890]\tvalid_0's multi_logloss: 0.189586\n",
      "[891]\tvalid_0's multi_logloss: 0.189584\n",
      "[892]\tvalid_0's multi_logloss: 0.189581\n",
      "[893]\tvalid_0's multi_logloss: 0.189577\n",
      "[894]\tvalid_0's multi_logloss: 0.189574\n",
      "[895]\tvalid_0's multi_logloss: 0.18957\n",
      "[896]\tvalid_0's multi_logloss: 0.189568\n",
      "[897]\tvalid_0's multi_logloss: 0.189562\n",
      "[898]\tvalid_0's multi_logloss: 0.189559\n",
      "[899]\tvalid_0's multi_logloss: 0.189554\n",
      "[900]\tvalid_0's multi_logloss: 0.18955\n",
      "[901]\tvalid_0's multi_logloss: 0.189547\n",
      "[902]\tvalid_0's multi_logloss: 0.189545\n",
      "[903]\tvalid_0's multi_logloss: 0.189539\n",
      "[904]\tvalid_0's multi_logloss: 0.189533\n",
      "[905]\tvalid_0's multi_logloss: 0.18953\n",
      "[906]\tvalid_0's multi_logloss: 0.189523\n",
      "[907]\tvalid_0's multi_logloss: 0.189521\n",
      "[908]\tvalid_0's multi_logloss: 0.189514\n",
      "[909]\tvalid_0's multi_logloss: 0.189507\n",
      "[910]\tvalid_0's multi_logloss: 0.189504\n",
      "[911]\tvalid_0's multi_logloss: 0.189502\n",
      "[912]\tvalid_0's multi_logloss: 0.189494\n",
      "[913]\tvalid_0's multi_logloss: 0.189486\n",
      "[914]\tvalid_0's multi_logloss: 0.189484\n",
      "[915]\tvalid_0's multi_logloss: 0.189481\n",
      "[916]\tvalid_0's multi_logloss: 0.189477\n",
      "[917]\tvalid_0's multi_logloss: 0.189471\n",
      "[918]\tvalid_0's multi_logloss: 0.189465\n",
      "[919]\tvalid_0's multi_logloss: 0.18946\n",
      "[920]\tvalid_0's multi_logloss: 0.189456\n",
      "[921]\tvalid_0's multi_logloss: 0.18945\n",
      "[922]\tvalid_0's multi_logloss: 0.189448\n",
      "[923]\tvalid_0's multi_logloss: 0.189445\n",
      "[924]\tvalid_0's multi_logloss: 0.18944\n",
      "[925]\tvalid_0's multi_logloss: 0.189435\n",
      "[926]\tvalid_0's multi_logloss: 0.18943\n",
      "[927]\tvalid_0's multi_logloss: 0.189425\n",
      "[928]\tvalid_0's multi_logloss: 0.189422\n",
      "[929]\tvalid_0's multi_logloss: 0.189418\n",
      "[930]\tvalid_0's multi_logloss: 0.189414\n",
      "[931]\tvalid_0's multi_logloss: 0.189408\n",
      "[932]\tvalid_0's multi_logloss: 0.189401\n",
      "[933]\tvalid_0's multi_logloss: 0.189396\n",
      "[934]\tvalid_0's multi_logloss: 0.189393\n",
      "[935]\tvalid_0's multi_logloss: 0.189389\n",
      "[936]\tvalid_0's multi_logloss: 0.189386\n",
      "[937]\tvalid_0's multi_logloss: 0.18938\n",
      "[938]\tvalid_0's multi_logloss: 0.189382\n",
      "[939]\tvalid_0's multi_logloss: 0.189376\n",
      "[940]\tvalid_0's multi_logloss: 0.189372\n",
      "[941]\tvalid_0's multi_logloss: 0.189372\n",
      "[942]\tvalid_0's multi_logloss: 0.189368\n",
      "[943]\tvalid_0's multi_logloss: 0.189364\n",
      "[944]\tvalid_0's multi_logloss: 0.189359\n",
      "[945]\tvalid_0's multi_logloss: 0.189356\n",
      "[946]\tvalid_0's multi_logloss: 0.189353\n",
      "[947]\tvalid_0's multi_logloss: 0.189352\n",
      "[948]\tvalid_0's multi_logloss: 0.18935\n",
      "[949]\tvalid_0's multi_logloss: 0.189345\n",
      "[950]\tvalid_0's multi_logloss: 0.189342\n",
      "[951]\tvalid_0's multi_logloss: 0.189337\n",
      "[952]\tvalid_0's multi_logloss: 0.189333\n",
      "[953]\tvalid_0's multi_logloss: 0.189329\n",
      "[954]\tvalid_0's multi_logloss: 0.189325\n",
      "[955]\tvalid_0's multi_logloss: 0.18932\n",
      "[956]\tvalid_0's multi_logloss: 0.189317\n",
      "[957]\tvalid_0's multi_logloss: 0.189312\n",
      "[958]\tvalid_0's multi_logloss: 0.189308\n",
      "[959]\tvalid_0's multi_logloss: 0.189302\n",
      "[960]\tvalid_0's multi_logloss: 0.1893\n",
      "[961]\tvalid_0's multi_logloss: 0.189293\n",
      "[962]\tvalid_0's multi_logloss: 0.189291\n",
      "[963]\tvalid_0's multi_logloss: 0.189289\n",
      "[964]\tvalid_0's multi_logloss: 0.189287\n",
      "[965]\tvalid_0's multi_logloss: 0.189284\n",
      "[966]\tvalid_0's multi_logloss: 0.189278\n",
      "[967]\tvalid_0's multi_logloss: 0.189273\n",
      "[968]\tvalid_0's multi_logloss: 0.189269\n",
      "[969]\tvalid_0's multi_logloss: 0.189267\n",
      "[970]\tvalid_0's multi_logloss: 0.189261\n",
      "[971]\tvalid_0's multi_logloss: 0.189254\n",
      "[972]\tvalid_0's multi_logloss: 0.189252\n",
      "[973]\tvalid_0's multi_logloss: 0.189246\n",
      "[974]\tvalid_0's multi_logloss: 0.189241\n",
      "[975]\tvalid_0's multi_logloss: 0.189237\n",
      "[976]\tvalid_0's multi_logloss: 0.189235\n",
      "[977]\tvalid_0's multi_logloss: 0.189233\n",
      "[978]\tvalid_0's multi_logloss: 0.189228\n",
      "[979]\tvalid_0's multi_logloss: 0.189223\n",
      "[980]\tvalid_0's multi_logloss: 0.189219\n",
      "[981]\tvalid_0's multi_logloss: 0.189216\n",
      "[982]\tvalid_0's multi_logloss: 0.189213\n",
      "[983]\tvalid_0's multi_logloss: 0.189209\n",
      "[984]\tvalid_0's multi_logloss: 0.189204\n",
      "[985]\tvalid_0's multi_logloss: 0.1892\n",
      "[986]\tvalid_0's multi_logloss: 0.189193\n",
      "[987]\tvalid_0's multi_logloss: 0.189187\n",
      "[988]\tvalid_0's multi_logloss: 0.189184\n",
      "[989]\tvalid_0's multi_logloss: 0.189178\n",
      "[990]\tvalid_0's multi_logloss: 0.189173\n",
      "[991]\tvalid_0's multi_logloss: 0.189167\n",
      "[992]\tvalid_0's multi_logloss: 0.189164\n",
      "[993]\tvalid_0's multi_logloss: 0.18916\n",
      "[994]\tvalid_0's multi_logloss: 0.189154\n",
      "[995]\tvalid_0's multi_logloss: 0.18915\n",
      "[996]\tvalid_0's multi_logloss: 0.189145\n",
      "[997]\tvalid_0's multi_logloss: 0.189142\n",
      "[998]\tvalid_0's multi_logloss: 0.189136\n",
      "[999]\tvalid_0's multi_logloss: 0.189135\n",
      "[1000]\tvalid_0's multi_logloss: 0.18913\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's multi_logloss: 0.18913\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameter for the LightGBM model\n",
    "params_lgb = {\n",
    "    'task' : 'train',\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'objective' : 'multiclass',\n",
    "    'num_class': 17,\n",
    "    'metric' : {'multi_logloss'},\n",
    "    'is_training_metric': True,\n",
    "    'max_bin': 255,\n",
    "    'num_leaves' : 64,\n",
    "    'learning_rate' : 0.1,\n",
    "    'feature_fraction' : 0.8,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'min_sum_hessian_in_leaf': 5,\n",
    "}\n",
    "\n",
    "# Convert train, validation data to fit in the LightGBM model\n",
    "train = lgbm.Dataset(X_trn_vld, label=y_trn_vld, feature_name=features)\n",
    "validate = lgbm.Dataset(X_eval_vld, label=y_eval_vld, feature_name=features, reference=train)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model_lgb = lgbm.train(params_lgb, train, num_boost_round=1000, valid_sets=validate, early_stopping_rounds=20)\n",
    "best_iteration = model_lgb.best_iteration\n",
    "\n",
    "# Save the trained model and best iteration detail\n",
    "model_lgb.save_model(\"model/lgbm.model.txt\")\n",
    "pickle.dump(best_iteration, open(\"model/lgbm.model.meta\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036521553551041475\n"
     ]
    }
   ],
   "source": [
    "# Getting the prediction with the validation data\n",
    "preds_vld_lgb = model_lgb.predict(tst_vld[features], ntree_limit=best_iteration)\n",
    "\n",
    "# Choose predictions only for top 16 classes\n",
    "preds_vld_lgb_16 = np.delete(preds_vld_lgb, 16, axis=1)\n",
    "\n",
    "# It is impossible to purchase products already did, so subtract 1 from the prediction\n",
    "preds_vld_lgb_16 = preds_vld_lgb_16 - tst_vld[[prod+'_prev' for prod in prods[target]]]\n",
    "result_lgb = predict_7_products(preds_vld_lgb_16.values)\n",
    "\n",
    "# Get 7 products and calculate MAP@7 with the predictions (0.03652)\n",
    "print(mapk(add_vld_list, result_lgb, 7, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data should be between -1 and 1 to be trained, so we scaled numeric features before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int8, int16, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(trn[features])\n",
    "\n",
    "X_trn_vld_norm = scaler.transform(X_trn_vld)\n",
    "X_eval_vld_norm = scaler.transform(X_eval_vld)\n",
    "\n",
    "y_trn_vld_matrix = trn_vld[[prod for prod in prods[target]]].values\n",
    "y_eval_vld_matrix = eval_vld[[prod for prod in prods[target]]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               31232     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 302,096\n",
      "Trainable params: 302,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_nn = models.Sequential()\n",
    "model_nn.add(layers.Dense(512, activation='relu', input_shape=(60,)))\n",
    "model_nn.add(layers.Dropout(0.8))\n",
    "model_nn.add(layers.Dense(512, activation='relu'))\n",
    "model_nn.add(layers.Dropout(0.5))\n",
    "model_nn.add(layers.Dense(16, activation='softmax'))\n",
    "\n",
    "model_nn.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\thisi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 10765757 samples, validate on 689132 samples\n",
      "Epoch 1/100\n",
      "10765757/10765757 [==============================] - 833s 77us/step - loss: 4.4280 - acc: 0.7962 - val_loss: 4.5847 - val_acc: 0.8338\n",
      "Epoch 2/100\n",
      "10765757/10765757 [==============================] - 839s 78us/step - loss: 6.0680 - acc: 0.8093 - val_loss: 5.8353 - val_acc: 0.8641\n",
      "Epoch 3/100\n",
      "10765757/10765757 [==============================] - 814s 76us/step - loss: 6.6630 - acc: 0.8154 - val_loss: 6.1597 - val_acc: 0.8705\n",
      "Epoch 4/100\n",
      "10765757/10765757 [==============================] - 830s 77us/step - loss: 6.9333 - acc: 0.8162 - val_loss: 6.5466 - val_acc: 0.8626\n",
      "Epoch 5/100\n",
      "10765757/10765757 [==============================] - 839s 78us/step - loss: 7.2520 - acc: 0.8153 - val_loss: 7.0494 - val_acc: 0.8623\n",
      "Epoch 6/100\n",
      "10765757/10765757 [==============================] - 830s 77us/step - loss: 7.5079 - acc: 0.8162 - val_loss: 7.4550 - val_acc: 0.8729\n",
      "Epoch 7/100\n",
      "10765757/10765757 [==============================] - 832s 77us/step - loss: 7.8727 - acc: 0.8193 - val_loss: 8.0043 - val_acc: 0.8766\n",
      "Epoch 8/100\n",
      "10765757/10765757 [==============================] - 831s 77us/step - loss: 8.1750 - acc: 0.8202 - val_loss: 8.1364 - val_acc: 0.8743\n",
      "Epoch 9/100\n",
      "10765757/10765757 [==============================] - 837s 78us/step - loss: 8.3627 - acc: 0.8181 - val_loss: 8.3968 - val_acc: 0.8779\n",
      "Epoch 10/100\n",
      "10765757/10765757 [==============================] - 827s 77us/step - loss: 8.4917 - acc: 0.8158 - val_loss: 8.5431 - val_acc: 0.8771\n",
      "Epoch 11/100\n",
      "10765757/10765757 [==============================] - 832s 77us/step - loss: 8.5457 - acc: 0.8170 - val_loss: 8.4976 - val_acc: 0.8727\n",
      "Epoch 12/100\n",
      "10765757/10765757 [==============================] - 830s 77us/step - loss: 8.5724 - acc: 0.8163 - val_loss: 8.5077 - val_acc: 0.8725\n",
      "Epoch 13/100\n",
      "10765757/10765757 [==============================] - 836s 78us/step - loss: 8.6056 - acc: 0.8145 - val_loss: 8.5786 - val_acc: 0.8725\n",
      "Epoch 14/100\n",
      "10765757/10765757 [==============================] - 830s 77us/step - loss: 8.6397 - acc: 0.8149 - val_loss: 8.6792 - val_acc: 0.8731\n",
      "Epoch 15/100\n",
      "10765757/10765757 [==============================] - 834s 77us/step - loss: 8.6493 - acc: 0.8180 - val_loss: 8.6499 - val_acc: 0.8736\n",
      "Epoch 16/100\n",
      "10765757/10765757 [==============================] - 825s 77us/step - loss: 8.6841 - acc: 0.8195 - val_loss: 8.7431 - val_acc: 0.8747\n",
      "Epoch 17/100\n",
      "10765757/10765757 [==============================] - 810s 75us/step - loss: 8.7430 - acc: 0.8178 - val_loss: 8.7675 - val_acc: 0.8755\n",
      "Epoch 18/100\n",
      "10765757/10765757 [==============================] - 816s 76us/step - loss: 8.7538 - acc: 0.8191 - val_loss: 8.8385 - val_acc: 0.8771\n",
      "Epoch 19/100\n",
      "10765757/10765757 [==============================] - 818s 76us/step - loss: 8.8443 - acc: 0.8196 - val_loss: 9.2628 - val_acc: 0.8763\n"
     ]
    }
   ],
   "source": [
    "# Train and save the model\n",
    "callback_list = [EarlyStopping(monitor='val_acc', patience = 10)]\n",
    "model_nn.fit(X_trn_vld_norm, y_trn_vld_matrix, epochs=100, batch_size=64, callbacks=callback_list, validation_data=(X_eval_vld_norm ,y_eval_vld_matrix))\n",
    "\n",
    "pickle.dump(model_nn, open(\"../model/neuralnetwork.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00884893083733709"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaled the validation data and predict the model\n",
    "X_tst_vld_norm = scaler.transform(tst_vld[features])\n",
    "vld_preds_nn = model_nn.predict(X_tst_vld_norm, batch_size=512)\n",
    "\n",
    "# Get products based on the predictions up to 7\n",
    "result_nn = []\n",
    "for ncodper, prds in zip(ncodpers_tst_vld, preds_prod):\n",
    "    r = [(ip,p) for ip, p in zip(target,prds) if p > 0]\n",
    "    r = sorted(r, key=lambda a:a[1], reverse=True)[:7]\n",
    "    result_nn.append([ip for ip,p in r])\n",
    "\n",
    "# Calculate MAP@7 with the predictions  (0.00884)\n",
    "mapk(add_vld_list, result_nn, 7, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "#### Starategy\n",
    "- XGBoost/LightGBM: I deisnged the model to target the 16 most popular products and 17th class indicating either no additions or an addition of one of the eight remaining products.\n",
    "<br>\n",
    "- NN: This model target presence of product in a given month regardless the products are newly purchased or not. It targeted a length 16 vector of the more more popular products and are trained on all customers.product. It has two hidden layers of 512 nodes and drop out layers, and the 16 node output layer.\n",
    "\n",
    "#### Overview Performance\n",
    "- Map@7 maximum : 0.04266\n",
    "- XGBoost : 0.03609\n",
    "- LightGBM : 0.03652\n",
    "- NN : 0.00884"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
